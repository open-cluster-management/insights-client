{"content":[{"summary":"OpenShift will drop the outgoing traffic from a project with an Egress IP that is in no HostSubnet or two or more HostSubnets.\n","reason":"The OpenShift cluster will drop the outgoing traffic from projects whose Egress IPs are in no HostSubnet or in two or more HostSubnets:\n{{~ pydata.no_hostsubnet :data }}\n- Egress IP '{{=data[0]}}' with no HostSubnet is used in NetNameSpace '{{=data[1]}}'\n{{~}}\n{{~ pydata.several_hostsubnets :data }}\n- Egress IP '{{=data[0]}}' with two or more HostSubnets is used in NetNameSpace '{{=data[1]}}'. HostSubnets:\n  {{~ data[2] :hostsubnet }}\n  - {{=hostsubnet}}\n  {{~}}\n{{~}}\n","resolution":"{{? pydata.no_hostsubnet }}\nRed Hat recommends that you configure HostSubnets for {{?pydata.no_hostsubnet.length\u003e1}}these Egress IP addresses{{??}}this Egress IP address{{?}}:\n\n{{~ pydata.no_hostsubnet :data }}\n- Egress IP '{{=data[0]}}' with no HostSubnet is used in NetNameSpace '{{=data[1]}}'\n{{~}}\n{{?}}\n\n{{? pydata.several_hostsubnets }}\nRed Hat recommends that you keep only one HostSubnet for {{?pydata.several_hostsubnets.length\u003e1}}these Egress IP addresses{{??}}the Egress IP address{{?}}:\n\n{{~ pydata.several_hostsubnets :data }}\n- Egress IP '{{=data[0]}}' with two or more HostSubnets is used in NetNameSpace '{{=data[1]}}'. HostSubnets:\n  {{~ data[2] :hostsubnet }}\n  - {{=hostsubnet}}\n  {{~}}\n{{~}}\n{{?}}\n\nSee [Configuring egress IPs for a project](https://docs.openshift.com/container-platform/{{=pydata.ocp_version}}/networking/openshift_sdn/assigning-egress-ips.html) for more information about configuring Egress IPs.\n","more_info":"","plugin":{"name":"The cluster drops outgoing traffic for a project when an Egress IP in its NetNamespace is in no HostSubnet or in two or more HostSubnets","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.sdn_egress_ip_hostsubnet_check"},"error_keys":{"SDN_EGRESS_IP_HOSTSUBNET_ISSUE":{"generic":"OpenShift will drop the outgoing traffic from a project with an Egress IP that is in no HostSubnet or in two or more HostSubnets.\n","metadata":{"condition":"An Egress IP present in a NetNamespace is in no HostSubnet; An Egress IP present in a NetNamespace is in two or more HostSubnets","description":"The cluster drops outgoing traffic for a project when a Egress IP in its NetNamespace is in no HostSubnet or in two or more HostSubnets","impact":"Cluster Availability","likelihood":3,"publish_date":"2021-02-09 14:00:00","status":"active","tags":["openshift","configuration","service_availability"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"One or more containers (within pods) are not-ready or have minimum 10 restarts.","reason":"The following containers are experiencing issues:\n{{~ pydata.containers :container }}\n    {{=container[\"name\"]}}\n    Pod of the effected container: {{=container[\"pod\"]}}\n    Ready: {{=container[\"ready\"]}}\n    Restart count: {{=container[\"restarts\"]}}\n{{~}}","resolution":"","more_info":"","plugin":{"name":"","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.pods_check_containers"},"error_keys":{"POD_CONTAINER_ISSUE":{"generic":"Not-ready or unstable(ones with multiple restarts) containers inside pods.","metadata":{"condition":"Not-ready or unstable(ones with multiple restarts) containers inside pods","description":"Application creation failure when pods have not-ready or unstable(ones with multiple restarts) containers","impact":"Application Crash","likelihood":3,"publish_date":"2019-10-24 07:10:00","status":"active","tags":["openshift","pod","container","incident","pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"OpenShift nodes on Red Hat Virtualization fail complaining of high disk usage.\n","reason":"Currently there are following OutOfSpace alerts in your OCP4 cluster running on RHV VM. This cluster will fail shortly due to disk size is not sufficient. This issue occurs due to disk size is too small in template.\n\n{{~pydata.alerts:alert}}\n**Alertname**: {{=alert[\"alertname\"]}}\n**Device**: {{=alert[\"device\"]}}\n**Namespace**: {{=alert[\"namespace\"]}}\n**Mountpoint**: {{=alert[\"mountpoint\"]}}\n**Service**: {{=alert[\"service\"]}}\n**Prometheus**: {{=alert[\"prometheus\"]}}\n**Pod**: {{=alert[\"pod\"]}}\n{{~}}\n","resolution":"You are hitting a [known bug](https://bugzilla.redhat.com/show_bug.cgi?id=1818577) and Red Hat recommends that you complete the following steps:\n\n\n1. [Uninstall the cluster](https://docs.openshift.com/container-platform/4.4/installing/installing_rhv/uninstalling-cluster-rhv.html).\n2. [Create a custom virtual machine template on RHV](https://docs.openshift.com/container-platform/4.4/installing/installing_rhv/installing-rhv-creating-custom-vm.html).\n3. Reinstall the cluster using the 120GB disk size recommended for production clusters using either the [Installing a cluster quickly on RHV](https://docs.openshift.com/container-platform/4.4/installing/installing_rhv/installing-rhv-default.html) or [Installing a cluster on RHV with customizations](https://docs.openshift.com/container-platform/4.4/installing/installing_rhv/installing-rhv-customizations.html) instructions.\n","more_info":"","plugin":{"name":"OpenShift nodes on Red Hat Virtualization fail complaining of high disk usage when the disk size in template is too small","node_id":"5043561","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.high_disc_usage"},"error_keys":{"NODE_FILE_SYSTEM_FILLING_OVIRT":{"generic":"OpenShift nodes on Red Hat Virtualization fail complaining of high disk usage due to small disk size in template.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/5043561)\n","metadata":{"condition":"One of these alerts, \"NodeFilesystemSpaceFillingUpr\" or \"NodeFilesystemAlmostOutOfSpace\" on RHV installation detected.","description":"OpenShift nodes on Red Hat Virtualization fail complaining of high disk usage when the disk size in template is too small","impact":"System Stability Loss","likelihood":4,"publish_date":"2020-03-26 12:00:00","status":"active","tags":["openshift","service_availability","pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"When an OpenShift version reaches its End of Life, it is no longer eligible to receive maintenance updates, and technical support is limited only to aid upgrades to in-support versions of OpenShift 4. \n","reason":"","resolution":"Red Hat recommends that you upgrade the OpenShift cluster to a more recent version. \n\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n\n* [OpenShift {{=pydata.minor_plus[1]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[1]['version']}}/updating/updating-cluster-between-minor.html)\n* [OpenShift {{=pydata.minor_plus[2]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[2]['version']}}/updating/updating-cluster-between-minor.html){{?pydata.beyond_eol}}\n* [OpenShift {{=pydata.minor_plus[3]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[3]['version']}}/updating/updating-cluster-between-minor.html){{?}}\n","more_info":"","plugin":{"name":"The running OpenShift version is near or beyond its End of Life","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.ocp_version_end_of_life"},"error_keys":{"OCP4X_BEYOND_EOL":{"generic":"The running OpenShift version has reached its End of Life. It is no longer eligible to receive maintenance updates. Technical support is limited only to aid upgrades to in-support versions of OpenShift 4. \n","metadata":{"condition":"The running OpenShift version is beyond its End of Life","description":"The running OpenShift version has reached its End of Life","impact":"Support Unavailable","likelihood":4,"publish_date":"2020-06-17 15:00:00","status":"active","tags":["openshift","service_availability","pssa"]},"reason":"  * The cluster is running OpenShift {{=pydata.current_version}}.\n  * OpenShift {{=pydata.minor_plus[0]['version']}} has reached its End of Life when OpenShift {{=pydata.minor_plus[3]['version']}} was released on {{=pydata.minor_plus[3]['ga']}}.\n\nPlease review [Red Hat OpenShift Container Platform Life Cycle Policy](https://access.redhat.com/support/policy/updates/openshift#dates) for more information.\n","HasReason":true},"OCP4X_EOL_APPROACHING":{"generic":"The running OpenShift version is estimated to reach its End of Life in less than 3 months. It will no longer be eligible to receive maintenance updates. Technical support will be limited only to aid upgrades to in-support versions of OpenShift 4. \n","metadata":{"condition":"The running OpenShift version is estimated to reach its End of Life in less than 3 months","description":"The running OpenShift version is estimated to reach its End of Life in less than 3 months","impact":"Support Unavailable","likelihood":1,"publish_date":"2020-06-17 15:00:00","status":"active","tags":["openshift","service_availability","pssa"]},"reason":"  * The cluster is running OpenShift {{=pydata.current_version}}.\n  * OpenShift {{=pydata.minor_plus[0]['version']}} will reach its End of Life at the release of OpenShift {{=pydata.minor_plus[3]['version']}}.\n  * Red Hat releases new minor versions of OpenShift 4 approximately every three months.\n  * OpenShift {{=pydata.minor_plus[2]['version']}} was released on {{=pydata.minor_plus[2]['ga']}}.\n  * OpenShift {{=pydata.minor_plus[3]['version']}} is estimated to be released in less than\n    {{?pydata.months_to_eol == 1}} {{=pydata.months_to_eol}} month\n    {{?}}{{?pydata.months_to_eol != 1}} {{=pydata.months_to_eol}} months{{?}}.\n\nPlease review [Red Hat OpenShift Container Platform Life Cycle Policy](https://access.redhat.com/support/policy/updates/openshift#dates) for more information.\n","HasReason":true},"OCP4X_EOL_IMMINENT":{"generic":"The running OpenShift version is estimated to reach its End of Life in less than 1 month. It will no longer be eligible to receive maintenance updates. Technical support will be limited only to aid upgrades to in-support versions of OpenShift 4. \n","metadata":{"condition":"The running OpenShift version is estimated to reach its End of Life in less than 1 month","description":"The running OpenShift version is estimated to reach its End of Life in less than 1 month","impact":"Support Unavailable","likelihood":2,"publish_date":"2020-06-17 15:00:00","status":"active","tags":["openshift","service_availability","pssa"]},"reason":"  * The cluster is running OpenShift {{=pydata.current_version}}.\n  * OpenShift {{=pydata.minor_plus[0]['version']}} will reach its End of Life at the release of OpenShift {{=pydata.minor_plus[3]['version']}}.\n  * Red Hat releases new minor versions of OpenShift 4 approximately every three months.\n  * OpenShift {{=pydata.minor_plus[2]['version']}} was released on {{=pydata.minor_plus[2]['ga']}}.\n  * OpenShift {{=pydata.minor_plus[3]['version']}} is estimated to be released in less than\n    {{?pydata.months_to_eol == 1}} {{=pydata.months_to_eol}} month\n    {{?}}{{?pydata.months_to_eol != 1}} {{=pydata.months_to_eol}} months{{?}}.\n\nPlease review [Red Hat OpenShift Container Platform Life Cycle Policy](https://access.redhat.com/support/policy/updates/openshift#dates) for more information.\n","HasReason":true}},"HasReason":false},{"summary":"When the persistent volume claim (PVC) for Prometheus is set to EmptyDir, which is a temporary directory. Prometheus metrics data will lose when the Prometheus pod being restarted or recreated\n","reason":"  * There is no persistent storage for Prometheus to store the metrics data\n\nPlease review [Configuring the monitoring stack](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/monitoring/configuring-the-monitoring-stack.html) for more information.\n","resolution":"Red Hat recommends that you configure permanent storage for the Prometheus Cluster Monitoring Stack. For more information, see the *Configuring persistent storage* section of the documentation of the desired OpenShift version:\n  * [OpenShift {{=pydata.ocp_branch}}](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/monitoring/cluster_monitoring/configuring-the-monitoring-stack.html#configuring-persistent-storage)\n","more_info":"","plugin":{"name":"Prometheus metrics data will lose when the Prometheus pod being restarted or recreated","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.empty_prometheus_db_volume"},"error_keys":{"PROMETHEUS_DB_VOLUME_IS_EMPTY":{"generic":"Prometheus metrics data will be lost when the Prometheus pod is restarted or recreated. The PVC for Prometheus is set to EmptyDir which is a temporary directory.\n","metadata":{"condition":"The persistent volume claim (PVC) for Prometheus is set to EmptyDir, which is a temporary directory","description":"Prometheus metrics data will be lost when the Prometheus pod is restarted or recreated","impact":"Invalid Configuration","likelihood":4,"publish_date":"2020-11-17 11:47:00","status":"active","tags":["openshift","service_availability"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"The OpenShift installer creates invalid objects when the control plane Machine and compute MachineSet manifests are not deleted during installation on vSphere using user provided infrastructure (UPI). The invalid objects trigger the 'MachineWithNoRunningPhase' alarms.\n","reason":"Starting with OpenShift 4.4, the OpenShift installer generates control plane Machines and compute MachineSets manifests for IPI installation on vSphere. If administrators do not delete these manifests when doing UPI installation on vSphere, the OpenShift installer creates invalid Machine and MachineSet objects which trigger the alarms.\n\nPlease review [Alarms for 'Machine is in phase' when deploying OpenShift Container Platform 4.4 on VMware](https://access.redhat.com/solutions/5086271) for more information.\n","resolution":"**Option 1:** If feasible, Red Hat recommends that you redeploy the cluster with the folowing modification of the installation procedure:\n\n1. Generate the Kubernetes manifests as instructed by the documentation:\n~~~\n$ openshift-install create manifests --dir=\u003cinstallation_directory\u003e\n~~~\n\n2. *NEW:* Delete the Machine and MachineSet manifests.\n~~~\n$ cd \u003cinstallation_directory\u003e\n$ find . -name '*machineset*' -o -name '*master-machine*'\n$ rm -f openshift/99_openshift-cluster-api_master-machines-*.yaml openshift/99_openshift-cluster-api_worker-machineset-*.yaml\n~~~\n\n3. Continue with creating the ignition files as instructed by the documentation:\n~~~\n$ openshift-install create ignition-configs --dir=\u003cinstallation_directory\u003e\n~~~\n\nSee also the [Remove Machines and MachineSets](https://github.com/openshift/installer/blob/master/docs/user/vsphere/install_upi.md#remove-machines-and-machinesets) step in the OpenShift installer documentation for installation on vSphere using user provided infrastructure (UPI).\n\n**Option 2:** If redeploying the cluster is not feasible, RedHat recommends that you remove the invalid control plane Machine and compute MachineSet objects as a workaround.\n\n*This workaround works only for OpenShift 4.4. Do not apply this workaround on OpenShift 4.5 and higher.*\n\n~~~\n$ oc delete machine unnecessary_machine_name_xxx\n$ oc delete machinesets unnecessary_machinesets_name_xxx\n~~~\n","more_info":"","plugin":{"name":"OpenShift cluster reports 'MachineWithNoRunningPhase' alarms when invalid control plane Machines and compute MachineSets are created during installation on vSphere with user provided infrastructure (UPI)","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.vsphere_upi_machine_is_in_phase"},"error_keys":{"VSPHERE_UPI_MACHINE_WITH_NO_RUNNING_PHASE":{"generic":"The OpenShift installer creates invalid objects when the control plane Machine and compute MachineSet manifests are not deleted during installation on vSphere using user provided infrastructure (UPI). The invalid objects trigger the 'MachineWithNoRunningPhase' alarms.\n","metadata":{"condition":"The cluster is running on vSphere and Hit the metrics alert \"MachineWithNoRunningPhase\"","description":"OpenShift cluster reports 'MachineWithNoRunningPhase' alarms when invalid control plane Machines and compute MachineSets are created during installation on vSphere with user provided infrastructure (UPI)","impact":"Invalid Configuration","likelihood":3,"publish_date":"2020-12-03 12:00:00","status":"active","tags":["openshift","fault_tolerance"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Etcd members are being incorrectly marked as unheathy due to a known bug which gives false alarms in the form of events in the `openshift-etcd-operator` project.","reason":"On this OCP 4 cluster, a known bug causes false events in the `openshift-etcd-operator` operator. These events state that the etcd members are unhealthy while they are healthy actually.","resolution":"Red Hat recommends that you upgrade the OpenShift cluster to a version 4.4.7 or above.","more_info":"For more in-depth information check the following:\n1. [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1832986).\n1. [KCS](https://access.redhat.com/solutions/5070671)","plugin":{"name":"The `openshift-etcd-operator` project gives false alarm as `UnhealthyEtcdMember` and `EtcdMemberDegraded`. No Impact on Cluster.","node_id":"5070671","product_code":"OCP4","python_module":"ccx_rules_ocp.external.bug_rules.bug_1832986"},"error_keys":{"BUGZILLA_BUG_1832986":{"generic":"Due to a known bug in OpenShift \"4.4.3\", \"4.4.4\", \"4.4.5\" and \"4.4.6\", the `EtcdMemberDegraded` and `UnhealthyEtcdMember` events occur in the `openshift-etcd` project which are false positive.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/5070671)\n","metadata":{"condition":"The OpenShift cluster version is \"4.4.3\", \"4.4.4\", \"4.4.5\" or \"4.4.6\"","description":"The `openshift-etcd-operator` project will get events of `EtcdMemberDegraded` and `UnhealthyEtcdMember` messages.","impact":"Diagnostics Failure","likelihood":2,"publish_date":"2020-05-07 15:08:00","status":"active","tags":["fault_tolerance","openshift","etcd"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"The rule checks if there have been *aborted updates* since the last successfully completed update.\n\nAn aborted update has an entry in the cluster version history that\n    - has 'Partial' state\n    - has defined 'completionTime'\n\n","reason":"There have been aborted updates since the last successfully completed update:\n\n{{~ pydata.aborted_updates :entry }}\n    Version '{{=entry[\"version\"]}}'\n    - State          : {{=entry[\"state\"]}}\n    - Started time   : {{=entry[\"started_time\"]}}\n    - Completion time: {{=entry[\"completion_time\"]}}\n    - Verified       : {{=entry[\"verified\"]}}\n{{~}}\n","resolution":"","more_info":"","plugin":{"name":"","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.version_retarget"},"error_keys":{"ABORTED_UPDATES_IN_RECENT_HISTORY":{"generic":"There are retargeted partial updates.","metadata":{"condition":"There have been aborted updates after the last successfully completed update","description":"Support may get problematic if the cluster state is a result of an aborted update","impact":"Compatibility Error","likelihood":3,"publish_date":"2020-02-20 15:25:00","status":"active","tags":["pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"This is an auxiliary health check used in internal RedHat test. Contact RedHat if you did not expect to see this health check, please.\n","reason":"This health check hit because the following pods have a specific label:\n\nProject: `{{=pydata.namespace}}`\n\nLabel: `{{=pydata.label_key}}={{=pydata.label_value}}`\n\nPods: {{~pydata.pods:pod}}\n- `{{=pod}}`{{~}}\n","resolution":"If you did not expect to see this health check, Red Hat recommends that you \n\n1. Inform RedHat about this incident via a support case.\n\n2. Remove the `{{=pydata.label_key}}={{=pydata.label_value}}` label from all pods in the `{{=pydata.namespace}}` project:\n   ```\n   # oc project {{=pydata.namespace}}\n   {{~pydata.pods:pod}}# oc label pod {{=pod}} {{=pydata.label_key}}-\n   {{~}}\n   ```\n","more_info":"","plugin":{"name":"Auxiliary rule used in CCXDEV end-to-end tests","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.ccxdev_auxiliary_rule"},"error_keys":{"CCXDEV_E2E_TEST_RULE":{"generic":"This is an auxiliary health check used in internal RedHat test. Contact RedHat if you did not expect to see this health check, please.\n","metadata":{"condition":"Any pod in the \"openshift-insights\" project has the \"ccxdev=make-e2e-test-rule-hit\" label","description":"Auxiliary health check used in internal RedHat tests","impact":"","likelihood":1,"publish_date":"2021-01-11 12:00:00","status":"active","tags":["openshift","service_availability"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"When a node is not given the minimum resource requirements then errors could occur with services running on said node, due to the lack of resources. The types of errors vary.","reason":"Node{{?pydata.nodes.length\u003e1}}s{{?}} not meeting the minimum requirements:\n{{~ pydata.nodes :node }}\n1. {{=node[\"name\"]}}\n  * Roles: {{=node[\"roles\"]}}{{?node.memory}}\n  * Minimum memory requirement is {{=node[\"memory_req\"]}}, but the node is configured with {{=node[\"memory\"]}}.{{?}}{{?node.cpu}}\n  * Minimum cpu requirement is {{=node[\"cpu_req\"]}}, but the node is configured with {{=node[\"cpu\"]}}.{{?}}{{~}}\n","resolution":"Red Hat recommends that you configure your nodes to meet the minimum resource requirements.\n\nMake sure that:\n\n{{~ pydata.nodes :node }}\n1. Node {{=node[\"name\"]}} ({{=node[\"roles\"]}}){{?node[\"memory\"]}}\n   * Has enough memory, minimum requirement is {{=node[\"memory_req\"]}}. Currently its only configured with {{=node[\"memory\"]}}GB.{{?}}{{?node.cpu}}\n   * Has enough allocatable cpu, minimum requirement is {{=node[\"cpu_req\"]}}. Currently its only configured with {{=node[\"cpu\"]}}.{{?}}{{~}}\n","more_info":"For more information about the minimum resource requirements for node, refer to [Minimum resource requirements](https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html#minimum-resource-requirements_installing-bare-metal)","plugin":{"name":"Unpredictable behavior when node doesn't meet the minimum resource requirements.","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.nodes_requirements_check"},"error_keys":{"NODES_MINIMUM_REQUIREMENTS_NOT_MET":{"generic":"Minimum resource requirements for Openshift 4 Nodes are not met, which could cause unexpected behavior.\n\n[Knowledgebase Article](https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html#minimum-resource-requirements_installing-bare-metal)\n","metadata":{"condition":"Node(s) doesn't meet the minimum resource requirements","description":"OCP node could behave unexpectedly when it doesn't meet the minimum resource requirements","impact":"Cluster Availability","likelihood":3,"publish_date":"2019-10-29 15:00:00","status":"active","tags":["openshift","configuration","performance","pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"OpenShift cluster can get damaged when an operator is configured to 'Unmanaged' state.","reason":"{{?pydata.unmanaged_operators.length \u003e 1}}The following operators have the `managementState` parameter set to `Unmanaged`. {{??}}The following operator has the `managementState` parameter set to `Unmanaged`. {{?}}Some Operators might not support this management state as it might damage the cluster and require manual recovery.\n\n**Operator Name:**\n{{~pydata.unmanaged_operators:item}}\n- {{=item}}\n{{~}}\n","resolution":"Red Hat recommends that you configure the operator parameter `managementState` to `Managed`.\n\nExample:\n{{~pydata.unmanaged_operators:item}}\n~~~\n# oc patch oc/{{=item}} --type='merge' -p '{\"Spec\":{\"managementState\": \"Managed\"}}'\n~~~\n{{~}}\n","more_info":"","plugin":{"name":"OpenShift cluster can get damaged when an operator is configured to 'Unmanaged' state","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.operator_unmanaged"},"error_keys":{"OPERATOR_UNMANAGED":{"generic":"OpenShift cluster can get damaged when an operator is configured to 'Unmanaged' state.","metadata":{"condition":"there is operator's managementState been set to 'Unmanaged'","description":"OpenShift cluster can get damaged when an operator is configured to 'Unmanaged' state","impact":"Cluster Availability","likelihood":2,"publish_date":"2021-01-11 16:00:00","status":"active","tags":["service_availability","openshift","operator"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"With the redesign of OpenShift 4.x and performance improvements of components, it is no longer necessary to have more than 3 control plane nodes and adding more control plane nodes increases complexity of the system.","reason":"On the OpenShift cluster, there are {{=pydata.replica_count}} control plane nodes. Currently, the supported replica count for control plane nodes in the OpenShift cluster is 3.\n\n{{?pydata.replica_count \u003e 3}}\nWith {{=pydata.replica_count}} members, the quorum has to pass traffic to all the members and wait for consensus from more members before committing a change. This introduces latency to etcd, which in turn increases latency for the API server and cluster operators impacting cluster's overall performance.\n{{??pydata.replica_count \u003c 3}}\nWith {{=pydata.replica_count}} members, the quorum for etcd members is not being met.\n{{?}}\n","resolution":"{{?pydata.replica_count \u003e 3}}\nRed Hat recommends to scale down the control plane to exactly 3 nodes.\n{{??pydata.replica_count \u003c 3}}\nRed Hat recommends to scale up the control plane to exactly 3 nodes.\n{{?}}\n\nIf feasible, cluster re-installation is also an option to meet the supportability criteria.","more_info":"Please review the [Official Documentation](https://docs.openshift.com/container-platform/{{=pydata.version}}/architecture/control-plane.html#defining-masters_control-plane) for more information.\n","plugin":{"name":"Scaling control plane nodes would basically scale the etcd nodes and it is currently not supported.","node_id":"4833531","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.control_plane_replicas"},"error_keys":{"CONTROL_PLANE_NODE_REPLICAS":{"generic":"OpenShift cluster with control plane node replicas other than 3 makes the cluster unsupported.\n","metadata":{"condition":"Control Plane node replica count is not equal to 3","description":"OpenShift cluster with more or less than 3 control plane node replicas is not supported by Red Hat.","impact":"Support Unavailable","likelihood":4,"publish_date":"2021-01-22","status":"active","tags":["openshift","etcd","performance"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Some control plane certificates have expired.\n","reason":"The following control plane certificates have expired:\n\n{{~ pydata.expired :cert }}\n* {{=cert.name}} (expired on {{=cert.not_valid_after}})\n{{~}}\n","resolution":"Red Hat recommends you to follow steps in [Recovering from expired control plane certificates](https://docs.openshift.com/container-platform/{{=pydata.version}}/backup_and_restore/disaster_recovery/scenario-3-expired-certs.html).\n\nIf you know a better solution for this issue, please let us know!\n","more_info":"","plugin":{"name":"Some control plane certificates have expired.","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.certificates_expired"},"error_keys":{"CERTIFICATES_EXPIRED":{"generic":"Some control plane certificates have expired.\n","metadata":{"condition":"There are expired control plane certificates.","description":"Control plane certificates have expired","impact":"Cluster Availability","likelihood":4,"publish_date":"2020-01-09 09:21:00","status":"active","tags":["pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Cluster is running with a 3rd party SDN plugin that will encounter support limitations.\n","reason":"This cluster is running with 3rd party SDN plugin `{{=pydata.network_type}}`, which will encounter support limitations.\n","resolution":"Red Hat recommends that you migrate to a supported SDN plugin.\n\n**Supported SDN plugin:**\n - *OpenShiftSDN*\n - *OVNKubernetes*\n - *Kuryr*\n\nFor update steps, please refer to this doc:\n{{? ['4.0', '4.1', '4.2', '4.3', '4.4', '4.5'].indexOf(pydata.current_version) \u003e= 0}}\n[Migrating to the OVN-Kubernetes default CNI network provider](https://docs.openshift.com/container-platform/4.5/networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.html)\n{{??}}\n[Migrating to the OVN-Kubernetes default CNI network provider](https://docs.openshift.com/container-platform/{{=pydata.current_version}}/networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.html)\n{{?}}\n\n","more_info":"","plugin":{"name":"Cluster is running with 3rd party SDN plugin","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.unsupport_sdn_plugin"},"error_keys":{"UNSUPPORT_SDN_PLUGIN":{"generic":"Cluster is running with a 3rd party SDN plugin that will encounter support limitations.\n","metadata":{"condition":"3rd party sdn plugin is used","description":"Cluster is running with 3rd party SDN plugin","impact":"Unsupported Functionality","likelihood":1,"publish_date":"2020-08-06 12:00:00","status":"active","tags":["sbr_shift","openshift","sdn","fault_tolerance","pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"If a machine is 'Degraded' or not 'Updated', then it's having an issue that needs further investigation.\n","reason":"The following machines are experiencing issues:\n{{~ pydata.machines :mac }}\n* {{=mac[\"name\"]}}\n  {{? mac[\"degraded\"][\"status\"] === true }}* Degraded: {{=mac[\"degraded\"][\"status\"]}}\n    * Message: {{=mac[\"degraded\"][\"message\"]}}\n    * Reason: {{=mac[\"degraded\"][\"reason\"]}}\n  {{?}}{{? mac[\"updated\"][\"status\"] === true }}* Updated : {{=mac[\"updated\"][\"status\"]}}\n    * Message: {{=mac[\"updated\"][\"message\"]}}\n    * Reason: {{=mac[\"updated\"][\"reason\"]}}\n  {{?}}{{? mac[\"render_degraded\"][\"status\"] === true }}* RenderDegraded: {{=mac[\"render_degraded\"][\"status\"]}}\n    * Message: {{=mac[\"render_degraded\"][\"message\"]}}\n    * Reason: {{=mac[\"render_degraded\"][\"reason\"]}}\n  {{?}}{{? mac[\"node_degraded\"][\"status\"] === true }}* NodeDegraded: {{=mac[\"node_degraded\"][\"status\"]}}\n    * Message: {{=mac[\"node_degraded\"][\"message\"]}}\n    * Reason: {{=mac[\"node_degraded\"][\"reason\"]}}{{?}}{{~}}","resolution":"","more_info":"","plugin":{"name":"","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.machine_pool_check"},"error_keys":{"MACHINE_POOL_NOT_OK":{"generic":"Machines are in not desired state.","metadata":{"condition":"Machines are 'Degraded' or not in 'Updated' state","description":"One or more machines are 'Degraded' or not 'Updated'","impact":"Application Failure","likelihood":3,"publish_date":"2020-01-23 08:25:00","status":"active","tags":["service_availability","openshift","pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"One or more pods are in a one of the following failing states:\n- Containers not ready\n- Not initialized\n- Not ready\n- Not scheduled\n","reason":"The following pods are experiencing issues:\n{{~ pydata.pods :pod }}\n    {{=pod[\"name\"]}}\n    Ready: {{=pod[\"ready\"][\"status\"]}}\n    Initialized: {{=pod[\"initialized\"][\"status\"]}}\n    Pod_scheduled: {{=pod[\"pod_scheduled\"][\"status\"]}}\n    Containers_ready: {{=pod[\"containers_ready\"][\"status\"]}}\n{{~}}","resolution":"","more_info":"","plugin":{"name":"","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.pods_check"},"error_keys":{"POD_HEALTHY":{"generic":"All pods are in a desired state.\n","metadata":{"condition":"Pods are healthy","description":"All pods are in a desired state","impact":"","likelihood":1,"publish_date":"2019-11-03 08:25:00","status":"active","tags":["pssa"]},"reason":"","HasReason":false},"POD_ISSUE":{"generic":"Pod(s) are in a failing state.\n","metadata":{"condition":"Pod(s) are in a failing state","description":"If an pod is in Containers not ready/Not initialized/Not ready/Not scheduled state, then its having an issue that needs further investigation","impact":"Application Failure","likelihood":3,"publish_date":"2019-11-03 08:25:00","status":"active","tags":["openshift","pod","incident","pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Pods falling to create the application will restart and try to create it again, if this happens multiple times, without success, it will result in a CrashLoopBackOff state.\n","reason":"Pods in a CrashLoopBackOff state:\n{{~ pydata.pods :pod }}\n    {{=pod[\"pod\"]}}\n    - Restart count: {{=pod[\"restarts\"]}}\n    - Reason: {{=pod[\"reason\"]}}\n    - Message: {{=pod[\"msg\"]}}\n{{~}}\n","resolution":"Red Hat recommends that you to consider the following:\n\nThis can happen when:\n* The application running in your container panics and dies;\n* Liveness probe failed;\n* Readiness probe failed; (Increasing initialDelaySeconds: helps);\n* The process started inside the image isn't a long running process and finds no tty and the container just exits and gets restarted repeatedly, which is a crash loop as far as OpenShift is concerned;\n* The container image needs to run as a specific user, in which case you will need to relax security in your cluster so that images are not forced to run as a pre-allocated UID. See documentation on Managing Security Context Constraints (4.x version);\n* A base image is being used and there is no long running command that would keep this container alive. Example: intended for use in the s2i process, but being used as-is.\n* If unable to determine why the pod keeps crashing. Try the following:\n\nPossible solution:\n1. If using a Dockerfile, take a look at the ENTRYPOINT;\n1. Take a look for Command and Args attributes while describing the container.\n~~~\n# oc describe pod \u003cpod-name\u003e\n~~~\n1. Try running your image locally (using docker or podman) to rule out if OpenShift is the cause;\n1. Look at the previous pod logs of an instance of the container:\n~~~\n# oc logs \u003cpod\u003e -p\n~~~\n1. If no logs appear, debug the pod. This should create a copy of the pod and provide a shell:\n~~~\n# oc debug \u003cpod\u003e\n~~~","more_info":"For more information about the error refer to:\n * [CrashLoopBackOff status for Openshift Pod](https://access.redhat.com/solutions/2137701)","plugin":{"name":"Not possible to create the application as the pod keeps on crashing resulting in CrashLoopBackOff state.","node_id":"2137701","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.pods_crash_loop_check"},"error_keys":{"POD_CRASHLOOP_ISSUE":{"generic":"Pod(s) in a \"CrashLoopBackOff\" state.","metadata":{"condition":"Pod(s) in a \"CrashLoopBackOff\" state","description":"Application creation failure when pods were found in a \"CrashLoopBackOff\" state","impact":"Application Crash","likelihood":3,"publish_date":"2020-01-03 10:00:00","status":"active","tags":["openshift","pod","incident","pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Clusteroperator is degraded with \"NodeInstallerDegraded\" in reason.\n","reason":"Clusteroperator{{?pydata.degraded_operators.length\u003e1}}s{{?}} degraded with NodeInstallerDegraded in reason:\n\n{{~ pydata.degraded_operators :operator }}\n**Cluster-operator:**  **{{=operator[\"name\"]}}**\n- *Reason:* {{=operator[\"degraded\"][\"reason\"]}}\n- *Message:* {{=operator[\"degraded\"][\"message\"]}}\n- *Last transition*: {{=operator[\"degraded\"][\"last_trans_time\"]}}\n\n{{~}}\n","resolution":"You may be hitting a [known bug](https://bugzilla.redhat.com/show_bug.cgi?id=1723966) and Red Hat recommends that you complete the following steps:\n\n{{~ pydata.degraded_operators :operator }}\n{{? operator[\"name\"] == \"kube-apiserver\"}}\n- For the **kube-apiserver** clusteroperator do:\n~~~\noc patch kubeapiserver/cluster --type merge -p \"{\\\"spec\\\":{\\\"forceRedeploymentReason\\\":\\\"Forcing new revision with random number $RANDOM to make message unique\\\"}}\"\n~~~\n{{?}}\n{{? operator[\"name\"] == \"kube-controller-manager\"}}\n- For the **kube-controller-manager** clusteroperator do:\n~~~\noc patch kubecontrollermanager/cluster --type merge -p \"{\\\"spec\\\":{\\\"forceRedeploymentReason\\\":\\\"Forcing new revision with random number $RANDOM to make message unique\\\"}}\"\n~~~\n{{?}}\n{{? operator[\"name\"] == \"kube-scheduler\"}}\n- For the **kube-scheduler** clusteroperator do:\n~~~\noc patch kubescheduler/cluster --type merge -p \"{\\\"spec\\\":{\\\"forceRedeploymentReason\\\":\\\"Forcing new revision with random number $RANDOM to make message unique\\\"}}\"\n~~~\n{{?}}\nThen wait several minutes and check if the operator is no longer degraded or progressing. If it is still degraded and the same error message is shown, retry (the race condition can be triggered again). If the error message is different or some retries do not make any improvement, open a support case to get further assistance.\n\nIf this solution solves your issue, but you are interested in tracking the definitive resolution of the bug, you can open a support case to do that as well.\n{{~}}","more_info":"","plugin":{"name":"Clusteroperator is degraded when the installer pods are removed too soon during upgrade","node_id":"4849711","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.node_installer_degraded"},"error_keys":{"NODE_INSTALLER_DEGRADED":{"generic":"Clusteroperator is degraded with \"NodeInstallerDegraded\" in status.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4849711)\n","metadata":{"condition":"One of these operators, \"kube-apiserver\", \"kube-controller-manager\" or \"kube-scheduler\" is degraded with \"NodeInstallerDegraded\" in status.","description":"Clusteroperator is degraded when the installer pods are removed too soon during upgrade","impact":"Cluster Availability","likelihood":4,"publish_date":"2020-03-06 12:00:00","status":"active","tags":["openshift","service_availability","pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Updates can fail because the next update is not available yet. If the user is in a stable upgrade channel they get certain updates later or they might skip one lesser release because these updates are not considered stable enough. Users sometimes hear about an update so they try to download it, but it will fail and cause this issue.\n","reason":"Version update {{=pydata.installed_version}} is not available on the upgrade channel {{=pydata.upgrade_channel}}.\n","resolution":"**Updating a cluster by using the CLI**\n\nIf updates are available, you can update your cluster by using the OpenShift CLI (oc).\nYou can find information about available OpenShift Container Platform advisories and updates in the [errata section](https://access.redhat.com/downloads/content/290/ver=4.3/rhel---8/4.3.0/x86_64/product-errata) of the Customer Portal.\n\n**Prerequisites**\n\n* Install the version of the OpenShift Command-line Interface (CLI), commonly known as `oc`, that matches the version for your updated version.\n* Log in to the cluster as user with `cluster-admin` privileges.\n* Install the `jq` package.\n\n**Procedure**\n\n1. Ensure that your cluster is available:\n  * `oc get clusterversion`\n2. Review the current update channel information and confirm that your channel is set to stable-:\n  * `oc get clusterversion -o json|jq \".items[0].spec\"`\n  * **IMPORTANT:** For production clusters, you must subscribe to a stable-* channel.\n3. View the available updates and note the version number of the update that you want to apply:\n  * `oc adm upgrade`\n  * To update to the latest version:\n    * `oc adm upgrade --to-latest=true 1`\n  * To update to a specific version:\n    * `oc adm upgrade --to=\u003cversion\u003e 1`\n","more_info":"For more information about upgrade channels, refer to [Upgrades phased roll out](https://access.redhat.com/articles/4495171)\n","plugin":{"name":"","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.update_not_yet_available_check"},"error_keys":{"UPDATE_MISSING_ERROR":{"generic":"Update is not yet available on current upgrade channel.\n","metadata":{"condition":"Update is not yet available on current upgrade channel","description":"Confusion because the clusterversion operator is in a failing state when the update is not available","impact":"Best Practice","likelihood":3,"publish_date":"2020-03-13 13:42:00","status":"active","tags":["pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"OpenShift versions prior to 4.2.36, 4.3.25, 4.4.8, 4.5.1, or 4.6.1 are vulnerable to CVE-2020-8555: Compromised node could escalate to cluster level privileges.\n","reason":"* The cluster is running OpenShift {{=pydata.current_version}}.\n","resolution":"Red Hat recommends that you upgrade the OpenShift cluster to a more recent version. \n\nFor more information, see the *Updating a cluster within a minor version from the web console* section of the documentation of the OpenShift version:\n\n* [OpenShift {{=pydata.minor_version}}](https://docs.openshift.com/container-platform/{{=pydata.minor_version}}/updating/updating-cluster.html)\n","more_info":"* For more information about the flaw, see [CVE-2020-8555](https://access.redhat.com/security/cve/CVE-2020-8555).\n* The Customer Portal page for the [Red Hat Security Team](https://access.redhat.com/security/) contains more information about policies, procedures, and alerts for Red Hat Products.\n* The Security Team also maintains a frequently updated blog at [securityblog.redhat.com](https://securityblog.redhat.com).\n","plugin":{"name":"CVE-2020-8555: compromised node could escalate to cluster level privileges","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.security.CVE_2020_8555_kubernetes"},"error_keys":{"CVE_2020_8555_KUBERNETES":{"generic":"The running OpenShift version is vulnerable to CVE-2020-8555: Compromised node could escalate to cluster level with Moderate security impact.\n\nThis flaw allows an attacker who can intercept requests on a compromised node to redirect those requests, along with their credentials, to perform actions on other endpoints that trust those credentials (including other clusters), allowing for escalation of privileges.\n","metadata":{"condition":"The running OpenShift version is vulnerable to CVE-2020-8555","description":"CVE-2020-8555: compromised node could escalate to cluster level privileges","impact":"Privilege Escalation","likelihood":2,"publish_date":"2021-01-08 12:00:00","status":"active","tags":["security","openshift","cve"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Continuous creation of lib-bucket-provisioner InstallPlan is overloading the etcd and impacting the performance of the OCP cluster.\n","reason":"The count of InstallPlans for the lib-bucket-provisioner operator is {{pydata.install_plans}}, which suggests that it is growing continuously.\n","resolution":"Red Hat recommends that you upgrade the OpenShift cluster to a more recent version.\n\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n\n* [OpenShift {{=pydata.ocp_branch}}](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/updating/updating-cluster-between-minor.html)\n","more_info":"","plugin":{"name":"The lib-bucket-provisioner operator will fail in the process of upgrading","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.lib_bucket_provisioner_check"},"error_keys":{"LIB_BUCKET_PROVISIONER_INSTALL_PLANS_ISSUE":{"generic":"When the count of InstallPlans for the lib-bucket-provisioner operator starts increasing continuously due to a bug in OpenShift Container Platform 4, the number of objects eventually decreases etcd performance.\n","metadata":{"condition":"The number of the lib-bucket-provisioner install plans is more than 5 and the service account 'lib-bucket-provisioner' does not exist in the openshift-storage namespace.","description":"The etcd performace will degrade when the lib-bucket-provisioner operator keeps creating new install plans during a failed upgrade","impact":"Cluster Availability","likelihood":3,"publish_date":"2020-11-18 12:00:00","status":"active","tags":["openshift","configuration","performance"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"One or more nodes are in a one of the following pressured states:\n- Not ready\n- Memory pressure\n- Disk pressure\n- PID pressure\n","reason":"The following nodes are experiencing pressure:\n{{~ pydata.pressured_nodes :node }}\n    {{=node[\"name\"]}}\n    {{~ node[\"issues\"] :pressure }}\n    Issue          : {{=pressure[0]}}\n    Reason         : {{=pressure[1][\"reason\"]}}\n    Message        : {{=pressure[1][\"message\"]}}\n    LastTransition : {{=pressure[1][\"last_trans_time\"]}}\n    {{~}}\n{{~}}\n","resolution":"","more_info":"","plugin":{"name":"","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.nodes_pressure_check"},"error_keys":{"NODE_PRESSURE":{"generic":"Nodes are under pressure.","metadata":{"condition":"Nodes are under pressure","description":"Nodes under pressure can cause unexpected behavior that needs investigation","impact":"OpenShift Performance Loss","likelihood":2,"publish_date":"2020-02-03 08:25:00","status":"active","tags":["pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Cluster monitoring operator pods are stuck in pending status due to an OpenShift bug.","reason":"This OpenShift cluster is running in the version {{=pydata.version}}. Due to a bug in this version, the following cluster monitor operator pods are stuck in pending status with an error message `container has runAsNonRoot and image has non-numeric user (nobody)`.\n\n**POD Name:**\n{{~pydata.issue_pods:item}}\n- {{=item}}\n{{~}}\n","resolution":"Red Hat recommends that you upgrade to 4.6.12 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.6/updating/updating-cluster-between-minor.html) for the steps.","more_info":"For more in-depth information please check the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1906836).","plugin":{"name":"Cluster monitoring operator pods stuck in pending status due to an OpenShift bug","node_id":"5663021","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.cmo_container_run_as_non_root"},"error_keys":{"CMO_CONTAINER_RUN_AS_NON_ROOT":{"generic":"Cluster monitoring operator pods are stuck in pending status due to an OpenShift bug.","metadata":{"condition":"cluster version is in [4.6.8, 4.6.9, 4.6.10, 4.6.11]; there exists the alert 'TargetDown\" for the job \"cluster-monitoring-operator\"; the CVO pod has an error message contains 'non-root'","description":"Cluster monitoring operator pods are stuck in pending status due to an OpenShift bug","impact":"Cluster Availability","likelihood":4,"publish_date":"2021-02-02 16:00:00","status":"active","tags":["service_availability","openshift","operator"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"When there is no ContainerRuntimeConfig, containers leak sensitive information about the available disk space of the underlying host.\n","reason":"* There {{?Object.keys(pydata.mcps).length\u003e1}}are machinepools that don't have{{??}}a machinepool that doesn't have{{?}} the ContainerRuntimeConfig:\n{{ for (var index in pydata.mcps) { }}\n - **{{=pydata.mcps[index]}}**\n{{?}}\n\nThat allows containers to show the available disk space of the underlying host.\n\nPlease review [Setting the default max container root partition size for overlay with cri-o](https://access.redhat.com/solutions/5216861) for more information.\n","resolution":"Red Hat recommends that you perform the following steps:\n1. Create the ContainerRuntimeConfig CRD, like the following example:\n~~~\n# cat \u003c\u003c EOF| oc create -f -\napiVersion: v1\nkind: ContainerRuntimeConfig\nmetadata:\n name: xxx\nspec:\n machineConfigPoolSelector:\n   matchLabels:\n     match_key_xxx: match_value_yyy\n...\nEOF\n~~~\n\n2. To apply the new container runtime configuration to your nodes, add the matchLabels name you set in the ContainerRuntimeConfig CRD to the corresponding machineconfigpool, like the following example:\n~~~\n# oc edit machineconfigpool node_name\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfigPool\nmetadata:\n  labels:\n    match_key_xxx: match_value_yyy\n~~~\n\nFor more information, see the Red Hat document [Setting the default max container root partition size for overlay with cri-o](https://access.redhat.com/solutions/5216861)\n","more_info":"","plugin":{"name":"The root partition of each container shows all of the available disk space of the underlying host","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.container_max_root_partition_size"},"error_keys":{"CONTAINER_ROOT_PARTITION_SIZE":{"generic":"If there is no ContainerRuntimeConfig, then the root partition of a container shows the available disk space of the underlying host.\n","metadata":{"condition":"There is no ContainerRuntimeConfig for relevant MachineConfigPool","description":"Containers leak available disk space of the underlying host when there is no ContainerRuntimeConfig","impact":"Best Practice","likelihood":4,"publish_date":"2020-11-17 11:10:00","status":"active","tags":["openshift","security"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Upgrade failed if a file from a node is modified outside the OpenShift context (i.e. manually running commands inside a node). The machine-config-operator daemon finds out that the file isn't what it's supposed to be and marks the node as Degraded.\n","reason":"Found machines upgrade failed due to mismatch file:\n\n{{~pydata.machines:item}}\n**Name:** {{=item[\"name\"]}}\n- *Reason:* {{=item[\"reason\"]}}\n- *Message:* {{=item[\"message\"]}}\n- *Last_trans_time:* {{=item[\"last_trans_time\"]}}\n{{~}}\n\n**Mismatch file**:\n{{~pydata.files:item}}\n\n{{=item}}\n{{~}}\n\n","resolution":"1. Check the mismatch files, copy the original file from a node that is working and make sure they have the same checksum.\n\n   ~~~\n   {{~pydata.files:item}} {{=item}} {{~}}\n   ~~~\n\n1. Wait for the node to be Available again.\n\n1. Check if the machine-config-operator is working again.\n","more_info":"","plugin":{"name":"","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.mcp_unexpected_on_disk_state_content_mismatch"},"error_keys":{"MCP_UNEXPECTED_ON_DISK_STATE_CONTENT_MISMATCH":{"generic":"Upgrade failed when there are files from a node modified outside the OpenShift context.\n","metadata":{"condition":"Upgrade failed due to mismatch files","description":"Upgrade failed when there are mismatch files existing","impact":"OpenShift Upgrade Failure","likelihood":3,"publish_date":"2020-07-10 12:00:00","status":"active","tags":null},"reason":"","HasReason":false}},"HasReason":true},{"summary":"When container runtime versions are inconsistent, nodes behave differently in some cases, which can lead to unexpected issues.\n","reason":"There {{?pydata.nodes_with_different_version.length \u003e 1}}are{{?}} {{?pydata.nodes_with_different_version.length == 1}}is{{?}} {{=pydata.nodes_with_different_version.length}} cluster node{{?pydata.nodes_with_different_version.length \u003e 1}}s{{?}} with container runtime version that is different from that of the master node. The correct package version should be `{{=pydata.expected_package}}`.\n\n{{~pydata.nodes_with_different_version:item}}\n**Node:** {{=item[\"node\"]}}\n- *Roles:* {{=item[\"roles\"]}}\n- *Version:* {{=item[\"version\"]}}\n\n{{~}}\n","resolution":"Red Hat recommends that you update container runtime version accordingly:\n\n1. Update container engine package to `{{=pydata.expected_package}}`.\n\n1. Reboot node.\n","more_info":"","plugin":{"name":"Nodes behave differently when container runtime versions are inconsistent","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.nodes_container_runtime_version_check"},"error_keys":{"NODES_CONTAINER_RUNTIME_VERSION":{"generic":"When container runtime versions are inconsistent, nodes behave differently in some cases, which can lead to unexpected issues.\n","metadata":{"condition":"Container runtime versions are inconsistent among nodes","description":"Issues are likely to occur when container runtime versions are inconsistent among nodes","impact":"Cluster Availability","likelihood":2,"publish_date":"2020-07-28 12:00:00","status":"active","tags":["sbr_shift","openshift","container","pod","service_availability","pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Clusterversion operator is not respecting the cluster wide proxy settings, due to a bug, which causes the operator to get stuck during a version update. This will cause the version update to fail.","reason":"On this OCP 4 cluster, a cluster wide proxy is set. Due to a bug, the CVO is not using the proxy. This will lead to a upgrade failure.","resolution":"Red Hat recommends that you set the proxy manually as a workaround:\n~~~\n# oc -n openshift-cluster-version set env deploy cluster-version-operator HTTP_PROXY=xxx HTTPS_PROXY=xxx NO_PROXY=xxx\n~~~\n","more_info":"For more in-depth information check the following:\n1. [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1766907).\n1. [KCS](https://access.redhat.com/solutions/4631541)","plugin":{"name":"OpenShift 4 upgrade fails when cluster-wide proxy is set.","node_id":"4631541","product_code":"OCP4","python_module":"ccx_rules_ocp.external.bug_rules.bug_1766907"},"error_keys":{"BUGZILLA_BUG_1766907":{"generic":"Due to a known bug in OpenShift 4 the Cluster Version Operator is not respecting the cluster-wide proxy settings.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4631541)\n","metadata":{"condition":"Proxy is set and the clusterversion operator is in a Progressing state","description":"The OpenShift cluster will experience upgrade failure when the cluster wide proxy is configured due to a bug","impact":"Cluster Availability","likelihood":2,"publish_date":"2020-01-17 11:10:00","status":"active","tags":["openshift","networking","service_availability"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Forcing a version updated can void support.","reason":"Forced versions:\n\n{{~ pydata.forced_versions :fv }}\n    Version '{{=fv[\"version\"]}}'\n    - State          : {{=fv[\"state\"]}}\n    - Started time   : {{=fv[\"started_time\"]}}\n    - Completion time: {{=fv[\"completion_time\"]}}\n    - Verified       : {{=fv[\"verified\"]}}\n{{~}}\n","resolution":"","more_info":"","plugin":{"name":"","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.version_forced"},"error_keys":{"FORCED_VERSION_UPDATES":{"generic":"Version in history is not verified.","metadata":{"condition":"Version in history is not verified","description":"Found forced version updates that can void support","impact":"Product Supportability","likelihood":3,"publish_date":"2020-02-20 08:25:00","status":"active","tags":["openshift","pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"When an OpenShift version reaches its End of Life, it is no longer eligible to receive maintenance updates, and technical support is limited only to aid upgrades to in-support versions of OpenShift 4. \n","reason":"","resolution":"Red Hat recommends that you upgrade the OpenShift cluster to the next Extended Update Support release.\n\nFor more information, about the Extended Update Support channel, see the product documentation:\n\n* [Understanding Upgrade Channels](https://docs.openshift.com/container-platform/{{=pydata.version_major}}.{{=pydata.version_minor}}/updating/updating-cluster-between-minor.html#understanding-upgrade-channels_updating-cluster-between-minor)\n* [Updating a cluster between minor versions](https://docs.openshift.com/container-platform/{{=pydata.version_major}}.{{=pydata.version_minor}}/updating/updating-cluster-between-minor.html)\n","more_info":"See also [Red Hat OpenShift Container Platform Life Cycle Policy](https://access.redhat.com/support/policy/updates/openshift#dates) for more information.\n\n","plugin":{"name":"The running OpenShift version with Extended Update Support is near or beyond its End of Life","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.ocp_version_end_of_life_eus"},"error_keys":{"OCP4XEUS_BEYOND_EOL":{"generic":"The running OpenShift version has reached its End of Life. It is no longer eligible to receive maintenance updates. Technical support is limited only to aid upgrades to in-support versions of OpenShift 4. \n","metadata":{"condition":"The running OpenShift EUS release is beyond its End of Life","description":"The running OpenShift version has reached its End of Life","impact":"Support Unavailable","likelihood":4,"publish_date":"2021-02-17 15:00:00","status":"active","tags":["openshift","service_availability","pssa"]},"reason":"The cluster is running OpenShift {{=pydata.version_full}} with Extended Update Support (EUS).\n\nOpenShift {{=pydata.version_major}}.{{=pydata.version_minor}} EUS has reached its End of Life on {{=pydata.eol_date}}.\n","HasReason":true},"OCP4XEUS_EOL_APPROACHING":{"generic":"The running OpenShift version will reach its End of Life in less than 3 months. It will no longer be eligible to receive maintenance updates. Technical support will be limited only to aid upgrades to in-support versions of OpenShift 4. \n","metadata":{"condition":"The running OpenShift EUS release will reach its End of Life in less than 3 months","description":"The running OpenShift version will reach its End of Life in less than 3 months","impact":"Support Unavailable","likelihood":1,"publish_date":"2021-02-17 15:00:00","status":"active","tags":["openshift","service_availability","pssa"]},"reason":"The cluster is running OpenShift {{=pydata.version_full}} with Extended Update Support (EUS).\n\nOpenShift {{=pydata.version_major}}.{{=pydata.version_minor}} EUS will reach its End of Life on {{=pydata.eol_date}}.\n","HasReason":true},"OCP4XEUS_EOL_IMMINENT":{"generic":"The running OpenShift version will reach its End of Life in less than 1 month. It will no longer be eligible to receive maintenance updates. Technical support will be limited only to aid upgrades to in-support versions of OpenShift 4. \n","metadata":{"condition":"The running OpenShift EUS release will reach its End of Life in less than 1 month","description":"The running OpenShift version will reach its End of Life in less than 1 month","impact":"Support Unavailable","likelihood":2,"publish_date":"2021-02-17 15:00:00","status":"active","tags":["openshift","service_availability","pssa"]},"reason":"The cluster is running OpenShift {{=pydata.version_full}} with Extended Update Support (EUS).\n\nOpenShift {{=pydata.version_major}}.{{=pydata.version_minor}} EUS will reach its End of Life on {{=pydata.eol_date}}.\n","HasReason":true}},"HasReason":false},{"summary":"Additional risks can occur when the master nodes are defined as MachineSets.\n","reason":"The following MachineSet{{?pydata.result.length \u003e 1}}s{{?}} include{{?pydata.result.length == 1}}s{{?}} master node. While this configuration does not provide obvious benefits, it does pose additional risks.\n\n{{~pydata.result:item}}\n**MachineSet Name**: {{=item}}\n{{~}}\n","resolution":"Red Hat recommends that you delete the affected machineset, replace master with a new healthy one.\n\nPlease refer to this [documentation](https://docs.openshift.com/container-platform/4.4/backup_and_restore/replacing-unhealthy-etcd-member.html#restore-replace-stopped-etcd-member_replacing-unhealthy-etcd-member) for detail information.\n","more_info":"","plugin":{"name":"Additional risks would occur possibly when having the masters defined as machinesets","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.master_defined_as_machinesets"},"error_keys":{"MASTER_DEFINED_AS_MACHINESETS":{"generic":"Additional risks can occur when the master nodes are defined as MachineSets.\n","metadata":{"condition":"there is master node defined as machineset","description":"Additional risks would occur possibly when having the masters defined as machinesets","impact":"Cluster Availability","likelihood":2,"publish_date":"2020-11-13 16:00:00","status":"active","tags":["openshift","configuration","service_availability"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"SDN behavior is not defined when two netnamespaces contain the same egress IP. For example, the OpenShift cluster might be dropping traffic.","reason":"On the OpenShift cluster, the following NetNamespaces contain the same egress IP. It is not a supported configuration and could cause OpenShift to drop outgoing traffic.\n\n{{ for (var key in pydata.netnamespaces) { }}\nNetNamespaces configured to the egress IP `{{=key}}`:\n  {{~pydata.netnamespaces[key]:item}}\n  -  {{=item}}\n  {{~}}\n{{ } }}","resolution":"Red Hat recommends that you configure different Egress IP for the following NetNamespaces.\n{{ for (var key in pydata.netnamespaces) { }}\nNetNamespaces configured to the egress IP `{{=key}}`:\n  {{~pydata.netnamespaces[key]:item}}\n  -  {{=item}}\n  {{~}}\n{{ } }}\nPlease refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/{{=pydata.ocp_version}}/networking/openshift_sdn/assigning-egress-ips.html) for the steps to configure the Egress IP for a project. \n\n","more_info":"","plugin":{"name":"SDN behavior is not defined when two NetNamespaces contain the same egress IP","node_id":"4102661","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.same_egress_ip_in_multiple_netnamespaces"},"error_keys":{"SAME_EGRESS_IP_IN_MULTIPLE_NETNAMESPACES":{"generic":"SDN behavior is not defined when two netnamespaces contain the same egress IP. For example, the OpenShift cluster might be dropping traffic.","metadata":{"condition":"two or more NetNamespaces contain the same egress IP.","description":"The OpenShift cluster drops traffic when two NetNamespaces contain the same egress IP","impact":"Cluster Availability","likelihood":3,"publish_date":"2021-02-18 14:00:00","status":"active","tags":["openshift","configuration","service_availability"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Certificates are not approved.","reason":"Certificates are not approved.\n\nList of unapproved certificates.\n\n{{~ pydata.unapproved :cert }}\n* {{=cert.name}}\n{{~}}","resolution":"Red Hat recommends you to follow steps in [approving the CSRs for your machines](https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html#installation-approve-csrs_installing-bare-metal).","more_info":"","plugin":{"name":"Some certificates are not approved.","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.certificates_not_approved"},"error_keys":{"CERTIFICATES_NOT_APPROVED":{"generic":"Certificates are not approved.","metadata":{"condition":"Certificate is not approved.","description":"Certificates are not approved.","impact":"Cluster Availability","likelihood":4,"publish_date":"2020-01-09 09:21:00","status":"active","tags":["pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"If a cluster admin changes any default SCC, cluster upgrade is prevented and error message appears.","reason":"The OCP-{{=pydata.desired}} update is blocked because default security context constraints (SCC) anyuid, hostaccess, hostmount-anyuid, hostnetwork, nonroot, privileged, or restricted have been modified\n\nUpgrading 4.3.8, 4.3.9, 4.3.10, 4.3.11, or 4.3.12 fails if security context constraints (SCC) are not the default.\n\nOCP 4.3.8 introduced a new check for modified or mutated default SCCs. If any of the SCCs anyuid, hostaccess, hostmount-anyuid, hostnetwork, nonroot, privileged, or restricted have been modified, upgrades to future releases are prevented. For more details see [BZ-1808602](https://bugzilla.redhat.com/show_bug.cgi?id=1808602) and [BZ-1810596](https://bugzilla.redhat.com/show_bug.cgi?id=1810596) from [Bug Fix Advisory RHBA-2020:0858](https://access.redhat.com/errata/RHBA-2020:0858).\n\nThis check is to ensure that environments with modified default SCCs could not be upgraded to 4.4 as changes or removal of the default SCCs could lead to unexpected behavior and system instability.\n\nOCP 4.3.13 ([Bug Fix Advisory RHBA-2020:1481](https://access.redhat.com/errata/RHBA-2020:1481)) relaxes this check and will no longer block the upgrade.\n\n","resolution":"OpenShift Container Platform (OCP) 4.3.13 will no longer block upgrades if the SCC is not the default.\n\nThe original issue raised affected versions 4.3.8, 4.3.9, 4.3.10, 4.3.11, and 4.3.12.\n\n- I have already upgraded to one of the affected versions:\n  - You will need to use the `--force` flag to upgrade.\n- I must upgrade to one of the affected versions before I can upgrade to 4.3.13:\n- This is not recommended. However, if you must upgrade to an affected version, be aware that you will need to use the `--force` flag to perform your next upgrade.\n\n**Using the `--force` flag**:\n\n**IMPORTANT:** Any changes you have made to the default SCCs `anyuid`, `hostaccess`, `hostmount-anyuid`, `hostnetwork`, `nonroot`, `privileged`, or `restricted` may be removed later when you upgrade to 4.4 which could cause system instability. You should address this issue by migrating any changes you made to the mentioned default SCCs to new SCCs.\n\n- Use of the `--force` flag will skip all precondition tests. You must verify that there are no other preconditions which need to be considered.\n- Upgrading using `--force` **will not** remove the changes you have made to the default SCCs. You should create a plan to migrate the changes you made to the default SCCs to new SCCs before you upgrade to 4.4.\n\nThe `--force` flag can be added to your `oc adm upgrade` command. For example:\n~~~\n# oc adm upgrade --force --to 4.3.13\n~~~\n","more_info":"For more in-depth information check the following:\n1. [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1821905).\n2. [KCS](https://access.redhat.com/solutions/4972291)","plugin":{"name":"Cannot upgrade due to \"DefaultSecurityContextConstraints_Mutated\"","node_id":"4972291","product_code":"OCP4","python_module":"ccx_rules_ocp.external.bug_rules.bug_1821905"},"error_keys":{"BUGZILLA_BUG_1821905":{"generic":"Due to a known bug in OpenShift 4 operators cannot upgrade due to \"DefaultSecurityContextConstraints_Mutated\".\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4972291)\n","metadata":{"condition":"Operator can't be upgraded with reason \"DefaultSecurityContextConstraints_Mutated\"","description":"Cluster upgrade will fail when default SCC gets changed","impact":"System Stability Loss","likelihood":3,"publish_date":"2020-04-17 16:00:00","status":"active","tags":["openshift","service_availability"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Due to a known bug in OpenShift \"4.6.x\", the `authentication` operator reports `Available` `False` with the `ReadyIngressNodes_NoReadyIngressNodes` reason, even in situations where ingress nodes are ready.\n","reason":"On this OCP 4.6 cluster, a known bug causes the `authentication` operator to interpret the lack of `worker` nodes as a ingress blocker, regardless of whether the `default` `IngressController`'s `nodePlacement` allows router pods to be scheduled on nodes with different roles.\n","resolution":"Red Hat recommends that you upgrade the OpenShift cluster to version 4.6.4 or above.  Alternatively, you can provision at least one node with the `node-role.kubernetes.io/worker` label.\n","more_info":"For more in-depth information check the following:\n1. [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1893386)\n","plugin":{"name":"The `authentication` operator falsely reports `ReadyIngressNodes_NoReadyIngressNodes`, blocking OpenShift updates.","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.bug_rules.bug_1893386"},"error_keys":{"BUGZILLA_BUG_1893386":{"generic":"Due to a known bug in OpenShift \"4.6.x\", the `authentication` operator reports `Available` `False` with the `ReadyIngressNodes_NoReadyIngressNodes` reason, even in situations where ingress nodes are ready.\n\n[Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1893386).\n","metadata":{"condition":"","description":"The `authentication` operator falsely reports `ReadyIngressNodes_NoReadyIngressNodes`","impact":"Cluster Availability","likelihood":2,"publish_date":"2020-10-30 23:40:00","status":"active","tags":["openshift","service_availability"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"The openshift-samples cluster-operator is degraded due to FailedImageImport due to a hiccup while talking to the Red Hat registry.\n","reason":"Due to a temporary hiccup talking to the Red Hat registry the openshift-samples failed to import some of the imagestreams.\n\n\nSource of the issue:\n\n**Cluster-operator:**  **{{=pydata.info[\"name\"]}}**\n- *Condition:* {{=pydata.info[\"condition\"]}}\n- *Reason:* {{=pydata.info[\"reason\"]}}\n- *Message:* {{=pydata.info[\"message\"]}}\n- *Last* Transition: {{=pydata.info[\"lastTransitionTime\"]}}\n","resolution":"Red Hat recommends that you to follow these steps:\n\n1. Fix 1, Try running:\n~~~\n# oc import-image \u003cfor the ImageStream(s) in question\u003e\n~~~\n\n1. Fix 2, Try running:\n~~~\n# oc delete configs.samples cluster\n~~~","more_info":"","plugin":{"name":"OpenShift 4.x openshift-samples operator degraded due to FailedImageImports.","node_id":"4563171","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.samples_op_failed_image_import_check"},"error_keys":{"SAMPLES_FAILED_IMAGE_IMPORT_ERR":{"generic":"The `openshift-samples` cluster-operator is degraded due to `FailedImageImport` because of a hiccup while talking to the Red Hat registry.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4563171)\n","metadata":{"condition":"The openshift-samples cluster-operator is degraded due to FailedImageImport","description":"Pods could fail to start if openshift-samples is degraded due to FailedImageImport which is caused by a hiccup while talking to the Red Hat registry","impact":"Application Failure","likelihood":2,"publish_date":"2020-02-07 14:19:00","status":"active","tags":["openshift","incident","networking","registry","service_availability","pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Red Hat Insights for OpenShift is a proactive management solution. It provides ongoing infrastructure analyses of your Red Hat OpenShift Container Platform 4.2 and later installations. Red Hat Insights helps you identify, prioritize, and resolve risks to security, performance, availability, and stability before they become urgent issues.\nRed Hat Insights for OpenShift uses the Remote Health Monitoring feature of OpenShift 4. The health checks are created by Red Hat subject matter experts and assessed according to severity and impact.\nThis is an example  recommendation that you can safely ignore. To disable it, click  the triple-dot menu button next to the header, and select Disable.\n","reason":"","resolution":"","more_info":"For more info about the Remote Health Monitoring, refer to [documentation](https://docs.openshift.com/container-platform/4.3/support/remote_health_monitoring/about-remote-health-monitoring.html)\n","plugin":{"name":"Introducing Insights for Red Hat OpenShift Container Platform","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocm.tutorial_rule"},"error_keys":{"TUTORIAL_ERROR":{"generic":"Red Hat Insights for OpenShift is a proactive management solution. It provides ongoing infrastructure analyses of your Red Hat OpenShift Container Platform 4.2 and later installations. Red Hat Insights helps you identify, prioritize, and resolve risks to security, performance, availability, and stability before they become urgent issues.\nRed Hat Insights for OpenShift uses the Remote Health Monitoring feature of OpenShift 4. The health checks are created by Red Hat subject matter experts and assessed according to severity and impact.\nThis is an example  recommendation that you can safely ignore. To disable it, click  the triple-dot menu button next to the header, and select Disable.\n\n[Knowledgebase Article](https://access.redhat.com/search/#/knowledgebase)\n\n","metadata":{"condition":"","description":"Introducing Insights for Red Hat OpenShift Container Platform","impact":"","likelihood":1,"publish_date":"2020-04-08 00:42:00","status":"active","tags":["openshift","service_availability"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"The Machine Config Daemon Pod gets removed from a node When any kind of **Taint** is set on the node and the Daemon Set **machine-config-daemon** does not set a universal **Toleration**.\n","reason":"","resolution":"Red Hat recommends that you perform the following steps:\n1. If MachineConfigDaemon pod gets removed from a node, patch the machine-config Daemonset to allow taints in the nodes and properly schedule the necessary pods:\n~~~\n# oc patch ds machine-config-daemon -n openshift-machine-config-operator  --type=merge -p '{\"spec\": {\"template\": { \"spec\": {\"tolerations\":[{\"operator\":\"Exists\"}]}}}}'\n~~~\n`Note`: this patch will be removed by the operator after some hours but will be sufficient to allow the upgrade on the stuck nodes.\n\n2. Upgrade the OpenShift cluster to a more recent version.\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n  * [OpenShift {{=pydata.ocp_branch}}](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/updating/updating-cluster-between-minor.html)\n","more_info":"","plugin":{"name":"MachineConfig stuck in updating mode when Node has taints applied","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.machineconfig_stuck_by_node_taints"},"error_keys":{"NODE_HAS_TAINTS_APPLIED":{"generic":"When the Machine Config Daemon Daemon Set does not set a universal Toleration, the Machine Config Daemon Pod gets removed from tainted nodes due to a bug in OpenShift Container Platform 4. This results in decreased cluster availability.\n","metadata":{"condition":"It is a buggy OCP version, and taints are set on the nodes","description":"The Machine Config Daemon stops working when a Node has taints applied","impact":"Cluster Availability","likelihood":3,"publish_date":"2020-11-18 12:00:00","status":"active","tags":["openshift","service_availability"]},"reason":"  * The cluster is running OpenShift {{=pydata.current_version}}.\n  * Some nodes have taints applied.\n\nPlease review [MachineConfig stuck in updating mode if OpenShift Container Platform - Node has taints applied](https://access.redhat.com/solutions/5137781) for more information.\n","HasReason":true}},"HasReason":false},{"summary":"After migrations of massive egress IPs, some of the changes in the host subnets are not reflected on the new node, the egress IPs are still in original node.\n","reason":"This cluster is running with version **{{=pydata.version}}**. Due to a known bug in this version, Egress IPs migration is in high risk of failing when the number of egress IPs is high. Following is hostsubnet{{?pydata.result.length\u003e1}}s{{?}} with massive egress IPs.\n{{~pydata.result:item }}\n  * Host: {{=item[\"host\"]}}\n  * Number: {{=item[\"length\"]}}\n\n{{~}}\n","resolution":"Red Hat recommends that you upgrade to 4.5 to avoid this issue.\n\n[Updating a cluster between minor versions](https://docs.openshift.com/container-platform/{{=pydata.version}}/updating/updating-cluster-between-minor.html) (official OCP4 documentation)\n","more_info":"[How to Upgrade OpenShift 4 between different minor versions via \"oc\" cli](https://access.redhat.com/solutions/4606811) (a KCS that extends the official documentation)\n","plugin":{"name":"Egress IP migration fails when there are massive egress IP","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.subnets_migration_failure_massive_egressip"},"error_keys":{"SUBNETS_MIGRATION_FAILURE_MASSIVE_EGRESSIP":{"generic":"After migrations of massive egress IPs, some of the changes in the host subnets are not reflected on the new node, the egress IPs are still in original node.\n","metadata":{"condition":"OCP version is before 4.5; Number of EGRESS IPs is over 90","description":"Egress IP migration fails when there are massive egress IPs","impact":"Cluster Availability","likelihood":3,"publish_date":"2020-10-30 16:00:00","status":"active","tags":["openshift","configuration","service_availability"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"Some control plane certificates will expire soon.\n","reason":"The following control plane certificates will expire in less than 7 days:\n\n{{~ pydata.expired :cert }}\n* {{=cert.name}} (not valid after {{=cert.not_valid_after}})\n{{~}}\n","resolution":"Red Hat recommends you to follow steps in [Recovering from expired control plane certificates](https://docs.openshift.com/container-platform/{{=pydata.version}}/backup_and_restore/disaster_recovery/scenario-3-expired-certs.html).\n\nIf you know a better solution for this issue, please let us know!\n","more_info":"","plugin":{"name":"Some control plane certificates will expire in less than 7 days.","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.certificates_expiring_soon_info"},"error_keys":{"CERTIFICATES_EXPIRING_SOON":{"generic":"Some control plane certificates will expire in less than 7 days.\n","metadata":{"condition":"Some control plane certificate will expire in less than 7 days.","description":"Control plane certificates will expire in less than 7 days","impact":"Cluster Availability","likelihood":4,"publish_date":"2020-01-09 09:21:00","status":"active","tags":["pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"One or more operators are in a one of the following failing states:\n- Not Available\n- Degraded\n- Progressing state\n","reason":"The following operators are experiencing issues:\n{{~ pydata.operators :op }}\n    {{=op[\"operator\"][\"name\"]}}\n    Issues:\n\t{{~ op[\"issues\"] :issues }}\n\t{{=issues[0]}}\n    - Reason: {{=issues[1][\"reason\"]}}\n    - Message: {{=issues[1][\"message\"]}}\n    - LastTransition: {{=issues[1][\"last_trans_time\"]}}\n\t{{~}}\n{{~}}","resolution":"","more_info":"","plugin":{"name":"","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.operators_check"},"error_keys":{"OPERATOR_HEALTHY":{"generic":"All operators are healthy.\n","metadata":{"condition":"All operators are healthy","description":"All operators are in desired state","impact":"","likelihood":1,"publish_date":"2019-11-03 08:25:00","status":"active","tags":["pssa"]},"reason":"","HasReason":false},"OPERATOR_ISSUE":{"generic":"Operator(s) are in a failing state.\n","metadata":{"condition":"Operator(s) are in a failing state","description":"If an operator is in Not Available/Degraded/Progressing state, then its having an issue that needs further investigation","impact":"Application Failure","likelihood":3,"publish_date":"2019-11-03 08:25:00","status":"active","tags":["openshift","incident","pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"A cluster failing to update to the desired version is a sign that the cluster is behaving irregularly.\n","reason":"The current version **({{=pydata.current}})** doesn't match the desired version **({{=pydata.desired}})**.\n","resolution":"","more_info":"","plugin":{"name":"","node_id":"","product_code":"OCP4","python_module":"ccx_rules_ocp.internal.rules.version_check"},"error_keys":{"CLUSTER_VERSION_MISMATCH":{"generic":"Cluster is not on desired version.","metadata":{"condition":"Cluster is not on desired version","description":"Cluster not updating to desired version","impact":"Unapplied Configuration","likelihood":3,"publish_date":"2019-10-17 11:13:00","status":"active","tags":["pssa"]},"reason":"","HasReason":false},"CLUSTER_VERSION_OK":{"generic":"The cluster is running the desired version.","metadata":{"condition":"The cluster is running the desired version","description":"The cluster is running the desired version","impact":"","likelihood":1,"publish_date":"2019-10-17 11:13:00","status":"active","tags":["pssa"]},"reason":"","HasReason":false}},"HasReason":true},{"summary":"When the cluster is configured to use a cluster-wide proxy, the authentication operator is Degraded.\n","reason":"Requests to routes and/or the public API endpoint are not being proxied to the cluster.\n","resolution":"Red Hat recommends that you to follow steps in the KCS article.\n * [Authentication operator Degraded with Reason `WellKnownEndpointDegradedError`](https://access.redhat.com/solutions/4569191)\n","more_info":"For more information about the configuring the proxy, refer to [enabling the cluster-wide proxy](https://docs.openshift.com/container-platform/4.3/networking/enable-cluster-wide-proxy.html#nw-proxy-configure-object_config-cluster-wide-proxy)\n","plugin":{"name":"Authentication operator is degraded while having a cluster wide proxy configured.","node_id":"4569191","product_code":"OCP4","python_module":"ccx_rules_ocp.external.rules.cluster_wide_proxy_auth_check"},"error_keys":{"AUTH_OPERATOR_PROXY_ERROR":{"generic":"When the cluster is configured to use a cluster-wide proxy, the `authentication` operator is `Degraded`.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4569191)\n","metadata":{"condition":"Authentication operator degraded with message \"WellKnownEndpointDegraded\"","description":"The authentication operator is degraded when cluster is configured to use a cluster-wide proxy","impact":"Application Failure","likelihood":2,"publish_date":"2020-02-03 08:25:00","status":"active","tags":["security","service_availability","pssa"]},"reason":"","HasReason":false}},"HasReason":true}],"status":"ok"}
