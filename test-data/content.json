{
  "content": [
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.image_registry_removed_state_reports_degraded"
      },
      "error_keys": {
        "BUGZILLA_BUG_1867792": {
          "metadata": {
            "description": "Pruner degrades image registry operator if the latter is removed",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Pruner degrades image registry operator if the latter is removed.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1867792](https://bugzilla.redhat.com/show_bug.cgi?id=1867792) for more information.",
          "reason": "The image resgistry operator's managementState is set to 'removed'.<br>\nConsidering the ManagementState, the pruner pod marks the operator to be degraded.<br>\nWe are likely to hit the highlighted bug.<br>\n\nOperator Details:<br>\nOperator Name           : {{=pydata.results[\"name\"]}}<br>\nOperator Degraded State : {{=pydata.results[\"degraded\"][\"status\"]}}<br>\nMessage                 : {{=pydata.results[\"degraded\"][\"message\"]}}<br>\nReason                  : {{=pydata.results[\"degraded\"][\"reason\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1867792](https://bugzilla.redhat.com/show_bug.cgi?id=1867792) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.nodes_container_runtime_version_check"
      },
      "error_keys": {
        "NODES_CONTAINER_RUNTIME_VERSION": {
          "metadata": {
            "description": "Issues are likely to occur when container runtime versions are inconsistent among nodes",
            "impact": 2,
            "likelihood": 2,
            "publish_date": "2020-07-28 12:00:00",
            "status": "active",
            "tags": [
              "sbr_shift",
              "openshift",
              "container",
              "pod",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "When container runtime versions are inconsistent, nodes behave differently in some cases, which can lead to unexpected issues.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you update container runtime version accordingly:\n\n1. Update container engine package to `{{=pydata.expected_package}}`.\n\n1. Reboot node.\n",
          "more_info": "",
          "reason": "There {{?pydata.nodes_with_different_version.length > 1}}are{{?}} {{?pydata.nodes_with_different_version.length == 1}}is{{?}} {{=pydata.nodes_with_different_version.length}} cluster node{{?pydata.nodes_with_different_version.length > 1}}s{{?}} with container runtime version that is different from that of the master node. The correct package version should be `{{=pydata.expected_package}}`.\n\n{{~pydata.nodes_with_different_version:item}}\n**Node:** {{=item[\"node\"]}}\n- *Roles:* {{=item[\"roles\"]}}\n- *Version:* {{=item[\"version\"]}}\n\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you update container runtime version accordingly:\n\n1. Update container engine package to `{{=pydata.expected_package}}`.\n\n1. Reboot node.\n",
      "more_info": "",
      "reason": "There {{?pydata.nodes_with_different_version.length > 1}}are{{?}} {{?pydata.nodes_with_different_version.length == 1}}is{{?}} {{=pydata.nodes_with_different_version.length}} cluster node{{?pydata.nodes_with_different_version.length > 1}}s{{?}} with container runtime version that is different from that of the master node. The correct package version should be `{{=pydata.expected_package}}`.\n\n{{~pydata.nodes_with_different_version:item}}\n**Node:** {{=item[\"node\"]}}\n- *Roles:* {{=item[\"roles\"]}}\n- *Version:* {{=item[\"version\"]}}\n\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.operator_unmanaged"
      },
      "error_keys": {
        "OPERATOR_UNMANAGED": {
          "metadata": {
            "description": "OpenShift cluster can get damaged when an operator is configured to 'Unmanaged' state",
            "impact": 2,
            "likelihood": 2,
            "publish_date": "2021-01-11 16:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift",
              "operator"
            ]
          },
          "total_risk": 2,
          "generic": "OpenShift cluster can get damaged when an operator is configured to 'Unmanaged' state.",
          "summary": "",
          "resolution": "Red Hat recommends that you configure the operator parameter `managementState` to `Managed`.\n\nExample:\n{{~pydata.unmanaged_operators:item}}\n~~~\n# oc patch oc/{{=item}} --type='merge' -p '{\"Spec\":{\"managementState\": \"Managed\"}}'\n~~~\n{{~}}\n",
          "more_info": "",
          "reason": "{{?pydata.unmanaged_operators.length > 1}}The following operators have the `managementState` parameter set to `Unmanaged`. {{??}}The following operator has the `managementState` parameter set to `Unmanaged`. {{?}}Some Operators might not support this management state as it might damage the cluster and require manual recovery.\n\n**Operator Name:**\n{{~pydata.unmanaged_operators:item}}\n- {{=item}}\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you configure the operator parameter `managementState` to `Managed`.\n\nExample:\n{{~pydata.unmanaged_operators:item}}\n~~~\n# oc patch oc/{{=item}} --type='merge' -p '{\"Spec\":{\"managementState\": \"Managed\"}}'\n~~~\n{{~}}\n",
      "more_info": "",
      "reason": "{{?pydata.unmanaged_operators.length > 1}}The following operators have the `managementState` parameter set to `Unmanaged`. {{??}}The following operator has the `managementState` parameter set to `Unmanaged`. {{?}}Some Operators might not support this management state as it might damage the cluster and require manual recovery.\n\n**Operator Name:**\n{{~pydata.unmanaged_operators:item}}\n- {{=item}}\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5100521",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.check_sap_sdi_observer_pods"
      },
      "error_keys": {
        "SAP_SDI_OBSERVER_POD_ERROR": {
          "metadata": {
            "description": "SAP Data Intelligence failures occur when the SDI Observer pod is not running",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-03-09 14:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "sap",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "SDI Observer is a pod that monitors and updates SAP Data Intelligence namespaces. It makes sure that the namespaces contain objects required by SAP Data Intelligence. When the pod is not running, the objects are not created and SAP Data Intelligence fails.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you {{?pydata.sdi_observer_pods==0}}deploy SDI Observer before installing SAP Data Intelligence 3{{??}}restart the SDI Observer pods that don't work well or contact Red Hat Global Support Services for further assistance{{?}}.\n\nFor more information about the SDI Observer, please see [SDI Observer](https://access.redhat.com/articles/5100521#sdi-observer)\n",
          "more_info": "",
          "reason": "SAP Data Intelligence failures will occur in the following namespaces because the SDI Observer pod is not running:\n\n* sdi-namespace-1\n\nThe SDI Observer pod is needed to monitor and update the namespaces. It maintains objects that SAP Data Intelligence needs to function correctly.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you {{?pydata.sdi_observer_pods==0}}deploy SDI Observer before installing SAP Data Intelligence 3{{??}}restart the SDI Observer pods that don't work well or contact Red Hat Global Support Services for further assistance{{?}}.\n\nFor more information about the SDI Observer, please see [SDI Observer](https://access.redhat.com/articles/5100521#sdi-observer)\n",
      "more_info": "",
      "reason": "SAP Data Intelligence failures will occur in the following namespaces because the SDI Observer pod is not running:\n\n* sdi-namespace-1\n\nThe SDI Observer pod is needed to monitor and update the namespaces. It maintains objects that SAP Data Intelligence needs to function correctly.\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.sap_data_intelligence_permissions"
      },
      "error_keys": {
        "SAP_DATA_INTELLIGENCE_PERMISSIONS": {
          "metadata": {
            "description": "The SAP Data Intelligence 3 installation will fail when the anyuid and privileged permissions are not set correctly",
            "impact": 2,
            "likelihood": 4,
            "publish_date": "2021-02-08 14:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "sap",
              "service_availability"
            ]
          },
          "total_risk": 3,
          "generic": "The SAP Data Intelligence 3 installation will fail when the anyuid and privileged permissions are not set correctly.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you login to OpenShift as a cluster-admin and perform the following configurations for the installation:\n\n{{ for (var index in pydata.namespaces) { }}\n~~~\n# oc project {{=pydata.namespaces[index]}}\n# oc adm policy add-scc-to-group anyuid \"system:serviceaccounts:$(oc project -q)\"\n# oc adm policy add-scc-to-user privileged -z default\n# oc adm policy add-scc-to-user privileged -z mlf-deployment-api\n# oc adm policy add-scc-to-user privileged -z vora-vflow-server\n# oc adm policy add-scc-to-user privileged -z \"vora-vsystem-$(oc project -q)\"\n# oc adm policy add-scc-to-user privileged -z \"vora-vsystem-$(oc project -q)-vrep\"\n# oc adm policy add-scc-to-user privileged -z \"$(oc project -q)-elasticsearch\"\n# oc adm policy add-scc-to-user privileged -z \"$(oc project -q)-fluentd\"\n~~~\n{{?}}\n\nFor more information about SAP Data Intelligence 3 installation, see [Project setup](https://access.redhat.com/articles/5100521#project-setup)\n",
          "more_info": "",
          "reason": "The SAP Data Intelligence needs the `anyuid` and `privileged` permissions during the installation phase.\n\n{{ for (var key in pydata.missing) { }}\n{{? \"anyuid\" == key }}\nThe following service account is not added to the `anyuid` {{? pydata.privileged_type == \"scc\" }}security context constraint{{?}}{{? pydata.privileged_type ==\"crb\" }}cluster role bindings{{?}}\n  * {{=pydata.missing[key]}}\n{{?}}\n\n{{? \"privileged\" == key }}\nThe following service account{{?pydata.missing[key].length>1}}s are {{??}} is{{?}} not added to the `privileged` {{? pydata.privileged_type == \"scc\" }}security context constraint{{?}}{{? pydata.privileged_type ==\"crb\" }}cluster role bindings{{?}}\n{{ for (var index in pydata.missing[key]) { }}\n  * {{=pydata.missing[key][index]}}\n{{}}}\n{{?}}\n{{}}}\n\nThe installation will fail when those permissions aren't set for SAP Data Intelligence.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you login to OpenShift as a cluster-admin and perform the following configurations for the installation:\n\n{{ for (var index in pydata.namespaces) { }}\n~~~\n# oc project {{=pydata.namespaces[index]}}\n# oc adm policy add-scc-to-group anyuid \"system:serviceaccounts:$(oc project -q)\"\n# oc adm policy add-scc-to-user privileged -z default\n# oc adm policy add-scc-to-user privileged -z mlf-deployment-api\n# oc adm policy add-scc-to-user privileged -z vora-vflow-server\n# oc adm policy add-scc-to-user privileged -z \"vora-vsystem-$(oc project -q)\"\n# oc adm policy add-scc-to-user privileged -z \"vora-vsystem-$(oc project -q)-vrep\"\n# oc adm policy add-scc-to-user privileged -z \"$(oc project -q)-elasticsearch\"\n# oc adm policy add-scc-to-user privileged -z \"$(oc project -q)-fluentd\"\n~~~\n{{?}}\n\nFor more information about SAP Data Intelligence 3 installation, see [Project setup](https://access.redhat.com/articles/5100521#project-setup)\n",
      "more_info": "",
      "reason": "The SAP Data Intelligence needs the `anyuid` and `privileged` permissions during the installation phase.\n\n{{ for (var key in pydata.missing) { }}\n{{? \"anyuid\" == key }}\nThe following service account is not added to the `anyuid` {{? pydata.privileged_type == \"scc\" }}security context constraint{{?}}{{? pydata.privileged_type ==\"crb\" }}cluster role bindings{{?}}\n  * {{=pydata.missing[key]}}\n{{?}}\n\n{{? \"privileged\" == key }}\nThe following service account{{?pydata.missing[key].length>1}}s are {{??}} is{{?}} not added to the `privileged` {{? pydata.privileged_type == \"scc\" }}security context constraint{{?}}{{? pydata.privileged_type ==\"crb\" }}cluster role bindings{{?}}\n{{ for (var index in pydata.missing[key]) { }}\n  * {{=pydata.missing[key][index]}}\n{{}}}\n{{?}}\n{{}}}\n\nThe installation will fail when those permissions aren't set for SAP Data Intelligence.\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5391601",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.image_registry_storage_comprehensive"
      },
      "error_keys": {
        "IMAGE_REGISTRY_STORAGE_EMPTYDIR_INT": {
          "metadata": {
            "description": "The Image Registry uses temporary storage (emptyDir)",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The Image Registry uses temporary storage (emptyDir). Data will be lost when the Image Registry pod is restarted.\n\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The Image Registry uses temporary storage (emptyDir).\n\nData will be lost when the Image Registry pod is restarted.\n\n* **Infrastructure:** {{?pydata.infrastructure}}{{=pydata.infrastructure}}{{??}}Unknown{{?}}\n* **Management state:** {{=pydata.management_state}}\n* **Storage type:** {{=pydata.storage_type}}\n",
          "HasReason": true
        },
        "IMAGE_REGISTRY_STORAGE_EXTERNAL_PASS": {
          "metadata": {
            "description": "The Image Registry uses an external storage",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The Image Registry uses an external storage. No further checks have been performed.\n\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The Image Registry uses an external storage.\n\nNo further checks have been performed.\n\n* **Infrastructure:** {{?pydata.infrastructure}}{{=pydata.infrastructure}}{{??}}Unknown{{?}}\n* **Management state:** {{=pydata.management_state}}\n* **Storage type:** {{=pydata.storage_type}}\n",
          "HasReason": true
        },
        "IMAGE_REGISTRY_STORAGE_NOT_CONFIGURED": {
          "metadata": {
            "description": "The Image Registry storage backend is not configured",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The Image Registry storage backend is not configured. The Image Registry Operator Config is Degraded.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The Image Registry storage backend is not configured.\n\nThe Image Registry Operator Config is Degraded.\n\n* **Infrastructure:** {{?pydata.infrastructure}}{{=pydata.infrastructure}}{{??}}Unknown{{?}}\n* **Management state:** {{=pydata.management_state}}\n* **Degraded reason:** {{=pydata.degraded_reason}}\n* **Degraded message:** {{=pydata.degraded_message}}\n",
          "HasReason": true
        },
        "IMAGE_REGISTRY_STORAGE_PVC_FAIL": {
          "metadata": {
            "description": "The Persistent Volume Claim for the Image Registry is not configured properly",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The Image Registry is configured with a Persistent Volume Claim. One or more issues have been found with the configuration.\n\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The Image Registry is configured with a Persistent Volume Claim.\n\n{{=pydata.issues.length}} issue{{?pydata.issues.length>1}}s have{{??}} has{{?}} been found (see below).\n\n* **Infrastructure:** {{?pydata.infrastructure}}{{=pydata.infrastructure}}{{??}}Unknown{{?}}\n* **Management state:** {{=pydata.management_state}}\n* **Storage type:** {{=pydata.storage_type}}\n* **PVC name:** {{=pydata.pvcname}}\n* **PV capacity:** {{=pydata.capacity}}\n* **PV reclaim policy:** {{=pydata.reclaim_policy}}\n* **PV phase:** {{=pydata.phase}}\n* **PV access modes:** {{for (index in pydata.access_modes) { }}{{? index == 0 }}{{=pydata.access_modes[index]}}{{??}}, {{=pydata.access_modes[index]}}{{?}}{{ } }}\n* **Replicas:** {{=pydata.replicas}}\n\nIssues found:\n{{~ pydata.issues:issue \n}}{{? issue == \"wrong_access_mode\" && pydata.replicas == 1 \n    }}* The PV supports neither ReadWriteOnce nor ReadWriteMany access modes. ReadWriteOnce would be enough because replicas are set to 1.\n{{?? issue == \"wrong_access_mode\" && pydata.replicas > 1\n    }}* The PV does not support the ReadWriteMany access mode while replicas are set to {{=pydata.replicas}}.\n{{?? issue == \"low_capacity\"\n    }}* The PV storage capacity does not meet the minimal requirement of 100Gi.\n{{??\nissue == \"wrong_reclaim_policy\"\n    }}* The PV reclaim policy is not Retain.\n{{??  issue == \"not_bound\" \n    }}* The PV phase is not Bound.\n{{?}}{{~}}\n\n",
          "HasReason": true
        },
        "IMAGE_REGISTRY_STORAGE_PVC_PASS": {
          "metadata": {
            "description": "The Image Registry is correctly configured with a Persistent Volume Claim",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The Image Registry is configured with a Persistent Volume Claim. No issues have been found.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The Image Registry is configured with a Persistent Volume Claim.\n\nNo issues have been found.\n\n* **Infrastructure:** {{?pydata.infrastructure}}{{=pydata.infrastructure}}{{??}}Unknown{{?}}\n* **Management state:** {{=pydata.management_state}}\n* **Storage type:** {{=pydata.storage_type}}\n* **PVC name:** {{=pydata.pvcname}}\n* **PV capacity:** {{=pydata.capacity}}\n* **PV reclaim policy:** {{=pydata.reclaim_policy}}\n* **PV phase:** {{=pydata.phase}}\n* **PV access modes:** {{for (index in pydata.access_modes) { }}{{? index == 0 }}{{=pydata.access_modes[index]}}{{??}}, {{=pydata.access_modes[index]}}{{?}}{{ } }}\n* **Replicas:** {{=pydata.replicas}}\n",
          "HasReason": true
        },
        "IMAGE_REGISTRY_STORAGE_PV_NOT_FOUND": {
          "metadata": {
            "description": "The Image Registry is configured with a Persistent Volume Claim but the Persistent Volume was not found",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The Image Registry is configured with a Persistent Volume Claim. The rule was unable to find the corresponding Persistent Volume to check its configuration and state.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The Image Registry is configured with a Persistent Volume Claim.\n\nThe rule was unable to find the corresponding Persistent Volume to check its\nconfiguration and state.\n\n* **Infrastructure:** {{?pydata.infrastructure}}{{=pydata.infrastructure}}{{??}}Unknown{{?}}\n* **Management state:** {{=pydata.management_state}}\n* **Storage type:** {{=pydata.storage_type}}\n* **PVC name:** {{=pydata.pvcname}}\n",
          "HasReason": true
        },
        "IMAGE_REGISTRY_STORAGE_REMOVED": {
          "metadata": {
            "description": "The Image Registry Operator is in the Removed management state",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The Image Registry Operator is in the Removed management state. The Image Registry has not been configured yet or the cluster is using an external registry.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The Image Registry Operator is in the Removed management state.\n\nThe Image Registry has not been configured yet or the cluster is using an external registry.\n\n* **Infrastructure:** {{?pydata.infrastructure}}{{=pydata.infrastructure}}{{??}}Unknown{{?}}\n* **Management state:** {{=pydata.management_state}}\n\n",
          "HasReason": true
        },
        "IMAGE_REGISTRY_STORAGE_UNMANAGED": {
          "metadata": {
            "description": "The Image Registry Operator is in the Unmanaged management state",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The Image Registry Operator is in the Unmanaged management state. The rule cannot rely on data in the Image Registry Operator Config to assess the Image Registry configuration and state.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The Image Registry Operator is in the Removed management state.\n\nThe rule cannot rely on data in the Image Registry Operator Config to assess the Image Registry configuration and state.\n\n* **Infrastructure:** {{?pydata.infrastructure}}{{=pydata.infrastructure}}{{??}}Unknown{{?}}\n* **Management state:** {{=pydata.management_state}}\n\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4324061",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.image_registry_failed_to_claim_nfs_volume"
      },
      "error_keys": {
        "IMAGE_REGISTRY_FAILED_TO_CLAIM_NFS_VOLUME": {
          "metadata": {
            "description": "The Image Registry failed to claim a NFS volume because of access modes mismatch",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The Image Registry failed to claim a NFS volume because of access modes mismatch. The Image Registry was trying to claim a volume with the ReadWriteMany access mode while the default storage class \"thin\" provided only ReadWriteOnce.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The Image Registry failed to claim a NFS volume because of access modes mismatch as indicated by this event:\n\n* **Event namespace:** openshift-image-registry\n* **Event type:** Warning\n* **Event reason:** Failed Scheduling\n* **Event message:** Failed to provision volume with StorageClass \"thin\": invalid AccessModes [ReadWriteMany]: only AccessModes [ReadWriteOnce] are supported\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5582161",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.monitoring_updating_openshift_state_metrics_failed"
      },
      "error_keys": {
        "BUGZILLA_BUG_1887354": {
          "metadata": {
            "description": "Monitoring degraded with \"UpdatingopenshiftStateMetricsFailed\" while upgrade from 4.5 to 4.6, for few minutes",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Monitoring degraded with \"UpdatingopenshiftStateMetricsFailed\" while upgrade from 4.5 to 4.6, for few minutes.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1887354](https://bugzilla.redhat.com/show_bug.cgi?id=1887354) for more information.",
          "reason": "Possible known bug found.<br>\nSummary     : Monitoring degraded for few minutes, with \"UpdatingopenshiftStateMetricsFailed\" while upgrade to OCP version < 4.5.19.<br>\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage :<br>\n~~~\n{{=pydata.results[pod_data][\"message\"]}}\n~~~\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1887354](https://bugzilla.redhat.com/show_bug.cgi?id=1887354) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6167652",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.insights_operator_degraded_UHC_authentication_failed"
      },
      "error_keys": {
        "INSIGHTS_OPERATOR_DEGRADED_UHC_AUTHENTICATION_FAILED": {
          "metadata": {
            "description": "Insights operator is degraded due to UHC services authentication failed",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Insights operator is degraded due to UHC services authentication failed.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The insights operator is degraded due to the UHC services authentication failed. The Pull Secret<br>\nmay be corrupted or it may require to be updated if the related account is no longer valid.<br>\n\n{{for (var pod_detail in pydata.results) { }}\nPod Name    : {{=pydata.results[pod_detail][\"pod_name\"]}}<br>\nMessage     :<br>\n{{=pydata.results[pod_detail][\"message\"]}}<br>\n{{}}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.openshift_sdn_cannot_access_kube_apiserver"
      },
      "error_keys": {
        "ERROR_OPENSHIFT_SDN_CANNOT_ACCESS_KUBE_APISERVER": {
          "metadata": {
            "description": "The OpenShift SDN pod is having issues accessing kube-apiserver when the kube-apiserver or etcd doesn't work as expected",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-03-31 12:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift"
            ]
          },
          "total_risk": 2,
          "generic": "The OpenShift SDN pod is having issues accessing kube-apiserver when the kube-apiserver or etcd doesn't work as expected.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The OpenShift SDN pod is having issues accessing kube-apiserver. Please check kube-apiserver or etcd.\n \nThe following log messages were detected:\n```{{~ pydata.res :log }}\n{{=log}}\n{{~}}```\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "The OpenShift SDN pod is having issues accessing kube-apiserver. Please check kube-apiserver or etcd.\n \nThe following log messages were detected:\n```{{~ pydata.res :log }}\n{{=log}}\n{{~}}```\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.lib_bucket_provisioner_check"
      },
      "error_keys": {
        "LIB_BUCKET_PROVISIONER_INSTALL_PLANS_ISSUE": {
          "metadata": {
            "description": "The etcd performace will degrade when the lib-bucket-provisioner operator keeps creating new install plans during a failed upgrade",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2020-11-18 12:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "configuration",
              "performance"
            ]
          },
          "total_risk": 2,
          "generic": "A bug in Openshift Container Platform 4 causes the count of InstallPlans for the `lib-bucket-provisioner` operator to increase continuously. The number of objects eventually decreases etcd performance which impacts the performance of the whole cluster.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to a more recent version.\n\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n\n* [OpenShift {{=pydata.ocp_branch}}](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/updating/updating-cluster-between-minor.html)\n",
          "more_info": "",
          "reason": "The count of InstallPlans for the lib-bucket-provisioner operator is {{pydata.install_plans}}, which suggests that it is growing continuously.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to a more recent version.\n\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n\n* [OpenShift {{=pydata.ocp_branch}}](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/updating/updating-cluster-between-minor.html)\n",
      "more_info": "",
      "reason": "The count of InstallPlans for the lib-bucket-provisioner operator is {{pydata.install_plans}}, which suggests that it is growing continuously.\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.machine_update_stuck"
      },
      "error_keys": {
        "MACHINE_STUCK_UPDATING": {
          "metadata": {
            "description": "Nodes are stuck updating because of repeated TLS handshake timeouts",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "If a node gets 'TLS handshake timeout' errors constantly when trying to download stuff, it will get stuck at updating. \n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Nodes are stuck updating because of repeated TLS handshake timeout:\n\n{{~ pydata.machines:m }}\n* **Machine Config Pool '{{=m[\"name\"]}}'**:\n  * Reason: {{=m[\"reason\"]}}\n  * Message: {{=m[\"message\"]}}\n  * LastTransitionTime: {{=m[\"last_trans_time\"]}}\n  * TLS handsake timeout: {{=m[\"timeout_count\"]}}\n{{~}}\n\nIt is possibly caused by pending unsigned CSR.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5232901",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.nvidia_gpu_operator_sro_check"
      },
      "error_keys": {
        "NVIDIA_GPU_OPERATOR_ERROR": {
          "metadata": {
            "description": "The NVIDIA GPU Operaor no longer works as expected in OpenShift 4.4",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule will check the NVIDIA GPU operaor and Special Resource Operator.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [KCS 5232481](https://access.redhat.com/solutions/5232481) for more information.",
          "reason": "{{? pydata.is_sro }}\nIn NVIDIA GPU Operator versions 1.3.0 or lower, the Special Resource Operator (SRO) did not work<br>\ncorrectly with most versions of Red Hat OpenShift Container Platform 4.5 and all versions of 4.6.<br>\n\nThis was due to a mismatch between the Red Hat Enterprise Linux CoreOS kernel versions in 4.5<br>\nand 4.6 and the kernel versions of the NVIDIA GPU driver container.<br>\n\nPlease refer to the Knowledgebase article: https://access.redhat.com/solutions/5232901<br>\n{{??}}\nOn Red Hat OpenShift Container Platform 4.4.z, the NVIDIA Operator no longer functions as expected<br>\nwhen the pod tries to create the needed kernel module<br>\n\nPlease refer to the Knowledgebase article: https://access.redhat.com/solutions/5232481{{?}}{{for (var pod_detail in pydata.logs) { }}\nPod Name    : {{=pydata.logs[pod_detail][\"pod_name\"]}}\nMessage     :<br>\n{{=pydata.logs[pod_detail][\"message\"]}}\n{{}}}",
          "HasReason": true
        },
        "NVIDIA_SPECIAL_RESOURCE_OPERATOR_ERROR": {
          "metadata": {
            "description": "The NVIDIA GPU Operator version 1.3.0 or lower does not work in OpenShift 4.5 and 4.6",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule will check the NVIDIA GPU operaor and Special Resource Operator.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [KCS 5232481](https://access.redhat.com/solutions/5232481) for more information.",
          "reason": "{{? pydata.is_sro }}\nIn NVIDIA GPU Operator versions 1.3.0 or lower, the Special Resource Operator (SRO) did not work<br>\ncorrectly with most versions of Red Hat OpenShift Container Platform 4.5 and all versions of 4.6.<br>\n\nThis was due to a mismatch between the Red Hat Enterprise Linux CoreOS kernel versions in 4.5<br>\nand 4.6 and the kernel versions of the NVIDIA GPU driver container.<br>\n\nPlease refer to the Knowledgebase article: https://access.redhat.com/solutions/5232901<br>\n{{??}}\nOn Red Hat OpenShift Container Platform 4.4.z, the NVIDIA Operator no longer functions as expected<br>\nwhen the pod tries to create the needed kernel module<br>\n\nPlease refer to the Knowledgebase article: https://access.redhat.com/solutions/5232481{{?}}{{for (var pod_detail in pydata.logs) { }}\nPod Name    : {{=pydata.logs[pod_detail][\"pod_name\"]}}\nMessage     :<br>\n{{=pydata.logs[pod_detail][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [KCS 5232481](https://access.redhat.com/solutions/5232481) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5482431",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.ingress_controller_node_placement"
      },
      "error_keys": {
        "INGRESS_CONTROLLER_NODE_PLACEMENT_NOT_SET": {
          "metadata": {
            "description": "Routing fails when router pods are rescheduled on nodes not configured in the load balancer fronting them",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Routing fails when router pods are rescheduled on nodes not configured in the load balancer fronting them.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the following links for more information:\n* [KCS 5482431](https://access.redhat.com/solutions/5482431)\n* [KCS 4211081](https://access.redhat.com/solutions/4211081)\n",
          "reason": "When RHOCP 4 Ingress is set up with the default `nodePlacement` configuration, any cluster upgrade,\nor any event that might trigger ingress pods to be rescheduled on different nodes might impact the\nIngress traffic if the load balancing is set up for specific nodes and the Ingress pods are\nrescheduled on new ones.\n\nWhen customers get the Ingress traffic issue, it is recommended, from a best practice point of view,\nthat the `nodePlacement` should always be set for the Ingress controller.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the following links for more information:\n* [KCS 5482431](https://access.redhat.com/solutions/5482431)\n* [KCS 4211081](https://access.redhat.com/solutions/4211081)\n",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.certificates_expiring_soon_info"
      },
      "error_keys": {
        "CERTIFICATES_EXPIRING_SOON": {
          "metadata": {
            "description": "Control plane certificates will expire in less than 7 days",
            "impact": 2,
            "likelihood": 4,
            "publish_date": "2020-01-09 09:21:00",
            "status": "active",
            "tags": null
          },
          "total_risk": 3,
          "generic": "Some control plane certificates will expire in less than 7 days.\n",
          "summary": "",
          "resolution": "Red Hat recommends you to follow steps in [Recovering from expired control plane certificates](https://docs.openshift.com/container-platform/{{=pydata.version}}/backup_and_restore/disaster_recovery/scenario-3-expired-certs.html).\n\nIf you know a better solution for this issue, please let us know at ccx@redhat.com!\n",
          "more_info": "",
          "reason": "The following control plane certificates will expire in less than 7 days:\n\n{{~ pydata.expired :cert }}\n* {{=cert.name}} (not valid after {{=cert.not_valid_after}})\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends you to follow steps in [Recovering from expired control plane certificates](https://docs.openshift.com/container-platform/{{=pydata.version}}/backup_and_restore/disaster_recovery/scenario-3-expired-certs.html).\n\nIf you know a better solution for this issue, please let us know at ccx@redhat.com!\n",
      "more_info": "",
      "reason": "The following control plane certificates will expire in less than 7 days:\n\n{{~ pydata.expired :cert }}\n* {{=cert.name}} (not valid after {{=cert.not_valid_after}})\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5669581",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.vsphere_reconnects_an_updated_secret"
      },
      "error_keys": {
        "BUGZILLA_BUG_1821280": {
          "metadata": {
            "description": "vSphere login issue about vSphere reconnects an updated secret",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "vSphere login issue about vSphere reconnects an updated secret.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1821280](https://bugzilla.redhat.com/show_bug.cgi?id=1821280) for more information.",
          "reason": "Found the vSphere login issue about vSphere reconnects an updated secret.<br>\n\nThe following log messages were detected.<br>\n<br>\n{{for (var pod_detail in pydata.res) { }}\nPod Name    : {{=pydata.res[pod_detail][\"pod_name\"]}}\nMessage     :<br>\n{{=pydata.res[pod_detail][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1821280](https://bugzilla.redhat.com/show_bug.cgi?id=1821280) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4102661",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.same_egress_ip_in_multiple_netnamespaces"
      },
      "error_keys": {
        "SAME_EGRESS_IP_IN_MULTIPLE_NETNAMESPACES": {
          "metadata": {
            "description": "The OpenShift cluster drops traffic when two NetNamespaces contain the same egress IP",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-02-18 14:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "configuration",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "SDN behavior is not defined when two netnamespaces contain the same egress IP. For example, the OpenShift cluster might be dropping traffic.",
          "summary": "",
          "resolution": "Red Hat recommends that you configure different Egress IP for the following NetNamespaces.\n{{ for (var key in pydata.netnamespaces) { }}\nNetNamespaces configured to the egress IP `{{=key}}`:\n  {{~pydata.netnamespaces[key]:item}}\n  -  {{=item}}\n  {{~}}\n{{ } }}\nPlease refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/{{=pydata.ocp_version}}/networking/openshift_sdn/assigning-egress-ips.html) for the steps to configure the Egress IP for a project. \n\n",
          "more_info": "",
          "reason": "On the OpenShift cluster, the following NetNamespaces contain the same egress IP. It is not a supported configuration and could cause OpenShift to drop outgoing traffic.\n\n{{ for (var key in pydata.netnamespaces) { }}\nNetNamespaces configured to the egress IP `{{=key}}`:\n  {{~pydata.netnamespaces[key]:item}}\n  -  {{=item}}\n  {{~}}\n{{ } }}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you configure different Egress IP for the following NetNamespaces.\n{{ for (var key in pydata.netnamespaces) { }}\nNetNamespaces configured to the egress IP `{{=key}}`:\n  {{~pydata.netnamespaces[key]:item}}\n  -  {{=item}}\n  {{~}}\n{{ } }}\nPlease refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/{{=pydata.ocp_version}}/networking/openshift_sdn/assigning-egress-ips.html) for the steps to configure the Egress IP for a project. \n\n",
      "more_info": "",
      "reason": "On the OpenShift cluster, the following NetNamespaces contain the same egress IP. It is not a supported configuration and could cause OpenShift to drop outgoing traffic.\n\n{{ for (var key in pydata.netnamespaces) { }}\nNetNamespaces configured to the egress IP `{{=key}}`:\n  {{~pydata.netnamespaces[key]:item}}\n  -  {{=item}}\n  {{~}}\n{{ } }}",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.version_check"
      },
      "error_keys": {
        "CLUSTER_VERSION_DONE_APPLYING": {
          "metadata": {
            "description": "The cluster has successfully completed installation of the desired version",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The cluster has successfully completed instalation of the desired version. The Cluster Version Operator is Available and not Progressing.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "{{? pydata.pass_key == \"CLUSTER_VERSION_DONE_APPLYING\" }}\nThe Cluster has successfully completed installation of {{=pydata.current}}.\n\n**Current version:** {{=pydata.current}}<br/>\n**Desired version:** {{=pydata.desired}}<br/>\n**Available:** {{=pydata.available[\"status\"]}}<br/>\n**Progressing:** {{=pydata.progressing[\"status\"]}}\n\n{{?? pydata.error_key == \"CLUSTER_VERSION_WORKING_TOWARDS\" }}\nThe cluster is working towards {{=pydata.desired}}. The Cluster Version Operator does not indicate any issues otherwise.\n\n**Current version:** {{=pydata.current}}<br/>\n**Desired version:** {{=pydata.desired}}<br/>\n**Available:** {{=pydata.available[\"status\"]}}\n\n**Progressing:** {{=pydata.progressing[\"status\"]}}<br/>\n{{=pydata.progressing[\"message\"]}}\n\n{{?? pydata.error_key == \"CLUSTER_VERSION_ERRORS\" }}\nThe Cluster Version Operator is indicating issues.\n\n**Current version:** {{=pydata.current}}<br/>\n**Desired version:** {{=pydata.desired}}<br/>\n**Available:** {{=pydata.available[\"status\"]}}\n\n**Progressing:** {{=pydata.progressing[\"status\"]}}{{? pydata.progressing[\"reason\"] }} ({{=pydata.progressing[\"reason\"]}}){{?}}<br/>\n{{=pydata.progressing[\"message\"]}}{{? pydata.failing[\"status\"] }}\n\n**Failing:** {{=pydata.failing[\"status\"]}}{{? pydata.failing[\"reason\"] }} ({{=pydata.failing[\"reason\"]}}){{?}}<br/>\n{{=pydata.failing[\"message\"]}}{{?}}{{? pydata.history_count == 1 }}\n\n**Note:**<br/>\nThe cluster is at or working towards its initial version.{{?}}\n\n{{?}}\n",
          "HasReason": true
        },
        "CLUSTER_VERSION_ERRORS": {
          "metadata": {
            "description": "The Cluster Version Operator is indicating issues",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The cluster has not completed installation of the desired issue and the Cluster Version Operator is not Available or not Progressing.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "{{? pydata.pass_key == \"CLUSTER_VERSION_DONE_APPLYING\" }}\nThe Cluster has successfully completed installation of {{=pydata.current}}.\n\n**Current version:** {{=pydata.current}}<br/>\n**Desired version:** {{=pydata.desired}}<br/>\n**Available:** {{=pydata.available[\"status\"]}}<br/>\n**Progressing:** {{=pydata.progressing[\"status\"]}}\n\n{{?? pydata.error_key == \"CLUSTER_VERSION_WORKING_TOWARDS\" }}\nThe cluster is working towards {{=pydata.desired}}. The Cluster Version Operator does not indicate any issues otherwise.\n\n**Current version:** {{=pydata.current}}<br/>\n**Desired version:** {{=pydata.desired}}<br/>\n**Available:** {{=pydata.available[\"status\"]}}\n\n**Progressing:** {{=pydata.progressing[\"status\"]}}<br/>\n{{=pydata.progressing[\"message\"]}}\n\n{{?? pydata.error_key == \"CLUSTER_VERSION_ERRORS\" }}\nThe Cluster Version Operator is indicating issues.\n\n**Current version:** {{=pydata.current}}<br/>\n**Desired version:** {{=pydata.desired}}<br/>\n**Available:** {{=pydata.available[\"status\"]}}\n\n**Progressing:** {{=pydata.progressing[\"status\"]}}{{? pydata.progressing[\"reason\"] }} ({{=pydata.progressing[\"reason\"]}}){{?}}<br/>\n{{=pydata.progressing[\"message\"]}}{{? pydata.failing[\"status\"] }}\n\n**Failing:** {{=pydata.failing[\"status\"]}}{{? pydata.failing[\"reason\"] }} ({{=pydata.failing[\"reason\"]}}){{?}}<br/>\n{{=pydata.failing[\"message\"]}}{{?}}{{? pydata.history_count == 1 }}\n\n**Note:**<br/>\nThe cluster is at or working towards its initial version.{{?}}\n\n{{?}}\n",
          "HasReason": true
        },
        "CLUSTER_VERSION_WORKING_TOWARDS": {
          "metadata": {
            "description": "The cluster is upgrading to the desired version",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The cluster is working towards the desired version. The Cluster Version Operator does not indicate any issues otherwise.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "{{? pydata.pass_key == \"CLUSTER_VERSION_DONE_APPLYING\" }}\nThe Cluster has successfully completed installation of {{=pydata.current}}.\n\n**Current version:** {{=pydata.current}}<br/>\n**Desired version:** {{=pydata.desired}}<br/>\n**Available:** {{=pydata.available[\"status\"]}}<br/>\n**Progressing:** {{=pydata.progressing[\"status\"]}}\n\n{{?? pydata.error_key == \"CLUSTER_VERSION_WORKING_TOWARDS\" }}\nThe cluster is working towards {{=pydata.desired}}. The Cluster Version Operator does not indicate any issues otherwise.\n\n**Current version:** {{=pydata.current}}<br/>\n**Desired version:** {{=pydata.desired}}<br/>\n**Available:** {{=pydata.available[\"status\"]}}\n\n**Progressing:** {{=pydata.progressing[\"status\"]}}<br/>\n{{=pydata.progressing[\"message\"]}}\n\n{{?? pydata.error_key == \"CLUSTER_VERSION_ERRORS\" }}\nThe Cluster Version Operator is indicating issues.\n\n**Current version:** {{=pydata.current}}<br/>\n**Desired version:** {{=pydata.desired}}<br/>\n**Available:** {{=pydata.available[\"status\"]}}\n\n**Progressing:** {{=pydata.progressing[\"status\"]}}{{? pydata.progressing[\"reason\"] }} ({{=pydata.progressing[\"reason\"]}}){{?}}<br/>\n{{=pydata.progressing[\"message\"]}}{{? pydata.failing[\"status\"] }}\n\n**Failing:** {{=pydata.failing[\"status\"]}}{{? pydata.failing[\"reason\"] }} ({{=pydata.failing[\"reason\"]}}){{?}}<br/>\n{{=pydata.failing[\"message\"]}}{{?}}{{? pydata.history_count == 1 }}\n\n**Note:**<br/>\nThe cluster is at or working towards its initial version.{{?}}\n\n{{?}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "{{? pydata.pass_key == \"CLUSTER_VERSION_DONE_APPLYING\" }}\nThe Cluster has successfully completed installation of {{=pydata.current}}.\n\n**Current version:** {{=pydata.current}}<br/>\n**Desired version:** {{=pydata.desired}}<br/>\n**Available:** {{=pydata.available[\"status\"]}}<br/>\n**Progressing:** {{=pydata.progressing[\"status\"]}}\n\n{{?? pydata.error_key == \"CLUSTER_VERSION_WORKING_TOWARDS\" }}\nThe cluster is working towards {{=pydata.desired}}. The Cluster Version Operator does not indicate any issues otherwise.\n\n**Current version:** {{=pydata.current}}<br/>\n**Desired version:** {{=pydata.desired}}<br/>\n**Available:** {{=pydata.available[\"status\"]}}\n\n**Progressing:** {{=pydata.progressing[\"status\"]}}<br/>\n{{=pydata.progressing[\"message\"]}}\n\n{{?? pydata.error_key == \"CLUSTER_VERSION_ERRORS\" }}\nThe Cluster Version Operator is indicating issues.\n\n**Current version:** {{=pydata.current}}<br/>\n**Desired version:** {{=pydata.desired}}<br/>\n**Available:** {{=pydata.available[\"status\"]}}\n\n**Progressing:** {{=pydata.progressing[\"status\"]}}{{? pydata.progressing[\"reason\"] }} ({{=pydata.progressing[\"reason\"]}}){{?}}<br/>\n{{=pydata.progressing[\"message\"]}}{{? pydata.failing[\"status\"] }}\n\n**Failing:** {{=pydata.failing[\"status\"]}}{{? pydata.failing[\"reason\"] }} ({{=pydata.failing[\"reason\"]}}){{?}}<br/>\n{{=pydata.failing[\"message\"]}}{{?}}{{? pydata.history_count == 1 }}\n\n**Note:**<br/>\nThe cluster is at or working towards its initial version.{{?}}\n\n{{?}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5286351",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.invalid_ca_cert_causing_install_failure"
      },
      "error_keys": {
        "INSTALL_FAILURE_DUE_TO_WRONG_CA_CERT_PASSED": {
          "metadata": {
            "description": "Rule to check if IPI installation on RHV failing while creating a worker nodes due to wrong CA cert",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Rule to check if IPI installation on RHV failing while creating a worker nodes due to wrong CA cert.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1860143) for more information.",
          "reason": "Ingress and Monitoring Operators are in degraded state.<br>\n\nFollowing error messages have been found :<br>\n\nPod : {{=pydata.log[0][\"pod_name\"]}}<br>\n<br>\nMessage:<br>\n<br>\n{{=pydata.log[0][\"message\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1860143) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5100521",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.check_sdi_preload_kernel_modules"
      },
      "error_keys": {
        "SDI_PRELOAD_KERNEL_MODULES_ERROR": {
          "metadata": {
            "description": "SAP Data Intelligence 3 installation will fail when the required kernel modules are not pre-loaded",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-03-26 12:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "sap",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "SAP Data Intelligence 3 requires several kernel modules to be pre-loaded on compute node which has the label \"node-role.kubernetes.io/sdi\". The installation fails otherwise.\n",
          "summary": "",
          "resolution": "{{?pydata.ocp_version > 4.8}}\n{{?pydata.has_machine_config}}\nRed Hat recommends that you check all compute nodes labeled with the key \"node-role.kubernetes.io/sdi\" to confirm that the required kernel modules are loaded.\n\n{{??}}\nRed Hat recommends that you create a new MachineConfig and MachineConfigPool that will pre-load the required kernel modules on all compute nodes  labeled with the key \"node-role.kubernetes.io/sdi\":\n         \n~~~   \noc create -f https://raw.githubusercontent.com/redhat-sap/sap-data-intelligence/master/snippets/mco/mc-75-worker-sap-data-intelligence.yaml\noc create -f https://raw.githubusercontent.com/redhat-sap/sap-data-intelligence/master/snippets/mco/mcp-sdi.yaml\n~~~   \n         \nIf the command produces the following error, please run the command with `oc replace -f -` instead of `oc create -f -`: \n         \n~~~   \nError from server (AlreadyExists): error when creating \"STDIN\": machineconfigs.machineconfiguration.openshift.io \"75-worker-sap-data-intelligence\" already exists\n~~~\n{{?}}\n{{??pydata.ocp_version < 4.8 }}\nRed Hat recommends that you follow the instructions in [Pre-load needed kernel modules](https://access.redhat.com/articles/5100521#preload-kernel-modules-post).\n{{?}}\n",
          "more_info": "",
          "reason": "The Openshift nodes running pods for SAP Data Intelligence 3 don't pre-load (in a permanent way) the following kernel modules: nfsd, nfsv4, ip_tables, ipt_REDIRECT, ipt_owner, iptable_nat, iptable_filter.\n\nLicense Manager for SAP Data Intelligence 3 cannot be initialized with the following error:\n  - {{=pydata.results}}\n\nPlease review [SAP Data Intelligence 3 on OpenShift Container Platform 4](https://access.redhat.com/articles/5100521#preload-kernel-modules-post) for more information.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "{{?pydata.ocp_version > 4.8}}\n{{?pydata.has_machine_config}}\nRed Hat recommends that you check all compute nodes labeled with the key \"node-role.kubernetes.io/sdi\" to confirm that the required kernel modules are loaded.\n\n{{??}}\nRed Hat recommends that you create a new MachineConfig and MachineConfigPool that will pre-load the required kernel modules on all compute nodes  labeled with the key \"node-role.kubernetes.io/sdi\":\n         \n~~~   \noc create -f https://raw.githubusercontent.com/redhat-sap/sap-data-intelligence/master/snippets/mco/mc-75-worker-sap-data-intelligence.yaml\noc create -f https://raw.githubusercontent.com/redhat-sap/sap-data-intelligence/master/snippets/mco/mcp-sdi.yaml\n~~~   \n         \nIf the command produces the following error, please run the command with `oc replace -f -` instead of `oc create -f -`: \n         \n~~~   \nError from server (AlreadyExists): error when creating \"STDIN\": machineconfigs.machineconfiguration.openshift.io \"75-worker-sap-data-intelligence\" already exists\n~~~\n{{?}}\n{{??pydata.ocp_version < 4.8 }}\nRed Hat recommends that you follow the instructions in [Pre-load needed kernel modules](https://access.redhat.com/articles/5100521#preload-kernel-modules-post).\n{{?}}\n",
      "more_info": "",
      "reason": "The Openshift nodes running pods for SAP Data Intelligence 3 don't pre-load (in a permanent way) the following kernel modules: nfsd, nfsv4, ip_tables, ipt_REDIRECT, ipt_owner, iptable_nat, iptable_filter.\n\nLicense Manager for SAP Data Intelligence 3 cannot be initialized with the following error:\n  - {{=pydata.results}}\n\nPlease review [SAP Data Intelligence 3 on OpenShift Container Platform 4](https://access.redhat.com/articles/5100521#preload-kernel-modules-post) for more information.\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5696641",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.networkpolicy_flows_for_non_pod_network_pods"
      },
      "error_keys": {
        "BUGZILLA_BUG_1914284": {
          "metadata": {
            "description": "Check the bug that generates NetworkPolicy flows for non-pod-network pods",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Check the bug that generates NetworkPolicy flows for non-pod-network pods.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1914284](https://bugzilla.redhat.com/show_bug.cgi?id=1914284) for more information.",
          "reason": "OpenShift SDN generates NetworkPolicy flows for non-pod-network pods<br>\nresults in cluster-wide DNS and connectivity issues.<br>\n\n{{for (var pod_detail in pydata.res) { }}\nPod Name    : {{=pydata.res[pod_detail][\"pod_name\"]}}\nMessage     :<br>\n{{=pydata.res[pod_detail][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1914284](https://bugzilla.redhat.com/show_bug.cgi?id=1914284) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5366111",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.route_resolves_to_localhost"
      },
      "error_keys": {
        "BUGZILLA_BUG_1829318": {
          "metadata": {
            "description": "Openshift routes resolves to local IP (127.0.0.1) inside the pod",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Openshift routes resolves to local IP (127.0.0.1) inside the pod.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1829318](https://bugzilla.redhat.com/show_bug.cgi?id=1829318) for more information.",
          "reason": "Possible known Bug Found.<br>\nSummary     : authentication operator unable to progress as it is stuck on unresolved oauth route.<br>\n\nOperator Name           : {{=pydata.state[\"name\"]}}<br>\nOperator Degraded State : {{=pydata.state[\"degraded\"][\"status\"]}}<br>\nMessage                 : {{=pydata.state[\"degraded\"][\"message\"]}}<br>\nReason                  : {{=pydata.state[\"degraded\"][\"reason\"]}}<br>\n{{for (var pod_detail in pydata.results) { }}\n\nPod Name: {{=pydata.results[pod_detail][\"pod_name\"]}}\nMessage: {{=pydata.results[pod_detail][\"message\"]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1829318](https://bugzilla.redhat.com/show_bug.cgi?id=1829318) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5589051",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.catalog_operator_index_out_of_range_with_length_0"
      },
      "error_keys": {
        "BUGZILLA_BUG_1899835": {
          "metadata": {
            "description": "This rule checks if catalog-operator crashes with \"runtimeerror:index out of range [0] with length 0\"",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule checks if catalog-operator crashes with \"runtimeerror: index out of range [0] with length 0\".",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1899835](https://bugzilla.redhat.com/show_bug.cgi?id=1899835) for more information.",
          "reason": "Possible known bug found.<br>\nSummary     : catalog-operator crashes with \"runtime error: index out of range [0] with length 0\".<br>\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage: {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1899835](https://bugzilla.redhat.com/show_bug.cgi?id=1899835) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5491081",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.prometheus_load_too_many_samples_into_memory"
      },
      "error_keys": {
        "BUGZILLA_BUG_1872786": {
          "metadata": {
            "description": "The cluster shows symptoms of BZ 1872786: A Prometheus query has hit the query.max-samples limit",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The cluster shows symptoms of BZ 1872786. A Prometheus query has hit the query.max-samples limit.\n",
          "summary": "",
          "resolution": "See [KCS 5491081](https://access.redhat.com/solutions/5491081).\n",
          "more_info": "* See [KCS 5491081](https://access.redhat.com/solutions/5491081) for more information about possible solutions.\n* See [BZ 1872786](https://bugzilla.redhat.com/show_bug.cgi?id=1872786) for more information about the bug.\n",
          "reason": "The cluster shows symptoms of BZ 1872786.\n\nPrometheus has a limit of 50M for query.max-samples. Prometheus logs indicate that a query has hit the limit.\n\n{{for (var pod_data in pydata.results) { }}\n* Pod: **{{=pydata.results[pod_data][\"pod_name\"]}}**<br/>\n  `{{=pydata.results[pod_data][\"message\"]}}`\n{{}}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "See [KCS 5491081](https://access.redhat.com/solutions/5491081).\n",
      "more_info": "* See [KCS 5491081](https://access.redhat.com/solutions/5491081) for more information about possible solutions.\n* See [BZ 1872786](https://bugzilla.redhat.com/show_bug.cgi?id=1872786) for more information about the bug.\n",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.pods_check_containers"
      },
      "error_keys": {
        "POD_CONTAINER_ISSUE": {
          "metadata": {
            "description": "Application creation failure when pods have not-ready or unstable(ones with multiple restarts) containers",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2019-10-24 07:10:00",
            "status": "active",
            "tags": [
              "openshift",
              "pod",
              "container",
              "incident"
            ]
          },
          "total_risk": 2,
          "generic": "One or more containers (within pods) are not-ready or have minimum 10 restarts.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The following containers are experiencing issues:\n{{~ pydata.containers :container }}\n    {{=container[\"name\"]}}\n    Pod of the effected container: {{=container[\"pod\"]}}\n    Ready: {{=container[\"ready\"]}}\n    Restart count: {{=container[\"restarts\"]}}\n{{~}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "The following containers are experiencing issues:\n{{~ pydata.containers :container }}\n    {{=container[\"name\"]}}\n    Pod of the effected container: {{=container[\"pod\"]}}\n    Ready: {{=container[\"ready\"]}}\n    Restart count: {{=container[\"restarts\"]}}\n{{~}}",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5367151",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.pruner_degrades_image_registry_operator_if_registry_removed"
      },
      "error_keys": {
        "PRUNER_DEGRADES_IMAGE_REGISTRY_OPERATOR_IF_REGISTRY_REMOVED": {
          "metadata": {
            "description": "The issue is if image registry is removed, the pruner degradesthe operator with the reason ImagePrunerJobFailed::Removed",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The issue is if image registry is removed, the pruner degradesthe operator with the reason ImagePrunerJobFailed::Removed.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1867792](https://bugzilla.redhat.com/show_bug.cgi?id=1867792) for more information.",
          "reason": "Pruner degrades image-registry operator if the registry is removed.<br>\n\nOperator name  : {{=pydata.operator[\"name\"]}}<br>\nDegraded status: {{=pydata.operator[\"degraded\"][\"status\"]}}<br>\nMessage        : {{=pydata.operator[\"degraded\"][\"message\"]}}<br>\nReason         : {{=pydata.operator[\"degraded\"][\"reason\"]}}<br>\nLast Transition: {{=pydata.operator[\"degraded\"][\"last_trans_time\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1867792](https://bugzilla.redhat.com/show_bug.cgi?id=1867792) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5382331",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.ovnkube_master_stuck_in_crashloopbackoff"
      },
      "error_keys": {
        "OVNKUKBE_MASTER_POD_CRASHLOOPBACKOFF": {
          "metadata": {
            "description": "`ovnkube-master` pod in `CrashLoopBackOff` in RHOCP 4.x with `ovn-kubenetes`",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "`ovnkube-master` pod in `CrashLoopBackOff` in RHOCP 4.x with `ovn-kubenetes`.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "{{? pydata.op_state[\"progressing\"][\"status\"] }}\nThe network operator switched to Progressing state as ovnkube-master pod is stuck in CrashLoopBackOff state.<br>\n{{?? pydata.op_state[\"degraded\"][\"message\"] }}\nThe network operator switched to Degraded state as ovnkube-master pod is stuck in CrashLoopBackOff state.<br>\n{{?}}\n\n{{for (var pod_detail in pydata.result) { }}\nPod Name    : {{=pydata.result[pod_detail][\"pod_name\"]}}<br>\nMessage     : {{=pydata.result[pod_detail][\"message\"]}}<br>\n{{}}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5188241",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.auth_operator_degraded_in_multitenant_plugin"
      },
      "error_keys": {
        "CLUSTER_AUTH_OPERATOR_DEGRADED_WITH_ERROR_MESSAGE": {
          "metadata": {
            "description": "This is a rule for checking Authentication Operator is in degraded state.It prevents the cluster from being upgraded and causes issue authenticatinginto the cluster.* Author:Arpit Jain <arpjain@redhat.com>* KCS:https://access.redhat.com/solutions/5188241* JIRA:https://issues.redhat.com/browse/INSIGHTOCP-81",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This is a rule for checking Authentication Operator is in degraded state.It prevents the cluster from being upgraded and causes issue authenticatinginto the cluster.* Author    : Arpit Jain <arpjain@redhat.com>* KCS       : https://access.redhat.com/solutions/5188241* JIRA      : https://issues.redhat.com/browse/INSIGHTOCP-81.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Cluster Version : {{=pydata.current}}\nOperator Name   : {{=pydata.op[\"name\"]}}<br>\nDegraded Status : {{=pydata.op[\"degraded\"][\"status\"]}}<br>\n<br>\nPlease check the network isolation mode. For Openshift version 4.4.0 to 4.4.6<br>\nopenshift-authentication and openshift-authentication-operator namespaces have<br>\ndifferent netid. Hence in  multitenant mode authentication-operator pod cannot<br>\ncommunicate with oauth-openshift pods.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.log_spam_after_applying_scheduler_policy_file"
      },
      "error_keys": {
        "BUGZILLA_BUG_1896833": {
          "metadata": {
            "description": "After applying a scheduler policy file the openshift-kube-scheduler-operator goes into a loop, constantly reporting that 'ObservedConfigMapNameChanged' scheduler configmap changed",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "After applying a scheduler policy file the openshift-kube-scheduler-operator goes into a loop, constantly reporting that 'ObservedConfigMapNameChanged' scheduler configmap changed.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1896833](https://bugzilla.redhat.com/show_bug.cgi?id=1896833) for more information.",
          "reason": "Possible known bug found.<br>\n\nAfter applying a scheduler policy file the openshift-kube-scheduler-operator goes into a loop,<br>\nconstantly reporting that 'ObservedConfigMapNameChanged' scheduler configmap changed.<br>\n\nLog spamming from openshift-kube-scheduler-operator detected:<br>\n\nPod:            {{=pydata.log[\"pod_name\"]}}<br>\nMessage count:  {{=pydata.log[\"count\"]}}<br>\nSample message: {{=pydata.log[\"message\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1896833](https://bugzilla.redhat.com/show_bug.cgi?id=1896833) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.container_max_root_partition_size"
      },
      "error_keys": {
        "CONTAINER_ROOT_PARTITION_SIZE": {
          "metadata": {
            "description": "Containers leak available disk space of the underlying host when there is no ContainerRuntimeConfig",
            "impact": 1,
            "likelihood": 4,
            "publish_date": "2020-11-17 11:10:00",
            "status": "active",
            "tags": [
              "openshift",
              "security"
            ]
          },
          "total_risk": 2,
          "generic": "If there is no ContainerRuntimeConfig, the root partition of a container leaks the available disk space of the underlying host.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you perform the following steps:\n1. Create the ContainerRuntimeConfig CRD, like the following example:\n~~~\n# cat << EOF| oc create -f -\napiVersion: v1\nkind: ContainerRuntimeConfig\nmetadata:\n name: xxx\nspec:\n machineConfigPoolSelector:\n   matchLabels:\n     match_key_xxx: match_value_yyy\n...\nEOF\n~~~\n\n2. To apply the new container runtime configuration to your nodes, add the matchLabels name you set in the ContainerRuntimeConfig CRD to the corresponding machineconfigpool, like the following example:\n~~~\n# oc edit machineconfigpool node_name\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfigPool\nmetadata:\n  labels:\n    match_key_xxx: match_value_yyy\n~~~\n\nFor more information, see the Red Hat document [Setting the default max container root partition size for overlay with cri-o](https://access.redhat.com/solutions/5216861)\n",
          "more_info": "",
          "reason": "* There {{?Object.keys(pydata.mcps).length>1}}are machinepools that don't have{{??}}a machinepool that doesn't have{{?}} the ContainerRuntimeConfig:\n{{ for (var index in pydata.mcps) { }}\n - **{{=pydata.mcps[index]}}**\n{{?}}\n\nThat allows containers to show the available disk space of the underlying host.\n\nPlease review [Setting the default max container root partition size for overlay with cri-o](https://access.redhat.com/solutions/5216861) for more information.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you perform the following steps:\n1. Create the ContainerRuntimeConfig CRD, like the following example:\n~~~\n# cat << EOF| oc create -f -\napiVersion: v1\nkind: ContainerRuntimeConfig\nmetadata:\n name: xxx\nspec:\n machineConfigPoolSelector:\n   matchLabels:\n     match_key_xxx: match_value_yyy\n...\nEOF\n~~~\n\n2. To apply the new container runtime configuration to your nodes, add the matchLabels name you set in the ContainerRuntimeConfig CRD to the corresponding machineconfigpool, like the following example:\n~~~\n# oc edit machineconfigpool node_name\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfigPool\nmetadata:\n  labels:\n    match_key_xxx: match_value_yyy\n~~~\n\nFor more information, see the Red Hat document [Setting the default max container root partition size for overlay with cri-o](https://access.redhat.com/solutions/5216861)\n",
      "more_info": "",
      "reason": "* There {{?Object.keys(pydata.mcps).length>1}}are machinepools that don't have{{??}}a machinepool that doesn't have{{?}} the ContainerRuntimeConfig:\n{{ for (var index in pydata.mcps) { }}\n - **{{=pydata.mcps[index]}}**\n{{?}}\n\nThat allows containers to show the available disk space of the underlying host.\n\nPlease review [Setting the default max container root partition size for overlay with cri-o](https://access.redhat.com/solutions/5216861) for more information.\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.pods_check"
      },
      "error_keys": {
        "POD_HEALTHY": {
          "metadata": {
            "description": "All pods are in a desired state",
            "impact": 1,
            "likelihood": 1,
            "publish_date": "2019-11-03 08:25:00",
            "status": "active",
            "tags": null
          },
          "total_risk": 1,
          "generic": "All pods are in a desired state.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The following pods are experiencing issues:\n{{~ pydata.pods :pod }}\n    {{=pod[\"name\"]}}\n    Ready: {{=pod[\"ready\"][\"status\"]}}\n    Initialized: {{=pod[\"initialized\"][\"status\"]}}\n    Pod_scheduled: {{=pod[\"pod_scheduled\"][\"status\"]}}\n    Containers_ready: {{=pod[\"containers_ready\"][\"status\"]}}\n{{~}}",
          "HasReason": true
        },
        "POD_ISSUE": {
          "metadata": {
            "description": "If an pod is in Containers not ready/Not initialized/Not ready/Not scheduled state, then its having an issue that needs further investigation",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2019-11-03 08:25:00",
            "status": "active",
            "tags": [
              "openshift",
              "pod",
              "incident"
            ]
          },
          "total_risk": 2,
          "generic": "One or more pods are in one of the following unhealthy states:\n- Containers not ready\n- Not initialized\n- Not ready\n- Not scheduled\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The following pods are experiencing issues:\n{{~ pydata.pods :pod }}\n    {{=pod[\"name\"]}}\n    Ready: {{=pod[\"ready\"][\"status\"]}}\n    Initialized: {{=pod[\"initialized\"][\"status\"]}}\n    Pod_scheduled: {{=pod[\"pod_scheduled\"][\"status\"]}}\n    Containers_ready: {{=pod[\"containers_ready\"][\"status\"]}}\n{{~}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "The following pods are experiencing issues:\n{{~ pydata.pods :pod }}\n    {{=pod[\"name\"]}}\n    Ready: {{=pod[\"ready\"][\"status\"]}}\n    Initialized: {{=pod[\"initialized\"][\"status\"]}}\n    Pod_scheduled: {{=pod[\"pod_scheduled\"][\"status\"]}}\n    Containers_ready: {{=pod[\"containers_ready\"][\"status\"]}}\n{{~}}",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.etcd_quorum_guard_pending_container_creating"
      },
      "error_keys": {
        "ETCD_QUORUM_GUARD_PENDING_CONTAINER_CREATING": {
          "metadata": {
            "description": "Etcd-quorum-guard pending ContainerCreating on updated control-plane node",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Etcd-quorum-guard pending ContainerCreating on updated control-plane node.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bug](https://bugzilla.redhat.com/show_bug.cgi?id=1899316) for more information.",
          "reason": "The etcd-quorum-guard are pending ContainerCreating on updated control-plane node more than 1 hours.<br>\n\n{{for (var pod in pydata.pods) { }}\n- {{=pydata.pods[pod]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bug](https://bugzilla.redhat.com/show_bug.cgi?id=1899316) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5320331",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.image_push_fails_unknown_blob"
      },
      "error_keys": {
        "IMAGE_PUSH_FAILS_UNKNOWN_BLOB": {
          "metadata": {
            "description": "Error \"manifest blob unknown blob unknown to registry\" while pushing images to registry",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Error \"manifest blob unknown blob unknown to registry\" while pushing images to registry.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Summary: With NFS as backend storage for image-registry, the image push fails with \"manifest blob unknown blob unknown to registry\" errors.<br>\n\nPod Names:\n{{for (var pod_detail in pydata.results) { }}\n- {{=pydata.results[pod_detail][\"pod_name\"]}}\n{{}}}\nConcerning Logs:\n{{=pydata.results[0][\"message\"]}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.apiserver_down_on_vsphere_48"
      },
      "error_keys": {
        "BUGZILLA_BUG_1987108": {
          "metadata": {
            "description": "kube-apiserver cannot reach aggregated apiserver after upgrade to 4.8 on VSphere",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "kube-apiserver cannot reach aggregated apiserver after upgrade to 4.8 on VSphere.\n",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1987108](https://bugzilla.redhat.com/show_bug.cgi?id=1987108) for more information.",
          "reason": "Possible known bug found.\n\nWhile upgrading to or deploying the OCP v4.8 cluster on VSphere, the kube-apiserver cannot reach aggregated apiserver with 'no route to host' messages.\n\n* **Operator Name:** {{=pydata.result[\"name\"]}}\n* **Available:** {{=pydata.result[\"available\"][\"status\"]}}\n* **Progressing:** {{=pydata.result[\"progressing\"][\"status\"]}}\n* **Degraded:** {{=pydata.result[\"degraded\"][\"status\"]}}\n* **Reason:** {{=pydata.result[\"available\"][\"reason\"]}}\n* **Message:**<br/>\n  `{{=pydata.result[\"available\"][\"message\"]}}`\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1987108](https://bugzilla.redhat.com/show_bug.cgi?id=1987108) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.nodes_not_ready"
      },
      "error_keys": {
        "ALL_NODES_READY": {
          "metadata": {
            "description": "All nodes are ready",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "All nodes are in the Ready status.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "{{? pydata.not_ready_count == 0}}All nodes are in the Ready status.\n{{?? pydata.not_ready_count == 1}}1 node is not in the Ready status.\n{{??}}{{=pydata.not_ready_count}} nodes are not in the Ready status.\n{{?}}\n\n{{~ pydata.nodes:n}}\n{{? ! n[\"ready\"] }}\n* **{{=n[\"name\"]}}**<br/>\n  Roles: {{=n[\"roles\"]}}<br/>\n  Ready: {{=n[\"ready\"]}}<br/>\n  {{=n[\"message\"]}}{{?}}{{~}}\n",
          "HasReason": true
        },
        "NODES_NOT_READY": {
          "metadata": {
            "description": "Some nodes are not ready",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Some nodes are not in the Ready status.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "{{? pydata.not_ready_count == 0}}All nodes are in the Ready status.\n{{?? pydata.not_ready_count == 1}}1 node is not in the Ready status.\n{{??}}{{=pydata.not_ready_count}} nodes are not in the Ready status.\n{{?}}\n\n{{~ pydata.nodes:n}}\n{{? ! n[\"ready\"] }}\n* **{{=n[\"name\"]}}**<br/>\n  Roles: {{=n[\"roles\"]}}<br/>\n  Ready: {{=n[\"ready\"]}}<br/>\n  {{=n[\"message\"]}}{{?}}{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "{{? pydata.not_ready_count == 0}}All nodes are in the Ready status.\n{{?? pydata.not_ready_count == 1}}1 node is not in the Ready status.\n{{??}}{{=pydata.not_ready_count}} nodes are not in the Ready status.\n{{?}}\n\n{{~ pydata.nodes:n}}\n{{? ! n[\"ready\"] }}\n* **{{=n[\"name\"]}}**<br/>\n  Roles: {{=n[\"roles\"]}}<br/>\n  Ready: {{=n[\"ready\"]}}<br/>\n  {{=n[\"message\"]}}{{?}}{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5828401",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.prometheus_duplicate_timestamps"
      },
      "error_keys": {
        "ERROR_PROMETHEUS_DUPLICATE_TIMESTAMPS": {
          "metadata": {
            "description": "PrometheusDuplicateTimestamps alerts after the upgrade of the OpenShift Serverless operator",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "PrometheusDuplicateTimestamps alerts after the upgrade of the OpenShift Serverless operator.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "When ServiceMonitors and Services are accidentally not deleted during the upgrade of the<br>\nOpenShift Serverless operator, prometheus will sent the PrometheusDuplicateTimestamps alerts.<br>\n\n{{for (var log in pydata.logs) { }}\nPod Name    : {{=pydata.logs[log][\"pod_name\"]}}\nMessage     :<br>\n{{=pydata.logs[log][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5252831",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.upgrade_to_ocp47_fails_on_vsphere"
      },
      "error_keys": {
        "RHBZ_1941714_PROACTIVE": {
          "metadata": {
            "description": "OpenShift Container Platform upgrade from 4.6 to 4.7 will fail when running on vSphere due to a kernel bug",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-05-20 16:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift",
              "vmxnet3"
            ]
          },
          "total_risk": 2,
          "generic": "OpenShift Container Platform upgrade from 4.6 to [4.7.0 ~ 4.7.10] will fail when running on vSphere due to a kernel bug in RHCOS 4.7. RHCOS 4.7 includes an updated VMXNET3 driver that causes dropped packet issues across the cluster when the cluster is running on a VMware virtual hardware version greater than 13.",
          "summary": "",
          "resolution": "Red Hat recommends that you apply the `99-vsphere-networking-fix` MachineConfig to all OpenShift nodes by following the steps in the [Knowledge Base Solution](https://access.redhat.com/solutions/5997331).\n",
          "more_info": "",
          "reason": "{{?pydata.ocp_version46}}\nThe OpenShift cluster is running on vSphere with the VMware virtual hardware version **{{=pydata.hw_version}}**. The upgrade to desired OpenShift Container version **{{=pydata.desired_version}}** will fail on this VMware virtual hardware version due to a [kernel bug](https://bugzilla.redhat.com/show_bug.cgi?id=1941714) which causes dropped packet issues across the cluster.\n{{??}}\nThe OpenShift cluster is running on vSphere with the VMware virtual hardware version **{{=pydata.hw_version}}**. The upgrade to OpenShift Container Platform **4.7** fails on this VMware virtual hardware version due to a [kernel bug](https://bugzilla.redhat.com/show_bug.cgi?id=1941714) which causes dropped packet issues across the cluster.\n{{?}}",
          "HasReason": true
        },
        "RHBZ_1941714_REACTIVE": {
          "metadata": {
            "description": "OpenShift Container Platform upgrade from 4.6 to 4.7 fails when running on vSphere due to a kernel bug",
            "impact": 2,
            "likelihood": 4,
            "publish_date": "2021-04-30 16:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift",
              "vmxnet3"
            ]
          },
          "total_risk": 3,
          "generic": "OpenShift Container Platform upgrade from 4.6 to 4.7 fails when running on vSphere due to a kernel bug in RHCOS 4.7. RHCOS 4.7 includes an updated VMXNET3 driver that causes dropped packet issues across the cluster when the cluster is running on a VMware virtual hardware version greater than 13.",
          "summary": "",
          "resolution": "Red Hat recommends that you apply the `99-vsphere-networking-fix` MachineConfig to all OpenShift nodes by following the steps in the [Knowledge Base Solution](https://access.redhat.com/solutions/5997331).\n",
          "more_info": "",
          "reason": "{{?pydata.ocp_version46}}\nThe OpenShift cluster is running on vSphere with the VMware virtual hardware version **{{=pydata.hw_version}}**. The upgrade to desired OpenShift Container version **{{=pydata.desired_version}}** will fail on this VMware virtual hardware version due to a [kernel bug](https://bugzilla.redhat.com/show_bug.cgi?id=1941714) which causes dropped packet issues across the cluster.\n{{??}}\nThe OpenShift cluster is running on vSphere with the VMware virtual hardware version **{{=pydata.hw_version}}**. The upgrade to OpenShift Container Platform **4.7** fails on this VMware virtual hardware version due to a [kernel bug](https://bugzilla.redhat.com/show_bug.cgi?id=1941714) which causes dropped packet issues across the cluster.\n{{?}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you apply the `99-vsphere-networking-fix` MachineConfig to all OpenShift nodes by following the steps in the [Knowledge Base Solution](https://access.redhat.com/solutions/5997331).\n",
      "more_info": "",
      "reason": "{{?pydata.ocp_version46}}\nThe OpenShift cluster is running on vSphere with the VMware virtual hardware version **{{=pydata.hw_version}}**. The upgrade to desired OpenShift Container version **{{=pydata.desired_version}}** will fail on this VMware virtual hardware version due to a [kernel bug](https://bugzilla.redhat.com/show_bug.cgi?id=1941714) which causes dropped packet issues across the cluster.\n{{??}}\nThe OpenShift cluster is running on vSphere with the VMware virtual hardware version **{{=pydata.hw_version}}**. The upgrade to OpenShift Container Platform **4.7** fails on this VMware virtual hardware version due to a [kernel bug](https://bugzilla.redhat.com/show_bug.cgi?id=1941714) which causes dropped packet issues across the cluster.\n{{?}}",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5743951",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.continous_reconciling_op_messages"
      },
      "error_keys": {
        "BUGZILLA_BUG_1917537": {
          "metadata": {
            "description": "Controllers continuously busy in reconciling operator",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Controllers continuously busy in reconciling operator.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1917537](https://bugzilla.redhat.com/show_bug.cgi?id=1917537) for more information.",
          "reason": "Possible known Bug Found.<br>\nSummary     : Due to continuous reconciling request to OLM operator, kube-apiserver and etcd pods tend to consume huge amount of resources leading to an unstable cluster.{{for (var pod_detail in pydata.result) { }}\n\nPod Name: {{=pydata.result[pod_detail][\"pod_name\"]}}\nMessage: {{=pydata.result[pod_detail][\"message\"]}}{{}}}\n\nFor detailed logs, please refer to the respective pods.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1917537](https://bugzilla.redhat.com/show_bug.cgi?id=1917537) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.certificates_not_approved"
      },
      "error_keys": {
        "CERTIFICATES_NOT_APPROVED": {
          "metadata": {
            "description": "Certificates are not approved",
            "impact": 2,
            "likelihood": 4,
            "publish_date": "2020-01-09 09:21:00",
            "status": "active",
            "tags": null
          },
          "total_risk": 3,
          "generic": "Certificates are not approved.",
          "summary": "",
          "resolution": "Red Hat recommends you to follow steps in [approving the CSRs for your machines](https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html#installation-approve-csrs_installing-bare-metal).",
          "more_info": "",
          "reason": "Certificates are not approved.\n\nList of unapproved certificates.\n\n{{~ pydata.unapproved :cert }}\n* {{=cert.name}}\n{{~}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends you to follow steps in [approving the CSRs for your machines](https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html#installation-approve-csrs_installing-bare-metal).",
      "more_info": "",
      "reason": "Certificates are not approved.\n\nList of unapproved certificates.\n\n{{~ pydata.unapproved :cert }}\n* {{=cert.name}}\n{{~}}",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5644661",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.cluster_unstable_due_to_prometheus_operator_bug"
      },
      "error_keys": {
        "BUGZILLA_BUG_1891815": {
          "metadata": {
            "description": "The OpenShift cluster became unstable due to a prometheus operator bug",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The OpenShift cluster became unstable due to a prometheus operator bug.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1891815](https://bugzilla.redhat.com/show_bug.cgi?id=1891815) for more information.",
          "reason": "The OpenShift cluster was unstable due to a prometheus operator bug.<br>\nWe detect the following error message.<br>\n\nThe affected pods are listed below.<br>\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage : {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1891815](https://bugzilla.redhat.com/show_bug.cgi?id=1891815) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.prometheus_invalid_syntax"
      },
      "error_keys": {
        "PROMETHEUS_INVALID_SYNTAX": {
          "metadata": {
            "description": "invalid syntax error to list PrometheusRule/ServiceMonitor",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "invalid syntax error to list PrometheusRule/ServiceMonitor.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1858991](https://bugzilla.redhat.com/show_bug.cgi?id=1858991) for more information.",
          "reason": "\"invalid syntax error to list PrometheusRule/ServiceMonitor\" error has been observed.<br>\n\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage : {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1858991](https://bugzilla.redhat.com/show_bug.cgi?id=1858991) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6080951",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.pods_pending_for_pvc_cannot_be_bound"
      },
      "error_keys": {
        "PODS_PENDING_FOR_PVC_CANNOT_BE_BOUND": {
          "metadata": {
            "description": "This rule will check if Pods are pending for PVC cannot be bound",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule will check if Pods are pending for PVC cannot be bound.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The following pods are pending for PVC cannot be bound:<br>\n\n{{for (var key in pydata.results) { }}\nPod: {{=key}}\nMessage: {{=pydata.results[key]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4606201",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.vsphere_vm_disk_enableuuid_not_set_to_true"
      },
      "error_keys": {
        "VSPHERE_VM_DISK_ENABLEUUID_NOT_SET_TO_TRUE": {
          "metadata": {
            "description": "Pods couldn't find attached VMDK on VMware node if disk.EnableUUID is not set to TRUE on the node VM",
            "impact": 1,
            "likelihood": 4,
            "publish_date": "2021-07-16 12:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "vsphere",
              "disk"
            ]
          },
          "total_risk": 2,
          "generic": "The parameter disk.EnableUUID is necessary so that the VMDK always presents a consistent UUID to the VM, thus allowing the disk to be mounted properly. Pods couldn't find attached VMDK on VMware node if disk.EnableUUID is not set to TRUE on the node VM.",
          "summary": "",
          "resolution": "Red Hat recommends that you set `disk.EnableUUID` to **TRUE** for all VMs.\n\n* Gather all the vm path, refer to the [Knowledge Base Solution](https://access.redhat.com/solutions/4606201) to retrieve the *govc* release.\n~~~\n# govc ls /datacenter/vm/<vm-folder-name>\n~~~\n\n* Set disk.EnableUUID to TRUE for all VMs, it is needed the node VM path and the VM being *offline*.\n~~~\n# govc vm.change -e=\"disk.EnableUUID=1\" -vm='VM Path'\n~~~\n\n* Check the vmx file to verify the setting is activated after it is booted up again.",
          "more_info": "",
          "reason": "In the OCP cluster, The parameter disk.EnableUUID is not set to true for the following vm nodes.\n{{~pydata.affected_nodes:node}}\n{{=node}}\n{{~}}\nThe parameter disk.EnableUUID is necessary so that the VMDK always presents a consistent UUID to the VM, thus allowing the disk to be mounted properly. Pods couldn't find attached VMDK on VMware node if disk.EnableUUID is not set to TRUE on the node VM.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you set `disk.EnableUUID` to **TRUE** for all VMs.\n\n* Gather all the vm path, refer to the [Knowledge Base Solution](https://access.redhat.com/solutions/4606201) to retrieve the *govc* release.\n~~~\n# govc ls /datacenter/vm/<vm-folder-name>\n~~~\n\n* Set disk.EnableUUID to TRUE for all VMs, it is needed the node VM path and the VM being *offline*.\n~~~\n# govc vm.change -e=\"disk.EnableUUID=1\" -vm='VM Path'\n~~~\n\n* Check the vmx file to verify the setting is activated after it is booted up again.",
      "more_info": "",
      "reason": "In the OCP cluster, The parameter disk.EnableUUID is not set to true for the following vm nodes.\n{{~pydata.affected_nodes:node}}\n{{=node}}\n{{~}}\nThe parameter disk.EnableUUID is necessary so that the VMDK always presents a consistent UUID to the VM, thus allowing the disk to be mounted properly. Pods couldn't find attached VMDK on VMware node if disk.EnableUUID is not set to TRUE on the node VM.",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.helm_chart_proxy_connection_timed_out"
      },
      "error_keys": {
        "HELM_CHART_PROXY_CONNECTION_TIMED_OUT": {
          "metadata": {
            "description": "helm/chartproxy:Error retrieving index file in OCP4.6",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": " helm/chartproxy: Error retrieving index file in OCP4.6.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1918748](https://bugzilla.redhat.com/show_bug.cgi?id=1918748) for more information.",
          "reason": "Configured HelmChartProxy does not appear in Developer Console in a<br>\ndisconnected environment with cluster-wide Proxy configured.<br>\n\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage : {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1918748](https://bugzilla.redhat.com/show_bug.cgi?id=1918748) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5136961",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.auth_operator_degraded_due_to_old_domain_name"
      },
      "error_keys": {
        "AUTH_OPERATOR_DEGRADED_DUE_TO_BAD_DOMAIN": {
          "metadata": {
            "description": "This rule checks if authentication operator is in degraded state due to the wrongdomain entry in status section of default ingress controller",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule checks if authentication operator is in degraded state due to the wrongdomain entry in status section of default ingress controller.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Operator Name           : {{=pydata.state[\"name\"]}}<br>\nOperator Degraded State : {{=pydata.state[\"degraded\"][\"status\"]}}<br>\nMessage                 : {{=pydata.state[\"degraded\"][\"message\"]}}<br>\nReason                  : {{=pydata.state[\"degraded\"][\"reason\"]}}<br>\n<br>\nPlease check if the status field in the default ingresscontroller is still having<br>\nthe old/wrong domain name. If yes, then  recreate it with the correct domain.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4602641",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.nodes_kubelet_version_check"
      },
      "error_keys": {
        "NODE_KUBELET_VERSION": {
          "metadata": {
            "description": "Kubelet versions differ among cluster nodes",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Kubelet versions differ among cluster nodes. This is usually a symptom of an\nunfinished upgrade. The symptom may be caused by many different reasons\nincluding (but not limited to):\n\n* Nodes are not assigned to Machine Config Pools correctly\n* Machine Configs are not assigned to Machine Config Pools\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Found nodes with different kubelet versions.\n\n{{~ pydata.occurences:o }}\n* {{=o[0]}} ({{=o[1]}} node{{?o[1]>1}}s{{?}}){{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5594541",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.prometheus_rule_evaluation_fail"
      },
      "error_keys": {
        "PROMETHEUS_RULE_EVALUATION_FAIL": {
          "metadata": {
            "description": "Prometheus has failed to evaluate some recording rules due to an OpenShift bug",
            "impact": 2,
            "likelihood": 4,
            "publish_date": "2021-04-16 16:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift"
            ]
          },
          "total_risk": 3,
          "generic": "Prometheus time series are not available when Prometheus fails to evaluate recording rules that generate them.",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade to 4.6.16 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.6/updating/updating-cluster-between-minor.html) for the steps.",
          "more_info": "For more in-depth information please check the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1879520).",
          "reason": "This OpenShift cluster is running in the version {{=pydata.version}}. \nDue to an OpenShift bug in this version, Prometheus fails to evaluate the following recording rules. The corresponding Prometheus time series are not available.\n\n- cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum\n- cluster:kubelet_volume_stats_used_bytes:provisioner:sum",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you upgrade to 4.6.16 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.6/updating/updating-cluster-between-minor.html) for the steps.",
      "more_info": "For more in-depth information please check the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1879520).",
      "reason": "This OpenShift cluster is running in the version {{=pydata.version}}. \nDue to an OpenShift bug in this version, Prometheus fails to evaluate the following recording rules. The corresponding Prometheus time series are not available.\n\n- cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum\n- cluster:kubelet_volume_stats_used_bytes:provisioner:sum",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4833531",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.control_plane_replicas"
      },
      "error_keys": {
        "CONTROL_PLANE_NODE_REPLICAS": {
          "metadata": {
            "description": "OpenShift cluster with more or less than 3 control plane node replicas is not supported by Red Hat",
            "impact": 1,
            "likelihood": 4,
            "publish_date": "2021-01-22 12:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "etcd",
              "performance"
            ]
          },
          "total_risk": 2,
          "generic": "OpenShift cluster with control plane node replicas other than 3 makes the cluster unsupported. Three control plane nodes are required because of the etcd cluster. More control plane nodes increase complexity of the system, and could lead to decreased performance.\n",
          "summary": "",
          "resolution": "{{?pydata.replica_count > 3}}\nRed Hat recommends to scale down the control plane to exactly 3 nodes.\n{{??pydata.replica_count < 3}}\nRed Hat recommends to scale up the control plane to exactly 3 nodes.\n{{?}}\n\nIf feasible, cluster re-installation is also an option to meet the supportability criteria.",
          "more_info": "Please review the [Official Documentation](https://docs.openshift.com/container-platform/{{=pydata.version}}/architecture/control-plane.html#defining-masters_control-plane) for more information.\n",
          "reason": "On the OpenShift cluster, there are {{=pydata.replica_count}} control plane nodes. Currently, the supported replica count for control plane nodes in the OpenShift cluster is 3.\n\n{{?pydata.replica_count > 3}}\nWith {{=pydata.replica_count}} members, the quorum has to pass traffic to all the members and wait for consensus from more members before committing a change. This introduces latency to etcd, which in turn increases latency for the API server and cluster operators impacting cluster's overall performance.\n{{??pydata.replica_count < 3}}\nWith {{=pydata.replica_count}} members, the quorum for etcd members is not being met.\n{{?}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "{{?pydata.replica_count > 3}}\nRed Hat recommends to scale down the control plane to exactly 3 nodes.\n{{??pydata.replica_count < 3}}\nRed Hat recommends to scale up the control plane to exactly 3 nodes.\n{{?}}\n\nIf feasible, cluster re-installation is also an option to meet the supportability criteria.",
      "more_info": "Please review the [Official Documentation](https://docs.openshift.com/container-platform/{{=pydata.version}}/architecture/control-plane.html#defining-masters_control-plane) for more information.\n",
      "reason": "On the OpenShift cluster, there are {{=pydata.replica_count}} control plane nodes. Currently, the supported replica count for control plane nodes in the OpenShift cluster is 3.\n\n{{?pydata.replica_count > 3}}\nWith {{=pydata.replica_count}} members, the quorum has to pass traffic to all the members and wait for consensus from more members before committing a change. This introduces latency to etcd, which in turn increases latency for the API server and cluster operators impacting cluster's overall performance.\n{{??pydata.replica_count < 3}}\nWith {{=pydata.replica_count}} members, the quorum for etcd members is not being met.\n{{?}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4972291",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.bug_rules.bug_1821905"
      },
      "error_keys": {
        "BUGZILLA_BUG_1821905": {
          "metadata": {
            "description": "Cluster upgrade will fail when default SCC gets changed",
            "impact": 3,
            "likelihood": 3,
            "publish_date": "2020-04-17 16:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "service_availability"
            ]
          },
          "total_risk": 3,
          "generic": "Due to a known bug in OpenShift 4 operators cannot upgrade due to \"DefaultSecurityContextConstraints_Mutated\".\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4972291)\n",
          "summary": "",
          "resolution": "OpenShift Container Platform (OCP) 4.3.13 will no longer block upgrades if the SCC is not the default.\n\nThe original issue raised affected versions 4.3.8, 4.3.9, 4.3.10, 4.3.11, and 4.3.12.\n\n- I have already upgraded to one of the affected versions:\n  - You will need to use the `--force` flag to upgrade.\n- I must upgrade to one of the affected versions before I can upgrade to 4.3.13:\n- This is not recommended. However, if you must upgrade to an affected version, be aware that you will need to use the `--force` flag to perform your next upgrade.\n\n**Using the `--force` flag**:\n\n**IMPORTANT:** Any changes you have made to the default SCCs `anyuid`, `hostaccess`, `hostmount-anyuid`, `hostnetwork`, `nonroot`, `privileged`, or `restricted` may be removed later when you upgrade to 4.4 which could cause system instability. You should address this issue by migrating any changes you made to the mentioned default SCCs to new SCCs.\n\n- Use of the `--force` flag will skip all precondition tests. You must verify that there are no other preconditions which need to be considered.\n- Upgrading using `--force` **will not** remove the changes you have made to the default SCCs. You should create a plan to migrate the changes you made to the default SCCs to new SCCs before you upgrade to 4.4.\n\nThe `--force` flag can be added to your `oc adm upgrade` command. For example:\n~~~\n# oc adm upgrade --force --to 4.3.13\n~~~\n",
          "more_info": "For more in-depth information check the following:\n1. [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1821905).\n2. [KCS](https://access.redhat.com/solutions/4972291)",
          "reason": "The OCP-{{=pydata.desired}} update is blocked because default security context constraints (SCC) anyuid, hostaccess, hostmount-anyuid, hostnetwork, nonroot, privileged, or restricted have been modified\n\nUpgrading 4.3.8, 4.3.9, 4.3.10, 4.3.11, or 4.3.12 fails if security context constraints (SCC) are not the default.\n\nOCP 4.3.8 introduced a new check for modified or mutated default SCCs. If any of the SCCs anyuid, hostaccess, hostmount-anyuid, hostnetwork, nonroot, privileged, or restricted have been modified, upgrades to future releases are prevented. For more details see [BZ-1808602](https://bugzilla.redhat.com/show_bug.cgi?id=1808602) and [BZ-1810596](https://bugzilla.redhat.com/show_bug.cgi?id=1810596) from [Bug Fix Advisory RHBA-2020:0858](https://access.redhat.com/errata/RHBA-2020:0858).\n\nThis check is to ensure that environments with modified default SCCs could not be upgraded to 4.4 as changes or removal of the default SCCs could lead to unexpected behavior and system instability.\n\nOCP 4.3.13 ([Bug Fix Advisory RHBA-2020:1481](https://access.redhat.com/errata/RHBA-2020:1481)) relaxes this check and will no longer block the upgrade.\n\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "OpenShift Container Platform (OCP) 4.3.13 will no longer block upgrades if the SCC is not the default.\n\nThe original issue raised affected versions 4.3.8, 4.3.9, 4.3.10, 4.3.11, and 4.3.12.\n\n- I have already upgraded to one of the affected versions:\n  - You will need to use the `--force` flag to upgrade.\n- I must upgrade to one of the affected versions before I can upgrade to 4.3.13:\n- This is not recommended. However, if you must upgrade to an affected version, be aware that you will need to use the `--force` flag to perform your next upgrade.\n\n**Using the `--force` flag**:\n\n**IMPORTANT:** Any changes you have made to the default SCCs `anyuid`, `hostaccess`, `hostmount-anyuid`, `hostnetwork`, `nonroot`, `privileged`, or `restricted` may be removed later when you upgrade to 4.4 which could cause system instability. You should address this issue by migrating any changes you made to the mentioned default SCCs to new SCCs.\n\n- Use of the `--force` flag will skip all precondition tests. You must verify that there are no other preconditions which need to be considered.\n- Upgrading using `--force` **will not** remove the changes you have made to the default SCCs. You should create a plan to migrate the changes you made to the default SCCs to new SCCs before you upgrade to 4.4.\n\nThe `--force` flag can be added to your `oc adm upgrade` command. For example:\n~~~\n# oc adm upgrade --force --to 4.3.13\n~~~\n",
      "more_info": "For more in-depth information check the following:\n1. [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1821905).\n2. [KCS](https://access.redhat.com/solutions/4972291)",
      "reason": "The OCP-{{=pydata.desired}} update is blocked because default security context constraints (SCC) anyuid, hostaccess, hostmount-anyuid, hostnetwork, nonroot, privileged, or restricted have been modified\n\nUpgrading 4.3.8, 4.3.9, 4.3.10, 4.3.11, or 4.3.12 fails if security context constraints (SCC) are not the default.\n\nOCP 4.3.8 introduced a new check for modified or mutated default SCCs. If any of the SCCs anyuid, hostaccess, hostmount-anyuid, hostnetwork, nonroot, privileged, or restricted have been modified, upgrades to future releases are prevented. For more details see [BZ-1808602](https://bugzilla.redhat.com/show_bug.cgi?id=1808602) and [BZ-1810596](https://bugzilla.redhat.com/show_bug.cgi?id=1810596) from [Bug Fix Advisory RHBA-2020:0858](https://access.redhat.com/errata/RHBA-2020:0858).\n\nThis check is to ensure that environments with modified default SCCs could not be upgraded to 4.4 as changes or removal of the default SCCs could lead to unexpected behavior and system instability.\n\nOCP 4.3.13 ([Bug Fix Advisory RHBA-2020:1481](https://access.redhat.com/errata/RHBA-2020:1481)) relaxes this check and will no longer block the upgrade.\n\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.prometheus_operator_deployment_misses_part_of_labels"
      },
      "error_keys": {
        "BUGZILLA_BUG_1987914": {
          "metadata": {
            "description": "Missing part-of-label on prometheus-operator deployment",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Missing part-of-label on prometheus-operator deployment.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1987914](https://bugzilla.redhat.com/show_bug.cgi?id=1987914) for more information.",
          "reason": "OpenShift upgrade is failed due to the missing part-of-label on prometheus-operator deployment.<br>\nWithout those lables, OpenShift can't schedule Prometheus pods to run on an available node.<br>\n\nHere are the error messages:<br>\nPod Name    : {{=pydata.result[\"pod_name\"]}}\nMessage     :<br>\n{{=pydata.result[\"message\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1987914](https://bugzilla.redhat.com/show_bug.cgi?id=1987914) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5796601",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rhcos.rhcos_wrong_selinux_context"
      },
      "error_keys": {
        "RHCOS_WRONG_SELINUX_CONTEXT": {
          "metadata": {
            "description": "The cluster node is unable to upgrade when ovs-vswitchd.service failed to start in RHCOS",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The cluster node is unable to upgrade when ovs-vswitchd.service failed to start in RHCOS.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The ovs-vswitchd.service failed to start in this RHCOS node due to wrong SELinux contexts.<br>\nIt will cause the cluster nodes to be unable to upgrade.<br>\n\n{{for (var key in pydata.error_log) { }}\nFollowing error log occurred in `{{=key}}`:<br>\n{{~pydata.error_log[key]:error}}\n    {{=error}}{{~}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.image_registry_storage"
      },
      "error_keys": {
        "IMAGE_REGISTRY_STORAGE_EMPTYDIR": {
          "metadata": {
            "description": "The image data will be lost after the pod restart when image registry storage is temporary storage",
            "impact": 1,
            "likelihood": 2,
            "publish_date": "2021-02-07 15:35:00",
            "status": "active",
            "tags": [
              "sbr_storage",
              "openshift",
              "configuration",
              "service_availability"
            ]
          },
          "total_risk": 1,
          "generic": "The image data will be lost after the pod restart when image registry storage is temporary storage.\n",
          "summary": "",
          "resolution": "{{? pydata.infra == \"aws\" }}\n\nRed Hat recommends that you use Amazon S3 storage for the Image Registry Operator.\n\nPlease refer to [Configuring the registry for AWS](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?? pydata.infra == \"gcp\" }}\n\nRed Hat recommends that you use Google Cloud Storage for the Image Registry Operator.\n\nPlease refer to [Configuring the registry for GCP](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-gcp-user-infrastructure.html) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?? pydata.infra == \"azure\" }}\n\nRed Hat recommends that you use Azure Blob Storage for the Image Registry Operator.\n\nPlease refer to [Configuring the registry for Azure](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-azure-user-infrastructure.html) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?? pydata.infra == \"vsphere\" }}\n\nRed Hat recommends that you change the registry storage backend to a non-NFS permanent volume, e.g. a block device.\n\nPlease refer to [Configuring the registry for vSphere](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-vsphere.html#installation-registry-storage-config_configuring-registry-storage-vsphere) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?}}\n",
          "more_info": "",
          "reason": "{{? pydata.storage == \"emptydir\"}}\nThe image registry storage backend is an empty directory, which is temporary storage. Image data will be lost whenever the image registry is restarted.\n{{?}}\n{{? pydata.storage == \"not_s3\"}}\nThe image registry performs best with the Amazon S3 Storage when running on Amazon Web Services.\n{{?}}\n{{? pydata.storage == \"not_gcs\"}}\nThe image registry performs best with the Google Cloud Storage when running on Google Cloud Platform.\n{{?}}\n{{? pydata.storage == \"not_azure\"}}\nThe image registry performs best with the Azure Blob Storage when running on Microsoft Azure Cloud Computing Services.\n{{?}}\n{{? pydata.storage == \"NFS\"}}\nThe image registry storage backend is an NFS volume. Red Hat testing shows issues with using the NFS server on RHEL as a storage backend for core services. Using RHEL NFS to back permanent volumes used by core OpenShift services is not recommended. Other NFS implementations might not have these issues.\n{{?}}\n",
          "HasReason": true
        },
        "IMAGE_REGISTRY_STORAGE_NFS": {
          "metadata": {
            "description": "There are issues observed during testing by Red Hat when the image registry storage is NFS",
            "impact": 1,
            "likelihood": 2,
            "publish_date": "2021-02-07 15:35:00",
            "status": "active",
            "tags": [
              "sbr_storage",
              "openshift",
              "configuration",
              "service_availability"
            ]
          },
          "total_risk": 1,
          "generic": "The image registry storage backend is an NFS volume, which is not recommended due to issues observed during testing by Red Hat.\n",
          "summary": "",
          "resolution": "{{? pydata.infra == \"aws\" }}\n\nRed Hat recommends that you use Amazon S3 storage for the Image Registry Operator.\n\nPlease refer to [Configuring the registry for AWS](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?? pydata.infra == \"gcp\" }}\n\nRed Hat recommends that you use Google Cloud Storage for the Image Registry Operator.\n\nPlease refer to [Configuring the registry for GCP](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-gcp-user-infrastructure.html) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?? pydata.infra == \"azure\" }}\n\nRed Hat recommends that you use Azure Blob Storage for the Image Registry Operator.\n\nPlease refer to [Configuring the registry for Azure](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-azure-user-infrastructure.html) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?? pydata.infra == \"vsphere\" }}\n\nRed Hat recommends that you change the registry storage backend to a non-NFS permanent volume, e.g. a block device.\n\nPlease refer to [Configuring the registry for vSphere](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-vsphere.html#installation-registry-storage-config_configuring-registry-storage-vsphere) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?}}\n",
          "more_info": "",
          "reason": "{{? pydata.storage == \"emptydir\"}}\nThe image registry storage backend is an empty directory, which is temporary storage. Image data will be lost whenever the image registry is restarted.\n{{?}}\n{{? pydata.storage == \"not_s3\"}}\nThe image registry performs best with the Amazon S3 Storage when running on Amazon Web Services.\n{{?}}\n{{? pydata.storage == \"not_gcs\"}}\nThe image registry performs best with the Google Cloud Storage when running on Google Cloud Platform.\n{{?}}\n{{? pydata.storage == \"not_azure\"}}\nThe image registry performs best with the Azure Blob Storage when running on Microsoft Azure Cloud Computing Services.\n{{?}}\n{{? pydata.storage == \"NFS\"}}\nThe image registry storage backend is an NFS volume. Red Hat testing shows issues with using the NFS server on RHEL as a storage backend for core services. Using RHEL NFS to back permanent volumes used by core OpenShift services is not recommended. Other NFS implementations might not have these issues.\n{{?}}\n",
          "HasReason": true
        },
        "IMAGE_REGISTRY_STORAGE_RECOMMENDED": {
          "metadata": {
            "description": "The image registry performs best with the storage from the public cloud platform where it is running",
            "impact": 1,
            "likelihood": 1,
            "publish_date": "2021-02-07 15:35:00",
            "status": "active",
            "tags": [
              "sbr_storage",
              "openshift",
              "configuration",
              "service_availability"
            ]
          },
          "total_risk": 1,
          "generic": "The image registry performs best with the storage from the public cloud platform where it is running.\n",
          "summary": "",
          "resolution": "{{? pydata.infra == \"aws\" }}\n\nRed Hat recommends that you use Amazon S3 storage for the Image Registry Operator.\n\nPlease refer to [Configuring the registry for AWS](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?? pydata.infra == \"gcp\" }}\n\nRed Hat recommends that you use Google Cloud Storage for the Image Registry Operator.\n\nPlease refer to [Configuring the registry for GCP](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-gcp-user-infrastructure.html) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?? pydata.infra == \"azure\" }}\n\nRed Hat recommends that you use Azure Blob Storage for the Image Registry Operator.\n\nPlease refer to [Configuring the registry for Azure](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-azure-user-infrastructure.html) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?? pydata.infra == \"vsphere\" }}\n\nRed Hat recommends that you change the registry storage backend to a non-NFS permanent volume, e.g. a block device.\n\nPlease refer to [Configuring the registry for vSphere](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-vsphere.html#installation-registry-storage-config_configuring-registry-storage-vsphere) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?}}\n",
          "more_info": "",
          "reason": "{{? pydata.storage == \"emptydir\"}}\nThe image registry storage backend is an empty directory, which is temporary storage. Image data will be lost whenever the image registry is restarted.\n{{?}}\n{{? pydata.storage == \"not_s3\"}}\nThe image registry performs best with the Amazon S3 Storage when running on Amazon Web Services.\n{{?}}\n{{? pydata.storage == \"not_gcs\"}}\nThe image registry performs best with the Google Cloud Storage when running on Google Cloud Platform.\n{{?}}\n{{? pydata.storage == \"not_azure\"}}\nThe image registry performs best with the Azure Blob Storage when running on Microsoft Azure Cloud Computing Services.\n{{?}}\n{{? pydata.storage == \"NFS\"}}\nThe image registry storage backend is an NFS volume. Red Hat testing shows issues with using the NFS server on RHEL as a storage backend for core services. Using RHEL NFS to back permanent volumes used by core OpenShift services is not recommended. Other NFS implementations might not have these issues.\n{{?}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "{{? pydata.infra == \"aws\" }}\n\nRed Hat recommends that you use Amazon S3 storage for the Image Registry Operator.\n\nPlease refer to [Configuring the registry for AWS](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-aws-user-infrastructure.html) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?? pydata.infra == \"gcp\" }}\n\nRed Hat recommends that you use Google Cloud Storage for the Image Registry Operator.\n\nPlease refer to [Configuring the registry for GCP](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-gcp-user-infrastructure.html) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?? pydata.infra == \"azure\" }}\n\nRed Hat recommends that you use Azure Blob Storage for the Image Registry Operator.\n\nPlease refer to [Configuring the registry for Azure](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-azure-user-infrastructure.html) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?? pydata.infra == \"vsphere\" }}\n\nRed Hat recommends that you change the registry storage backend to a non-NFS permanent volume, e.g. a block device.\n\nPlease refer to [Configuring the registry for vSphere](https://docs.openshift.com/container-platform/{{=pydata.ocp}}/registry/configuring_registry_storage/configuring-registry-storage-vsphere.html#installation-registry-storage-config_configuring-registry-storage-vsphere) in the OpenShift Container Platform documentation for specific instructions.\n\n{{?}}\n",
      "more_info": "",
      "reason": "{{? pydata.storage == \"emptydir\"}}\nThe image registry storage backend is an empty directory, which is temporary storage. Image data will be lost whenever the image registry is restarted.\n{{?}}\n{{? pydata.storage == \"not_s3\"}}\nThe image registry performs best with the Amazon S3 Storage when running on Amazon Web Services.\n{{?}}\n{{? pydata.storage == \"not_gcs\"}}\nThe image registry performs best with the Google Cloud Storage when running on Google Cloud Platform.\n{{?}}\n{{? pydata.storage == \"not_azure\"}}\nThe image registry performs best with the Azure Blob Storage when running on Microsoft Azure Cloud Computing Services.\n{{?}}\n{{? pydata.storage == \"NFS\"}}\nThe image registry storage backend is an NFS volume. Red Hat testing shows issues with using the NFS server on RHEL as a storage backend for core services. Using RHEL NFS to back permanent volumes used by core OpenShift services is not recommended. Other NFS implementations might not have these issues.\n{{?}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5371801",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.openshift_samples_has_not_yet_rolled_out"
      },
      "error_keys": {
        "OPENSHIFT_SAMPLES_OPERATOR_HAS_NOT_YET_SUCCESSFULLY_ROLLED_OUT": {
          "metadata": {
            "description": "This rule checks if  cluster operator openshift-samples has not yet successfully rolled outcausing problem in Upgrade",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule checks if  cluster operator openshift-samples has not yet successfully rolled outcausing problem in Upgrade.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Following Message have been found for object configs.samples.operator.openshift.io:<br>\n\n{{=pydata.msg}}\n\nError from sample operator pod logs ><br>\n{{for (var temp in pydata.logs) { }}\nPod name: {{=pydata.logs[temp][\"pod_name\"]}}\n\nPod log: {{=pydata.logs[temp][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.openshift_route_annotations"
      },
      "error_keys": {
        "ERROR_OPENSHIFT_IP_WHITELIST_ANNOTATIONS": {
          "metadata": {
            "description": "ip_whitelist for routes is not supported",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "ip_whitelist for routes is not supported.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1908125](https://bugzilla.redhat.com/show_bug.cgi?id=1908125) for more information.\n\nRefer to the [Product Documentation](https://docs.openshift.com/container-platform/4.5/networking/routes/route-configuration.html#nw-route-specific-annotations_route-configuration) for more information.",
          "reason": "The annotation ip_whitelist for routes is not supported in {{=pydata.ocp_version}}\n\n{{for (var route in pydata.annotations) { }}\n{{=route}}, has annotations: {{=pydata.annotations[route]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1908125](https://bugzilla.redhat.com/show_bug.cgi?id=1908125) for more information.\n\nRefer to the [Product Documentation](https://docs.openshift.com/container-platform/4.5/networking/routes/route-configuration.html#nw-route-specific-annotations_route-configuration) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5367681",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.invalid_image_reference"
      },
      "error_keys": {
        "IMAGE_PRUNER_FAILS_INVALID_FORMAT": {
          "metadata": {
            "description": "Image-Pruner is failing due to image reference format error",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Due to an invalid image reference the image-pruner pod reports 'Failed' state.\nThis further leads to degraded state of image-registry operator.\n",
          "summary": "",
          "resolution": "",
          "more_info": "* See [KCS 5367681](https://access.redhat.com/solutions/5367681) for more information.\n",
          "reason": "Due to an invalid image reference the image-pruner pod reports 'Failed' state.\nThis further leads to degraded state of image-registry operator.\n\n**Operator: {{=pydata.result[\"name\"]}}**\n* **Available Status:** {{=pydata.result[\"available\"][\"status\"]}}\n* **Degraded Status:** {{=pydata.result[\"degraded\"][\"status\"]}}\n* **Degraded Reason:** {{=pydata.result[\"degraded\"][\"reason\"]}}\n* **Degraded Message:** {{=pydata.result[\"degraded\"][\"message\"]}}\n\n**Affected Pruner Pods:**\n\n{{~ pydata.pod_details:pod }}\n- {{=pod[\"pod_name\"]}}\n{{~}}\n\n**Sample pod log message:**\n\n```\n{{=pydata.pod_details[pydata.pod_details.length-1][\"message\"]}}\n```\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "* See [KCS 5367681](https://access.redhat.com/solutions/5367681) for more information.\n",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5663021",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.cmo_pod_rollout_stuck"
      },
      "error_keys": {
        "CLUSTER_MONITORING_OPERATOR_POD_ROLLOUT_STUCK": {
          "metadata": {
            "description": "Cluster Monitoring Operator pod rollout is stuck because of being wrongly assigned non root SCC",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Cluster Monitoring Operator pod rollout is stuck because of being wrongly assigned non root SCC.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1904538](https://bugzilla.redhat.com/show_bug.cgi?id=1904538) for more information.",
          "reason": "Cluster-monitoring-operator pod rollout stuck due to CreateContainerConfigError.<br>\n\nFollowing error message found:<br>\n\nPod_Name : {{=pydata.error}}\nMessage  : container has runAsNonRoot and image has non-numeric user (nobody), cannot verify user is non-root",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1904538](https://bugzilla.redhat.com/show_bug.cgi?id=1904538) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.openshift_sdn_egress_ip_in_multiple_hostsubnets"
      },
      "error_keys": {
        "OPENSHIFT_SDN_EGRESS_IP_IN_MULTIPLE_HOSTSUBNETS_UNUSED": {
          "metadata": {
            "description": "The cluster drops outgoing traffic from a project when its egress IP address is assigned to multiple nodes",
            "impact": 2,
            "likelihood": 2,
            "publish_date": "2021-02-09 14:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "configuration",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "The cluster drops outgoing traffic from a project when its egress IP address is assigned to multiple nodes.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you keep only one HostSubnet for {{?pydata.multiple_hostsubnets.length>1}}these egress IP addresses{{??}}this egress IP address{{?}}:\n\n{{~ pydata.multiple_hostsubnets :entry }}\n{{? entry['netnamespace'] != null }}\n- {{=entry['egress_ip']}}\n  - HostSubnets: {{= entry['hostsubnets'].join(', ') }}\n  - This egress IP is used by the '{{=entry['netnamespace']}}' project\n{{?}}\n{{~}}\n{{~ pydata.multiple_hostsubnets :entry }}\n{{? entry['netnamespace'] == null }}\n- {{=entry['egress_ip']}}\n  - HostSubnets: {{= entry['hostsubnets'].join(', ') }}\n  - No project is using this egress IP at the moment\n{{?}}\n{{~}}\n\nSee [Configuring egress IPs for a project](https://docs.openshift.com/container-platform/{{=pydata.ocp_version}}/networking/openshift_sdn/assigning-egress-ips.html) for more information about configuring egress IP addresses.\n",
          "more_info": "",
          "reason": "{{?pydata.multiple_hostsubnets.length>1}}These egress IP addresses have{{??}}This egress IP address has{{?}} been found in multiple HostSubnets:\n\n{{~ pydata.multiple_hostsubnets :entry }}\n{{? entry['netnamespace'] != null }}\n- {{=entry['egress_ip']}}\n  - HostSubnets: {{= entry['hostsubnets'].join(', ') }}\n  - This egress IP address is used by the '{{=entry['netnamespace']}}' project\n{{?}}\n{{~}}\n{{~ pydata.multiple_hostsubnets :entry }}\n{{? entry['netnamespace'] == null }}\n- {{=entry['egress_ip']}}\n  - HostSubnets: {{= entry['hostsubnets'].join(', ') }}\n  - No project is using this egress IP address at the moment\n{{?}}\n{{~}}\n",
          "HasReason": true
        },
        "OPENSHIFT_SDN_EGRESS_IP_IN_MULTIPLE_HOSTSUBNETS_USED": {
          "metadata": {
            "description": "The cluster drops outgoing traffic from a project when its egress IP is assigned to multiple nodes",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-02-09 14:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "configuration",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "The cluster drops outgoing traffic from a project when its egress IP address is assigned to multiple nodes.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you keep only one HostSubnet for {{?pydata.multiple_hostsubnets.length>1}}these egress IP addresses{{??}}this egress IP address{{?}}:\n\n{{~ pydata.multiple_hostsubnets :entry }}\n{{? entry['netnamespace'] != null }}\n- {{=entry['egress_ip']}}\n  - HostSubnets: {{= entry['hostsubnets'].join(', ') }}\n  - This egress IP is used by the '{{=entry['netnamespace']}}' project\n{{?}}\n{{~}}\n{{~ pydata.multiple_hostsubnets :entry }}\n{{? entry['netnamespace'] == null }}\n- {{=entry['egress_ip']}}\n  - HostSubnets: {{= entry['hostsubnets'].join(', ') }}\n  - No project is using this egress IP at the moment\n{{?}}\n{{~}}\n\nSee [Configuring egress IPs for a project](https://docs.openshift.com/container-platform/{{=pydata.ocp_version}}/networking/openshift_sdn/assigning-egress-ips.html) for more information about configuring egress IP addresses.\n",
          "more_info": "",
          "reason": "{{?pydata.multiple_hostsubnets.length>1}}These egress IP addresses have{{??}}This egress IP address has{{?}} been found in multiple HostSubnets:\n\n{{~ pydata.multiple_hostsubnets :entry }}\n{{? entry['netnamespace'] != null }}\n- {{=entry['egress_ip']}}\n  - HostSubnets: {{= entry['hostsubnets'].join(', ') }}\n  - This egress IP address is used by the '{{=entry['netnamespace']}}' project\n{{?}}\n{{~}}\n{{~ pydata.multiple_hostsubnets :entry }}\n{{? entry['netnamespace'] == null }}\n- {{=entry['egress_ip']}}\n  - HostSubnets: {{= entry['hostsubnets'].join(', ') }}\n  - No project is using this egress IP address at the moment\n{{?}}\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you keep only one HostSubnet for {{?pydata.multiple_hostsubnets.length>1}}these egress IP addresses{{??}}this egress IP address{{?}}:\n\n{{~ pydata.multiple_hostsubnets :entry }}\n{{? entry['netnamespace'] != null }}\n- {{=entry['egress_ip']}}\n  - HostSubnets: {{= entry['hostsubnets'].join(', ') }}\n  - This egress IP is used by the '{{=entry['netnamespace']}}' project\n{{?}}\n{{~}}\n{{~ pydata.multiple_hostsubnets :entry }}\n{{? entry['netnamespace'] == null }}\n- {{=entry['egress_ip']}}\n  - HostSubnets: {{= entry['hostsubnets'].join(', ') }}\n  - No project is using this egress IP at the moment\n{{?}}\n{{~}}\n\nSee [Configuring egress IPs for a project](https://docs.openshift.com/container-platform/{{=pydata.ocp_version}}/networking/openshift_sdn/assigning-egress-ips.html) for more information about configuring egress IP addresses.\n",
      "more_info": "",
      "reason": "{{?pydata.multiple_hostsubnets.length>1}}These egress IP addresses have{{??}}This egress IP address has{{?}} been found in multiple HostSubnets:\n\n{{~ pydata.multiple_hostsubnets :entry }}\n{{? entry['netnamespace'] != null }}\n- {{=entry['egress_ip']}}\n  - HostSubnets: {{= entry['hostsubnets'].join(', ') }}\n  - This egress IP address is used by the '{{=entry['netnamespace']}}' project\n{{?}}\n{{~}}\n{{~ pydata.multiple_hostsubnets :entry }}\n{{? entry['netnamespace'] == null }}\n- {{=entry['egress_ip']}}\n  - HostSubnets: {{= entry['hostsubnets'].join(', ') }}\n  - No project is using this egress IP address at the moment\n{{?}}\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6329921",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.api_removed_in_next_release_in_use"
      },
      "error_keys": {
        "ADMIN_ACK": {
          "metadata": {
            "description": "Upgrades are blocked because a manual acknowledgment for to-be-removed APIs has not been provided",
            "impact": 2,
            "likelihood": 4,
            "publish_date": "2021-09-24 16:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift"
            ]
          },
          "total_risk": 3,
          "generic": "4.8.14 introduced a requirement that an administrator provide a manual acknowledgment before\nthe cluster can be upgraded from OpenShift Container Platform 4.8 to 4.9.\nThis is to help prevent issues after upgrading to OpenShift Container Platform 4.9,\nwhere APIs that have been removed are still in use by workloads, tools, or other components\nrunning on or interacting with the cluster. Administrators must evaluate their cluster for any\nAPIs in use that will be removed and migrate the affected components to use the appropriate new API version.\nAfter this is done, the administrator can provide the administrator acknowledgment.",
          "summary": "",
          "resolution": "{{?pydata.api_removed}}\nRed Hat recommends that you evaluate your cluster for removed APIs and migrate instances of removed APIs. Please refer to the [OpenShift Documentation](https://access.redhat.com/articles/6329921) for the steps.\n{{??}}\nIf you have completed [the evaluation of removed APIs](https://access.redhat.com/articles/6329921) and your cluster is ready to upgrade to OpenShift Container Platform 4.9, please run the following command to provide the administrator acknowledgment.\n~~~\n$ oc -n openshift-config patch cm admin-acks --patch '{\"data\":{\"ack-4.8-kube-1.22-api-removals-in-4.9\":\"true\"}}' --type=merge\n~~~\n{{?}}",
          "more_info": "",
          "reason": "The cluster is running {{=pydata.version}}, upgrades from OpenShift Container Platform 4.8 to 4.9 are blocked because the administrator has not provided a manual acknowledgment.",
          "HasReason": true
        },
        "API_REMOVED_IN_NEXT_RELEASE_IN_USE": {
          "metadata": {
            "description": "Workloads are still using the depracated APIs which will be removed in the next release",
            "impact": 2,
            "likelihood": 4,
            "publish_date": "2021-09-24 16:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift",
              "incident"
            ]
          },
          "total_risk": 3,
          "generic": "The OpenShift Container Platform 4.9 is expected to use Kubernetes 1.22. Kubernetes 1.22 removed a significant number of deprecated v1beta1 APIs. If your cluster, or any idle workloads or tools use any of these APIs, you must migrate them to the appropriate new version before upgrading to OpenShift Container Platform 4.9.",
          "summary": "",
          "resolution": "{{?pydata.api_removed}}\nRed Hat recommends that you evaluate your cluster for removed APIs and migrate instances of removed APIs. Please refer to the [OpenShift Documentation](https://access.redhat.com/articles/6329921) for the steps.\n{{??}}\nIf you have completed [the evaluation of removed APIs](https://access.redhat.com/articles/6329921) and your cluster is ready to upgrade to OpenShift Container Platform 4.9, please run the following command to provide the administrator acknowledgment.\n~~~\n$ oc -n openshift-config patch cm admin-acks --patch '{\"data\":{\"ack-4.8-kube-1.22-api-removals-in-4.9\":\"true\"}}' --type=merge\n~~~\n{{?}}",
          "more_info": "",
          "reason": "Workloads/tools are still using the following deprecated APIs.\n\n{{?pydata.api_removed.APIRemovedInNextReleaseInUse}}\nAPIs in use that will be removed in the next release:\n{{~pydata.api_removed.APIRemovedInNextReleaseInUse: api}}\n- **{{=api}}**\n{{~}}\n{{?}}\n\n{{?pydata.api_removed.APIRemovedInNextEUSReleaseInUse}}\nAPIs in use that will be removed in the next EUS release:\n{{~pydata.api_removed.APIRemovedInNextEUSReleaseInUse: api}}\n- **{{=api}}**\n{{~}}\n{{?}}\n\nThe following prometheus alert{{?Object.keys(pydata.api_removed).length>=2}}s are{{??}} is{{?}} firing because of this issue.\n{{ for (var key in pydata.api_removed) { }}\n- **{{=key}}**\n{{ } }}\n\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "{{?pydata.api_removed}}\nRed Hat recommends that you evaluate your cluster for removed APIs and migrate instances of removed APIs. Please refer to the [OpenShift Documentation](https://access.redhat.com/articles/6329921) for the steps.\n{{??}}\nIf you have completed [the evaluation of removed APIs](https://access.redhat.com/articles/6329921) and your cluster is ready to upgrade to OpenShift Container Platform 4.9, please run the following command to provide the administrator acknowledgment.\n~~~\n$ oc -n openshift-config patch cm admin-acks --patch '{\"data\":{\"ack-4.8-kube-1.22-api-removals-in-4.9\":\"true\"}}' --type=merge\n~~~\n{{?}}",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6184401",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.okd_cluster_unsupported"
      },
      "error_keys": {
        "OKD_CLUSTER_UNSUPPORTED": {
          "metadata": {
            "description": "GSS does not provide enterprise-level support for an OKD cluster",
            "impact": 1,
            "likelihood": 4,
            "publish_date": "2021-07-22 12:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "sbr_shift",
              "openshift",
              "okd"
            ]
          },
          "total_risk": 2,
          "generic": "Red Hat Global Support Services does not provide enterprise-level support for an OKD cluster.",
          "summary": "",
          "resolution": "Red Hat Global Support Services may require you to demonstrate the problem on a Red Hat OpenShift Container platform or be unable \nto assist and reject your case in accordance with the [Production Support Scope of Coverage](https://access.redhat.com/support/offerings/production/soc).",
          "more_info": "",
          "reason": "This cluster appears to be running a version of OKD cluster.\n\n{{?pydata.okd_cluster.okd_version}}\n* Version: **{{=pydata.okd_cluster.okd_version}}**\n{{?}}\n{{?pydata.okd_cluster.okd_release_image}}\n* Release image: **{{=pydata.okd_cluster.okd_release_image}}**\n{{?}}\n\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat Global Support Services may require you to demonstrate the problem on a Red Hat OpenShift Container platform or be unable \nto assist and reject your case in accordance with the [Production Support Scope of Coverage](https://access.redhat.com/support/offerings/production/soc).",
      "more_info": "",
      "reason": "This cluster appears to be running a version of OKD cluster.\n\n{{?pydata.okd_cluster.okd_version}}\n* Version: **{{=pydata.okd_cluster.okd_version}}**\n{{?}}\n{{?pydata.okd_cluster.okd_release_image}}\n* Release image: **{{=pydata.okd_cluster.okd_release_image}}**\n{{?}}\n\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5891131",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.canary_check_repeat_fail"
      },
      "error_keys": {
        "CANARY_CHECK_REPEAT_FAIL": {
          "metadata": {
            "description": "Ingress cluster operator degraded when accessing canary route from Ingress operator pod failed",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Ingress cluster operator degraded when accessing canary route from Ingress operator pod failed.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Ingress cluster operator degraded when accessing canary route from Ingress operator pod failed.<br>\n\nDetailed logs:<br>\n{{for (var log in pydata.logs) { }}\n- **PodName**:  {{=pydata.logs[log][\"pod_name\"]}}<br>\n  **ErrorMessage**:  {{=pydata.logs[log][\"err_msg\"]}}{{}}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5514051",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.check_machine_config_ignition_version"
      },
      "error_keys": {
        "MACHINE_CONFIG_IGNITION_VERSION": {
          "metadata": {
            "description": "It's incompatible between Ignition spec v2 and Ignition spec v3",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "It's incompatible between Ignition spec v2 and Ignition spec v3.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The running cluster is {{=pydata.ocp_version}} and it uses the Ignition spec v3, which is not<br>\ncompatible with Ignition spec v2. That will lead to adding new nodes to the UPI cluster<br>\nfails after upgrading to OpenShift 4.6+",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.update_not_yet_available_check"
      },
      "error_keys": {
        "UPDATE_MISSING_ERROR": {
          "metadata": {
            "description": "Confusion because the clusterversion operator is in a failing state when the update is not available",
            "impact": 1,
            "likelihood": 3,
            "publish_date": "2020-03-13 13:42:00",
            "status": "active",
            "tags": null
          },
          "total_risk": 2,
          "generic": "The customer has attempted an upgrade that is not yet available on their current upgrade channel.\n",
          "summary": "",
          "resolution": "**Updating a cluster by using the CLI**\n\nIf updates are available, you can update your cluster by using the OpenShift CLI (oc).\nYou can find information about available OpenShift Container Platform advisories and updates in the [errata section](https://access.redhat.com/downloads/content/290/ver=4.3/rhel---8/4.3.0/x86_64/product-errata) of the Customer Portal.\n\n**Prerequisites**\n\n* Install the version of the OpenShift Command-line Interface (CLI), commonly known as `oc`, that matches the version for your updated version.\n* Log in to the cluster as user with `cluster-admin` privileges.\n* Install the `jq` package.\n\n**Procedure**\n\n1. Ensure that your cluster is available:\n  * `oc get clusterversion`\n2. Review the current update channel information and confirm that your channel is set to stable-:\n  * `oc get clusterversion -o json|jq \".items[0].spec\"`\n  * **IMPORTANT:** For production clusters, you must subscribe to a stable-* channel.\n3. View the available updates and note the version number of the update that you want to apply:\n  * `oc adm upgrade`\n  * To update to the latest version:\n    * `oc adm upgrade --to-latest=true 1`\n  * To update to a specific version:\n    * `oc adm upgrade --to=<version> 1`\n",
          "more_info": "For more information about upgrade channels, refer to [Upgrades phased roll out](https://access.redhat.com/articles/4495171)\n",
          "reason": "Version update {{=pydata.installed_version}} is not available on the upgrade channel {{=pydata.upgrade_channel}}.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "**Updating a cluster by using the CLI**\n\nIf updates are available, you can update your cluster by using the OpenShift CLI (oc).\nYou can find information about available OpenShift Container Platform advisories and updates in the [errata section](https://access.redhat.com/downloads/content/290/ver=4.3/rhel---8/4.3.0/x86_64/product-errata) of the Customer Portal.\n\n**Prerequisites**\n\n* Install the version of the OpenShift Command-line Interface (CLI), commonly known as `oc`, that matches the version for your updated version.\n* Log in to the cluster as user with `cluster-admin` privileges.\n* Install the `jq` package.\n\n**Procedure**\n\n1. Ensure that your cluster is available:\n  * `oc get clusterversion`\n2. Review the current update channel information and confirm that your channel is set to stable-:\n  * `oc get clusterversion -o json|jq \".items[0].spec\"`\n  * **IMPORTANT:** For production clusters, you must subscribe to a stable-* channel.\n3. View the available updates and note the version number of the update that you want to apply:\n  * `oc adm upgrade`\n  * To update to the latest version:\n    * `oc adm upgrade --to-latest=true 1`\n  * To update to a specific version:\n    * `oc adm upgrade --to=<version> 1`\n",
      "more_info": "For more information about upgrade channels, refer to [Upgrades phased roll out](https://access.redhat.com/articles/4495171)\n",
      "reason": "Version update {{=pydata.installed_version}} is not available on the upgrade channel {{=pydata.upgrade_channel}}.\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5069531",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.upgrade_fails_csi_operator_degraded"
      },
      "error_keys": {
        "CLUSTER_OPERATOR_CSI_DEGRADED_WITH_ERROR": {
          "metadata": {
            "description": "This rule is checking the csi-snapshot-controller operator statuswhich is in degraded state",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule is checking the csi-snapshot-controller operator statuswhich is in degraded state.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The cluster operator csi-snapshot-controller has not<br>\nyet successfully rolled out.<br>\n\nOperator name  : {{=pydata.operator[\"name\"]}}<br>\nDegraded status: {{=pydata.operator[\"degraded\"][\"status\"]}}<br>\nMessage        : {{=pydata.operator[\"degraded\"][\"message\"]}}<br>\nReason         : {{=pydata.operator[\"degraded\"][\"reason\"]}}<br>\nLast Transition: {{=pydata.operator[\"degraded\"][\"last_trans_time\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.nodes_pressure_check"
      },
      "error_keys": {
        "NODE_NO_PRESSURE": {
          "metadata": {
            "description": "No nodes are reporting memory pressure, CPU pressure or PID pressure",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "No nodes are reporting memory pressure, CPU pressure or PID pressure.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "{{? pydata.error_key }}\nThe following nodes are reporting pressure:\n{{~ pydata.pressured_nodes :node }}\n    {{=node[\"name\"]}}\n    {{~ node[\"issues\"] :pressure }}\n    Issue          : {{=pressure[0]}}\n    Reason         : {{=pressure[1][\"reason\"]}}\n    Message        : {{=pressure[1][\"message\"]}}\n    LastTransition : {{=pressure[1][\"last_trans_time\"]}}\n    {{~}}\n{{~}}\n{{??}}\nAll nodes have sufficient memory, CPU and PIDs. They do not report any pressure conditions.\n{{?}}\n",
          "HasReason": true
        },
        "NODE_PRESSURE": {
          "metadata": {
            "description": "One or more nodes are reporting memory, CPU or PID pressure",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "One or more nodes are reporting memory, CPU or PID pressure. Nodes under pressure can cause unexpected behavior. \n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "{{? pydata.error_key }}\nThe following nodes are reporting pressure:\n{{~ pydata.pressured_nodes :node }}\n    {{=node[\"name\"]}}\n    {{~ node[\"issues\"] :pressure }}\n    Issue          : {{=pressure[0]}}\n    Reason         : {{=pressure[1][\"reason\"]}}\n    Message        : {{=pressure[1][\"message\"]}}\n    LastTransition : {{=pressure[1][\"last_trans_time\"]}}\n    {{~}}\n{{~}}\n{{??}}\nAll nodes have sufficient memory, CPU and PIDs. They do not report any pressure conditions.\n{{?}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "{{? pydata.error_key }}\nThe following nodes are reporting pressure:\n{{~ pydata.pressured_nodes :node }}\n    {{=node[\"name\"]}}\n    {{~ node[\"issues\"] :pressure }}\n    Issue          : {{=pressure[0]}}\n    Reason         : {{=pressure[1][\"reason\"]}}\n    Message        : {{=pressure[1][\"message\"]}}\n    LastTransition : {{=pressure[1][\"last_trans_time\"]}}\n    {{~}}\n{{~}}\n{{??}}\nAll nodes have sufficient memory, CPU and PIDs. They do not report any pressure conditions.\n{{?}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.prometheus_backed_by_pvc"
      },
      "error_keys": {
        "PROMETHEUS_BACKED_BY_PVC": {
          "metadata": {
            "description": "The cluster could experience OOM kills during upgrade to OpenShift Container Platform 4.7 when running cluster monitoring with an attached PVC for Prometheus",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-03-26 16:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift"
            ]
          },
          "total_risk": 2,
          "generic": "Due to an OpenShift bug, the cluster could experience OOM kills during upgrade to OpenShift Container Platform 4.7 when running cluster monitoring with an attached PVC for Prometheus.",
          "summary": "",
          "resolution": "Red Hat recommends that you allow worker nodes with double the size of memory that was available **prior to upgrade to OpenShift 4.7** for the duration of the upgrade.\n",
          "more_info": "",
          "reason": "This OpenShift cluster is running in the version {{=pydata.version}} and cluster monitoring has an attached PVC for Prometheus. Prometheus memory usage doubles during cluster upgrade to OpenShift 4.7 and for several hours after upgrade is complete. It could lead to OOM kills during upgrade.\n\nFor more in-depth information please check the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1925061) and the OpenShift 4.7 [release notes](https://docs.openshift.com/container-platform/4.7/release_notes/ocp-4-7-release-notes.html#ocp-4-7-known-issues) .\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you allow worker nodes with double the size of memory that was available **prior to upgrade to OpenShift 4.7** for the duration of the upgrade.\n",
      "more_info": "",
      "reason": "This OpenShift cluster is running in the version {{=pydata.version}} and cluster monitoring has an attached PVC for Prometheus. Prometheus memory usage doubles during cluster upgrade to OpenShift 4.7 and for several hours after upgrade is complete. It could lead to OOM kills during upgrade.\n\nFor more in-depth information please check the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1925061) and the OpenShift 4.7 [release notes](https://docs.openshift.com/container-platform/4.7/release_notes/ocp-4-7-release-notes.html#ocp-4-7-known-issues) .\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5161361",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.etcd_operator_tries_connecting_to_defunct_bootstrap_etcd_member"
      },
      "error_keys": {
        "BUGZILLA_BUG_1832923": {
          "metadata": {
            "description": "Etcd operator tries to connect to defunct bootstrap nodesconsidering it as an etcd member and fails",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Etcd operator tries to connect to defunct bootstrap nodesconsidering it as an etcd member and fails.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1832923](https://bugzilla.redhat.com/show_bug.cgi?id=1832923) for more information.",
          "reason": "Possible known bug found.<br>\nSummary     : etcd-operator unhealthy and trying to connect to defunct bootstrap etcd member endpoint",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1832923](https://bugzilla.redhat.com/show_bug.cgi?id=1832923) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4976641",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.fail_to_debug_node_with_noexecute_effect"
      },
      "error_keys": {
        "ERROR_FAIL_TO_DEBUG_NODE_WITH_NOEXECUTE_EFFECT": {
          "metadata": {
            "description": "Taint a node with NoExecute effect causes oc debug node to fail in OCP 4",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Taint a node with NoExecute effect causes oc debug node to fail in OCP 4.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "It will fail to debug a node When taint a node with NoExecute effect in OCP 4.<br>\nThe workaround is to add toleration to a \"dummy\" namespace and lets the debug<br>\npod be created in this namespace with `--to-namespace` option.<br>\n\nThe following nodes are with the `NoExecute` effect:<br>\n{{for (var node in pydata.no_execute_nodes) { }}\n- {{=pydata.no_execute_nodes[node]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5043561",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.high_disc_usage"
      },
      "error_keys": {
        "NODE_FILE_SYSTEM_FILLING_OVIRT": {
          "metadata": {
            "description": "OpenShift nodes on Red Hat Virtualization fail complaining of high disk usage when the disk size in template is too small",
            "impact": 3,
            "likelihood": 4,
            "publish_date": "2020-03-26 12:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "service_availability"
            ]
          },
          "total_risk": 3,
          "generic": "OpenShift nodes on Red Hat Virtualization fail complaining of high disk usage due to small disk size in template.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/5043561)\n",
          "summary": "",
          "resolution": "You are hitting a [known bug](https://bugzilla.redhat.com/show_bug.cgi?id=1818577) and Red Hat recommends that you complete the following steps:\n\n\n1. [Uninstall the cluster](https://docs.openshift.com/container-platform/4.4/installing/installing_rhv/uninstalling-cluster-rhv.html).\n2. [Create a custom virtual machine template on RHV](https://access.redhat.com/sites/default/files/attachments/creating-a-custom-virtual-machine-template-for-rhv.pdf).\n3. Reinstall the cluster using the 120GB disk size recommended for production clusters using either the [Installing a cluster quickly on RHV](https://docs.openshift.com/container-platform/4.4/installing/installing_rhv/installing-rhv-default.html) or [Installing a cluster on RHV with customizations](https://docs.openshift.com/container-platform/4.4/installing/installing_rhv/installing-rhv-customizations.html) instructions.\n",
          "more_info": "",
          "reason": "Currently there are following OutOfSpace alerts in your OCP4 cluster running on RHV VM. This cluster will fail shortly due to disk size is not sufficient. This issue occurs due to disk size is too small in template.\n\n{{~pydata.alerts:alert}}\n**Alertname**: {{=alert[\"alertname\"]}}\n**Device**: {{=alert[\"device\"]}}\n**Namespace**: {{=alert[\"namespace\"]}}\n**Mountpoint**: {{=alert[\"mountpoint\"]}}\n**Service**: {{=alert[\"service\"]}}\n**Prometheus**: {{=alert[\"prometheus\"]}}\n**Pod**: {{=alert[\"pod\"]}}\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "You are hitting a [known bug](https://bugzilla.redhat.com/show_bug.cgi?id=1818577) and Red Hat recommends that you complete the following steps:\n\n\n1. [Uninstall the cluster](https://docs.openshift.com/container-platform/4.4/installing/installing_rhv/uninstalling-cluster-rhv.html).\n2. [Create a custom virtual machine template on RHV](https://access.redhat.com/sites/default/files/attachments/creating-a-custom-virtual-machine-template-for-rhv.pdf).\n3. Reinstall the cluster using the 120GB disk size recommended for production clusters using either the [Installing a cluster quickly on RHV](https://docs.openshift.com/container-platform/4.4/installing/installing_rhv/installing-rhv-default.html) or [Installing a cluster on RHV with customizations](https://docs.openshift.com/container-platform/4.4/installing/installing_rhv/installing-rhv-customizations.html) instructions.\n",
      "more_info": "",
      "reason": "Currently there are following OutOfSpace alerts in your OCP4 cluster running on RHV VM. This cluster will fail shortly due to disk size is not sufficient. This issue occurs due to disk size is too small in template.\n\n{{~pydata.alerts:alert}}\n**Alertname**: {{=alert[\"alertname\"]}}\n**Device**: {{=alert[\"device\"]}}\n**Namespace**: {{=alert[\"namespace\"]}}\n**Mountpoint**: {{=alert[\"mountpoint\"]}}\n**Service**: {{=alert[\"service\"]}}\n**Prometheus**: {{=alert[\"prometheus\"]}}\n**Pod**: {{=alert[\"pod\"]}}\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5174781",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.upgrade_stuck_monitoring_operator"
      },
      "error_keys": {
        "UPGRADE_STUCK_MONITORING_PVC": {
          "metadata": {
            "description": "Upgrade gets blocked at monitoring level when upgrading from 4.3.x to 4.4",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Upgrade gets blocked at monitoring level when upgrading from 4.3.x to 4.4.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "As the releases 4.4.0 through 4.4.8 have the bug which ignores the custom PVC_prefix,<br>\nUpgrade will get blocked at monitoring level when upgrading to 4.4<br>\n\n{{for (var version in pydata.target_versions) { }}\nTarget version: {{=pydata.target_versions[version]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6126571",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.cluster_version_operator_hung_during_shutdown"
      },
      "error_keys": {
        "ERROR_CLUSTER_VERSION_OPERATOR_HUNG_DURING_SHUTDOWN": {
          "metadata": {
            "description": "OpenShift Cluster Version Operator Hung During Shutdown",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "OpenShift Cluster Version Operator Hung During Shutdown.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bz](https://bugzilla.redhat.com/show_bug.cgi?id=1891143) for more information.",
          "reason": "CVO has a bug where a stuck metrics-serving goroutine can hang<br>\nthe CVO in shutdown, breaking install and upgrade.<br>\n{{for (var log in pydata.logs) { }}\nPod Names: {{=pydata.logs[log][\"pod_name\"]}}\nError Logs: {{=pydata.logs[log][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bz](https://bugzilla.redhat.com/show_bug.cgi?id=1891143) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6092191",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.pod_thanos_ruler_user_workload_failed"
      },
      "error_keys": {
        "ERROR_POD_THANOS_RULER_USER_WORKLOAD_FAILED": {
          "metadata": {
            "description": "Cluster upgrade failed due to issues with thanos-ruler-user-workload pods",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Cluster upgrade failed due to issues with thanos-ruler-user-workload pods.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Cluster upgrade failed due to issues in thanos-ruler-user-workload pods that<br>\nthey cannot unmarshal DNS message. The pods of thanos-ruler-user-workload are<br>\nin crashloopbackoff state due to the observed error message:\n{{for (var p in pydata.thanos_pods) { }}\nPod Name    : {{=pydata.thanos_pods[p][\"pod\"]}}<br>\nMessage     :<br>\n{{=pydata.thanos_pods[p][\"message\"]}}<br>\n{{}}}\n\nThe following error message is in Operator `monitoring`:<br>\n{{=pydata.monitoring_operator}}<br>\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5926951",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.vsphere_problem_detector_controller_syncerror"
      },
      "error_keys": {
        "VSPHERE_PROBLEM_DETECTOR_CONTROLLER_SYNCERROR": {
          "metadata": {
            "description": "Storage Cluster Operator was degraded with VSphereProblemDetectorController_SyncError when the vSphere Problem Detector Operator accesses the vCenter server with incorrect username or password",
            "impact": 1,
            "likelihood": 1,
            "publish_date": "2021-09-30 15:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "vsphere",
              "openshift",
              "incident",
              "configuration"
            ]
          },
          "total_risk": 1,
          "generic": "Storage Cluster Operator was degraded with VSphereProblemDetectorController_SyncError error when the vSphere Problem Detector Operator accesses the vCenter server with incorrect username or password.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you double-check if the communication from your OpenShift cluster to your vSphere environment is functional.\n\n1. Run the following command to get the credentials from OpenShift and confirm within vCenter that they are still valid. Use the credentials to log in to the vCenter with the vSphere user used during cluster installation.\n   ~~~\n   $ for data in $(oc get secret vsphere-creds -n kube-system -o json | jq -r '.data[]'); do echo $data | base64 -d; echo; done\n   ~~~\n\n1. Check if there has been any change in vCenter recently; if the user used for logging in to vSphere had a password expired flag set on it? In such cases, the password needs to be updated in the [vSphere vCenter](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vcenter.configuration.doc/GUID-523261AF-B36C-4C42-AD0C-8AD8D6AAEFE5.html) and in the [OpenShift Container Platform](https://access.redhat.com/solutions/4618011).\n\n1. Verify if the user account in vSphere has been disabled. Re-enabling will help with the operator's progress.\n\n1. If the credentials are correct and the user is able to properly login to the vCenter, then there is a possibility where secret data consists of encoded whitespaces. To avoid such scenario, confirm the length of username and password given does not contain any whitespaces. Compare the count of characters from secret and vSphere credentials.\n\n1. Be mindful of creating secrets with passwords and remember to encode them with the below command to avoid any whitespaces.\n   ~~~\n   # echo -n <password> | base64 -w0\n   ~~~\n",
          "more_info": "",
          "reason": "Storage Cluster Operator was degraded with `VSphereProblemDetectorController_SyncError` error due to the vSphere Problem Detector Operator accesses the vCenter server with incorrect username or password.\n\nThe vSphere Problem Detector Operator checks the vSphere cluster that is deployed on for common installation and misconfiguration issues that are related to storage. It uses the credentials from the Cloud Credential Operator to connect to vSphere cluster.\n\n**Error Message:** {{=pydata.message}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you double-check if the communication from your OpenShift cluster to your vSphere environment is functional.\n\n1. Run the following command to get the credentials from OpenShift and confirm within vCenter that they are still valid. Use the credentials to log in to the vCenter with the vSphere user used during cluster installation.\n   ~~~\n   $ for data in $(oc get secret vsphere-creds -n kube-system -o json | jq -r '.data[]'); do echo $data | base64 -d; echo; done\n   ~~~\n\n1. Check if there has been any change in vCenter recently; if the user used for logging in to vSphere had a password expired flag set on it? In such cases, the password needs to be updated in the [vSphere vCenter](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vcenter.configuration.doc/GUID-523261AF-B36C-4C42-AD0C-8AD8D6AAEFE5.html) and in the [OpenShift Container Platform](https://access.redhat.com/solutions/4618011).\n\n1. Verify if the user account in vSphere has been disabled. Re-enabling will help with the operator's progress.\n\n1. If the credentials are correct and the user is able to properly login to the vCenter, then there is a possibility where secret data consists of encoded whitespaces. To avoid such scenario, confirm the length of username and password given does not contain any whitespaces. Compare the count of characters from secret and vSphere credentials.\n\n1. Be mindful of creating secrets with passwords and remember to encode them with the below command to avoid any whitespaces.\n   ~~~\n   # echo -n <password> | base64 -w0\n   ~~~\n",
      "more_info": "",
      "reason": "Storage Cluster Operator was degraded with `VSphereProblemDetectorController_SyncError` error due to the vSphere Problem Detector Operator accesses the vCenter server with incorrect username or password.\n\nThe vSphere Problem Detector Operator checks the vSphere cluster that is deployed on for common installation and misconfiguration issues that are related to storage. It uses the credentials from the Cloud Credential Operator to connect to vSphere cluster.\n\n**Error Message:** {{=pydata.message}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5398941",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.monitoring_op_degraded_after_enabling_userworkloadmonitoring"
      },
      "error_keys": {
        "MONITORING_OPERATOR_DEGRADED_ERROR_UPDATINGUSERWORKLOAD": {
          "metadata": {
            "description": "Monitoring operator found to be in degraded state",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "\"/\" Monitoring operator found to be in degraded state \"/\".",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Monitoring operator degraded after enabling UserWorkLoadMonitoring in the cluster.<br>\n\nOperator name  : {{=pydata.operator[\"name\"]}}<br>\nDegraded status: {{=pydata.operator[\"degraded\"][\"status\"]}}<br>\nMessage        : {{=pydata.operator[\"degraded\"][\"message\"]}}<br>\nReason         : {{=pydata.operator[\"degraded\"][\"reason\"]}}<br>\nLast Transition: {{=pydata.operator[\"degraded\"][\"last_trans_time\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6304881",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.crio_pid_leak"
      },
      "error_keys": {
        "CRIO_PID_LEAK_ON_OCP_CLUSTER": {
          "metadata": {
            "description": "Nodes will become Not Ready due to a CRI-O PID leak in the running OpenShift Container Platform version",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-09-03 14:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "crio",
              "service_availability",
              "memory_leak"
            ]
          },
          "total_risk": 2,
          "generic": "OCP versions 4.6.43, 4.7.24-4.7.28 and 4.8.5-4.8.9 include a regression bug in the underlying RHCOS versions. The bug causes high memory utilization due to a CRI-O PID leak, which eventually leads to nodes becoming Not Ready.",
          "summary": "",
          "resolution": "{{?pydata.issue_nodes}}\nRed Hat recommends that you make sure all the affected nodes are upgraded.\n{{??}}\n{{?pydata.cluster_version.startsWith(\"4.6\")}}\nRed Hat recommends that you upgrade to 4.6.44 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.6/updating/updating-cluster-between-minor.html) for the steps.\n{{??pydata.cluster_version.startsWith(\"4.7\")}}\nRed Hat recommends that you upgrade to 4.7.29 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.7/updating/updating-cluster-between-minor.html) for the steps.\n{{??}}\nRed Hat recommends that you upgrade to 4.8.10 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.8/updating/updating-cluster-between-minor.html) for the steps.\n{{?}}\n{{?}}",
          "more_info": "Refer to the [Bugzilla 11997062](https://bugzilla.redhat.com/show_bug.cgi?id=1997062) for more information.",
          "reason": "{{?pydata.issue_nodes}}\nThe OpenShift is running on version **{{=pydata.cluster_version}}**. \nThe following OpenShift Nodes are running affected RHCOS versions.\n\n<table border=\"1\" align=\"left\">\n  <tr>\n    <th style=\"text-align:center;\">Node Name</th>\n    <th style=\"text-align:center;\">OS Version</th>\n  </tr>\n{{~pydata.issue_nodes:node}}\n<tr>\n    <td style=\"background-color:#FAFAFA;text-align:center;font-weight:normal;\">{{=node.node_name}} </td>\n    <td style=\"background-color:#FAFAFA;text-align:left;font-weight:normal;\">{{=node.os_version}} </td>\n</tr>\n{{~}}\n</table>\n\nDue to a regression bug on the versions, CRI-O leaks PID files under `/run/crio/exec-pid-dir` when running exec probes. \nThe leaked PID files will eventually fill up /run (a tmpfs volume), causing the Node to run out of memory and be marked Not Ready.\n{{??}}\nThe OpenShift cluster is running on version **{{=pydata.cluster_version}}**.\n\nDue to a regression bug in this version, CRI-O leaks PID files under\n`/run/crio/exec-pid-dir` when running exec probes.  The leaked PID files will\neventually fill up `/run` (a tmpfs volume), causing the Node to run out of memory\nand be marked Not Ready.\n{{?}}\n",
          "HasReason": true
        },
        "CRIO_PID_LEAK_ON_OCP_NODE": {
          "metadata": {
            "description": "Nodes will become Not Ready due to a CRI-O PID leak in the running RHCOS versions",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-09-03 14:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "crio",
              "service_availability",
              "memory_leak"
            ]
          },
          "total_risk": 2,
          "generic": "OCP versions 4.6.43, 4.7.24-4.7.28 and 4.8.5-4.8.9 include a regression bug in the underlying RHCOS versions. The bug causes high memory utilization due to a CRI-O PID leak, which eventually leads to nodes becoming Not Ready. While the cluster is not running an affected version, some nodes are still running affected RHCOS versions.\n",
          "summary": "",
          "resolution": "{{?pydata.issue_nodes}}\nRed Hat recommends that you make sure all the affected nodes are upgraded.\n{{??}}\n{{?pydata.cluster_version.startsWith(\"4.6\")}}\nRed Hat recommends that you upgrade to 4.6.44 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.6/updating/updating-cluster-between-minor.html) for the steps.\n{{??pydata.cluster_version.startsWith(\"4.7\")}}\nRed Hat recommends that you upgrade to 4.7.29 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.7/updating/updating-cluster-between-minor.html) for the steps.\n{{??}}\nRed Hat recommends that you upgrade to 4.8.10 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.8/updating/updating-cluster-between-minor.html) for the steps.\n{{?}}\n{{?}}",
          "more_info": "Refer to the [Bugzilla 11997062](https://bugzilla.redhat.com/show_bug.cgi?id=1997062) for more information.",
          "reason": "{{?pydata.issue_nodes}}\nThe OpenShift is running on version **{{=pydata.cluster_version}}**. \nThe following OpenShift Nodes are running affected RHCOS versions.\n\n<table border=\"1\" align=\"left\">\n  <tr>\n    <th style=\"text-align:center;\">Node Name</th>\n    <th style=\"text-align:center;\">OS Version</th>\n  </tr>\n{{~pydata.issue_nodes:node}}\n<tr>\n    <td style=\"background-color:#FAFAFA;text-align:center;font-weight:normal;\">{{=node.node_name}} </td>\n    <td style=\"background-color:#FAFAFA;text-align:left;font-weight:normal;\">{{=node.os_version}} </td>\n</tr>\n{{~}}\n</table>\n\nDue to a regression bug on the versions, CRI-O leaks PID files under `/run/crio/exec-pid-dir` when running exec probes. \nThe leaked PID files will eventually fill up /run (a tmpfs volume), causing the Node to run out of memory and be marked Not Ready.\n{{??}}\nThe OpenShift cluster is running on version **{{=pydata.cluster_version}}**.\n\nDue to a regression bug in this version, CRI-O leaks PID files under\n`/run/crio/exec-pid-dir` when running exec probes.  The leaked PID files will\neventually fill up `/run` (a tmpfs volume), causing the Node to run out of memory\nand be marked Not Ready.\n{{?}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "{{?pydata.issue_nodes}}\nRed Hat recommends that you make sure all the affected nodes are upgraded.\n{{??}}\n{{?pydata.cluster_version.startsWith(\"4.6\")}}\nRed Hat recommends that you upgrade to 4.6.44 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.6/updating/updating-cluster-between-minor.html) for the steps.\n{{??pydata.cluster_version.startsWith(\"4.7\")}}\nRed Hat recommends that you upgrade to 4.7.29 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.7/updating/updating-cluster-between-minor.html) for the steps.\n{{??}}\nRed Hat recommends that you upgrade to 4.8.10 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.8/updating/updating-cluster-between-minor.html) for the steps.\n{{?}}\n{{?}}",
      "more_info": "Refer to the [Bugzilla 11997062](https://bugzilla.redhat.com/show_bug.cgi?id=1997062) for more information.",
      "reason": "{{?pydata.issue_nodes}}\nThe OpenShift is running on version **{{=pydata.cluster_version}}**. \nThe following OpenShift Nodes are running affected RHCOS versions.\n\n<table border=\"1\" align=\"left\">\n  <tr>\n    <th style=\"text-align:center;\">Node Name</th>\n    <th style=\"text-align:center;\">OS Version</th>\n  </tr>\n{{~pydata.issue_nodes:node}}\n<tr>\n    <td style=\"background-color:#FAFAFA;text-align:center;font-weight:normal;\">{{=node.node_name}} </td>\n    <td style=\"background-color:#FAFAFA;text-align:left;font-weight:normal;\">{{=node.os_version}} </td>\n</tr>\n{{~}}\n</table>\n\nDue to a regression bug on the versions, CRI-O leaks PID files under `/run/crio/exec-pid-dir` when running exec probes. \nThe leaked PID files will eventually fill up /run (a tmpfs volume), causing the Node to run out of memory and be marked Not Ready.\n{{??}}\nThe OpenShift cluster is running on version **{{=pydata.cluster_version}}**.\n\nDue to a regression bug in this version, CRI-O leaks PID files under\n`/run/crio/exec-pid-dir` when running exec probes.  The leaked PID files will\neventually fill up `/run` (a tmpfs volume), causing the Node to run out of memory\nand be marked Not Ready.\n{{?}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "2137701",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.pods_crash_loop_check"
      },
      "error_keys": {
        "POD_CRASHLOOP_ISSUE": {
          "metadata": {
            "description": "Application creation failure when pods were found in a \"CrashLoopBackOff\" state",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2020-01-03 10:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "pod",
              "incident"
            ]
          },
          "total_risk": 2,
          "generic": "Some pods in the `openshift-*` namespaces are in the \"CrashLoopBackOff\" state - they repeatedly failed to start.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you to consider the following:\n\nThis can happen when:\n* The application running in your container panics and dies;\n* Liveness probe failed;\n* Readiness probe failed; (Increasing initialDelaySeconds: helps);\n* The process started inside the image isn't a long running process and finds no tty and the container just exits and gets restarted repeatedly, which is a crash loop as far as OpenShift is concerned;\n* The container image needs to run as a specific user, in which case you will need to relax security in your cluster so that images are not forced to run as a pre-allocated UID. See documentation on Managing Security Context Constraints (4.x version);\n* A base image is being used and there is no long running command that would keep this container alive. Example: intended for use in the s2i process, but being used as-is.\n* If unable to determine why the pod keeps crashing. Try the following:\n\nPossible solution:\n1. If using a Dockerfile, take a look at the ENTRYPOINT;\n1. Take a look for Command and Args attributes while describing the container.\n~~~\n# oc describe pod <pod-name>\n~~~\n1. Try running your image locally (using docker or podman) to rule out if OpenShift is the cause;\n1. Look at the previous pod logs of an instance of the container:\n~~~\n# oc logs <pod> -p\n~~~\n1. If no logs appear, debug the pod. This should create a copy of the pod and provide a shell:\n~~~\n# oc debug <pod>\n~~~",
          "more_info": "For more information about the error refer to:\n * [CrashLoopBackOff status for Openshift Pod](https://access.redhat.com/solutions/2137701)",
          "reason": "Pods in a CrashLoopBackOff state:\n{{~ pydata.pods :pod }}\n    {{=pod[\"pod\"]}}\n    - Restart count: {{=pod[\"restarts\"]}}\n    - Reason: {{=pod[\"reason\"]}}\n    - Message: {{=pod[\"msg\"]}}\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you to consider the following:\n\nThis can happen when:\n* The application running in your container panics and dies;\n* Liveness probe failed;\n* Readiness probe failed; (Increasing initialDelaySeconds: helps);\n* The process started inside the image isn't a long running process and finds no tty and the container just exits and gets restarted repeatedly, which is a crash loop as far as OpenShift is concerned;\n* The container image needs to run as a specific user, in which case you will need to relax security in your cluster so that images are not forced to run as a pre-allocated UID. See documentation on Managing Security Context Constraints (4.x version);\n* A base image is being used and there is no long running command that would keep this container alive. Example: intended for use in the s2i process, but being used as-is.\n* If unable to determine why the pod keeps crashing. Try the following:\n\nPossible solution:\n1. If using a Dockerfile, take a look at the ENTRYPOINT;\n1. Take a look for Command and Args attributes while describing the container.\n~~~\n# oc describe pod <pod-name>\n~~~\n1. Try running your image locally (using docker or podman) to rule out if OpenShift is the cause;\n1. Look at the previous pod logs of an instance of the container:\n~~~\n# oc logs <pod> -p\n~~~\n1. If no logs appear, debug the pod. This should create a copy of the pod and provide a shell:\n~~~\n# oc debug <pod>\n~~~",
      "more_info": "For more information about the error refer to:\n * [CrashLoopBackOff status for Openshift Pod](https://access.redhat.com/solutions/2137701)",
      "reason": "Pods in a CrashLoopBackOff state:\n{{~ pydata.pods :pod }}\n    {{=pod[\"pod\"]}}\n    - Restart count: {{=pod[\"restarts\"]}}\n    - Reason: {{=pod[\"reason\"]}}\n    - Message: {{=pod[\"msg\"]}}\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6002051",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.node_ip_changes_to_api_vip"
      },
      "error_keys": {
        "BUGZILLA_BUG_1948533": {
          "metadata": {
            "description": "On RHV platform, intermittently node IP is set to API_VIP",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "On RHV platform, intermittently node IP is set to API_VIP.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1948533](https://bugzilla.redhat.com/show_bug.cgi?id=1948533) for more information.",
          "reason": "Possible known Bug Found.<br>\nSummary     :  The master node IPs are switching to api_vip randomly, leading to unstable cluster state.<br>\n\nDetected, following pods to have hostIP populated as api_vip instead of expected master IP.<br>\n{{for (var pod in pydata.result) { }}\n* {{=pydata.result[pod]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1948533](https://bugzilla.redhat.com/show_bug.cgi?id=1948533) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5114881",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.image_registry_unable_to_sync_object_modified"
      },
      "error_keys": {
        "IMAGE_REGISTRY_DEGRADED_UNABLE_TO_SYNC": {
          "metadata": {
            "description": "After changing imageregistry cluster object, registry pod is not created.This rule checks \"unable to sync\" log, cluster configuration for stroage",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "After changing imageregistry cluster object, registry pod is not created.This rule checks \"unable to sync\" log, cluster configuration for stroage.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Following issues found with image-registry:<br>\n\n1. The status section of the image-registry cluster object does not show the storage defined.<br>\n\n2. Found 'unable to sync: Operation cannot be fulfilled on configs.imageregistry.operator.openshift.io \"cluster\"'<br>\n   log in below pods:<br>\n   ~~~\n   {{for (var pod_name in pydata.results[\"pods\"]) { }}{{=pydata.results[\"pods\"][pod_name]}}\n   {{}}}~~~\n\n   Log sample:<br>\n   {{=pydata.results[\"log_sample\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5696661",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.marketplace_operatorexited_while_upgrade"
      },
      "error_keys": {
        "BUGZILLA_BUG_1909217": {
          "metadata": {
            "description": "Upgrade stuck due to marketplace operator showing OperatorExited",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": " Upgrade stuck due to marketplace operator showing OperatorExited.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1909217](https://bugzilla.redhat.com/show_bug.cgi?id=1909217) for more information.",
          "reason": "Possible known Bug Found.<br>\nSummary     : Upgrade is stuck due to OpenShift Marketplace operator reporting 'OperatorExited' messages.<br>\n\nOperator Name               : {{=pydata.state[\"name\"]}}<br>\nOperator Available State    : {{=pydata.state[\"available\"][\"status\"]}}<br>\nOperator Progressing State  : {{=pydata.state[\"progressing\"][\"status\"]}}<br>\nOperator Degraded State     : {{=pydata.state[\"degraded\"][\"status\"]}}<br>\nMessage                     : {{=pydata.state[\"degraded\"][\"message\"]}}<br>\nReason                      : {{=pydata.state[\"degraded\"][\"reason\"]}}<br>\n<br>\n{{for (var pod_detail in pydata.logs) { }}\nPod Name: {{=pydata.logs[pod_detail][\"pod_name\"]}}\nMessage: {{=pydata.logs[pod_detail][\"message\"]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1909217](https://bugzilla.redhat.com/show_bug.cgi?id=1909217) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4828091",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.bug_1801300"
      },
      "error_keys": {
        "BUGZILLA_BUG_1801300": {
          "metadata": {
            "description": "The cluster shows symptoms of BZ 1801300: cluster autoscaler metrics collection breaks after upgrade to 4.3",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The cluster shows symptoms of BZ 1801300. Cluster autoscaler metrics collection breaks after upgrade to 4.3.\n",
          "summary": "",
          "resolution": "See [KCS 4828091](https://access.redhat.com/solutions/4828091).\n",
          "more_info": "* See [KCS 4828091](https://access.redhat.com/solutions/4828091) for more information about possible solutions.\n* See [BZ 1801300](https://bugzilla.redhat.com/show_bug.cgi?id=1801300) for more information about the bug.\n",
          "reason": "The cluster shows symptoms of BZ 1801300.\n\nThe cluster version is 4.2.x or 4.3.x and cluster-autoscaler-operator's deployment is using port 8080 for metrics.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "See [KCS 4828091](https://access.redhat.com/solutions/4828091).\n",
      "more_info": "* See [KCS 4828091](https://access.redhat.com/solutions/4828091) for more information about possible solutions.\n* See [BZ 1801300](https://bugzilla.redhat.com/show_bug.cgi?id=1801300) for more information about the bug.\n",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5436171",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.unsupported_cni_plugin"
      },
      "error_keys": {
        "CERTIFIED_CNI_PLUGIN": {
          "metadata": {
            "description": "The cluster is using a 3rd-party CNI plugin that has been certified by Red Hat",
            "impact": 3,
            "likelihood": 3,
            "publish_date": "2020-08-06 12:00:00",
            "status": "active",
            "tags": [
              "sbr_shift",
              "openshift",
              "sdn",
              "fault_tolerance"
            ]
          },
          "total_risk": 3,
          "generic": "The cluster is using a 3rd-party CNI plugin that has been certified by Red Hat. This configuration is supported by Red Hat.\n",
          "summary": "",
          "resolution": "Red Hat recommends using Red Hat CNI plugins or Red Hat certified 3rd party CNI plugins:\n\n - [*OpenShiftSDN*](https://docs.openshift.com/container-platform/{{=pydata.current_version}}/networking/cluster-network-operator.html)\n - [*OVNKubernetes*](https://docs.openshift.com/container-platform/{{=pydata.ovn_docs_version}}/networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.html)\n - [*Kuryr*](https://docs.openshift.com/container-platform/{{=pydata.current_version}}/installing/installing_openstack/installing-openstack-installer-kuryr.html)\n - Certified 3rd party CNI plugins\n   - [Red Hat Ecosystem Catalog](https://catalog.redhat.com/software/containers/search?q=CNI)\n   - [Knowledge Base Article](https://access.redhat.com/articles/5436171)\n",
          "more_info": "",
          "reason": "The cluster is using a 3rd-party CNI plugin that has been certified by Red Hat. \n\n* **OCP version:** {{=pydata.current_version}}\n* **Platform**: {{=pydata.platform}}\n* **CNI plugin:** {{=pydata.network_type}}\n\n",
          "HasReason": true
        },
        "SUPPORTED_CNI_PLUGIN": {
          "metadata": {
            "description": "The cluster is using a CNI plugin that is supported by Red Hat",
            "impact": 3,
            "likelihood": 3,
            "publish_date": "2020-08-06 12:00:00",
            "status": "active",
            "tags": [
              "sbr_shift",
              "openshift",
              "sdn",
              "fault_tolerance",
              "pssa"
            ]
          },
          "total_risk": 3,
          "generic": "The cluster is using one of the default CNI plugins (OpenShiftSDN, OVNKubernetes, Kuryr). This configuration is supported by Red Hat.\n",
          "summary": "",
          "resolution": "Red Hat recommends using Red Hat CNI plugins or Red Hat certified 3rd party CNI plugins:\n\n - [*OpenShiftSDN*](https://docs.openshift.com/container-platform/{{=pydata.current_version}}/networking/cluster-network-operator.html)\n - [*OVNKubernetes*](https://docs.openshift.com/container-platform/{{=pydata.ovn_docs_version}}/networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.html)\n - [*Kuryr*](https://docs.openshift.com/container-platform/{{=pydata.current_version}}/installing/installing_openstack/installing-openstack-installer-kuryr.html)\n - Certified 3rd party CNI plugins\n   - [Red Hat Ecosystem Catalog](https://catalog.redhat.com/software/containers/search?q=CNI)\n   - [Knowledge Base Article](https://access.redhat.com/articles/5436171)\n",
          "more_info": "",
          "reason": "The cluster is using one of the default CNI plugins. \n\n* **Platform**: {{=pydata.platform}}\n* **CNI plugin:** {{=pydata.network_type}}\n",
          "HasReason": true
        },
        "UNSUPPORTED_CNI_PLUGIN": {
          "metadata": {
            "description": "The cluster is using a 3rd party CNI plugin that has not been certified by Red Hat",
            "impact": 3,
            "likelihood": 1,
            "publish_date": "2020-08-06 12:00:00",
            "status": "active",
            "tags": [
              "sbr_shift",
              "openshift",
              "sdn",
              "fault_tolerance"
            ]
          },
          "total_risk": 2,
          "generic": "The cluster is using a 3rd party CNI plugin that has not been certified by Red Hat. This configuration may still be supported by a Red Hat partner and/or the CNI plugin vendor.\n",
          "summary": "",
          "resolution": "Red Hat recommends using Red Hat CNI plugins or Red Hat certified 3rd party CNI plugins:\n\n - [*OpenShiftSDN*](https://docs.openshift.com/container-platform/{{=pydata.current_version}}/networking/cluster-network-operator.html)\n - [*OVNKubernetes*](https://docs.openshift.com/container-platform/{{=pydata.ovn_docs_version}}/networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.html)\n - [*Kuryr*](https://docs.openshift.com/container-platform/{{=pydata.current_version}}/installing/installing_openstack/installing-openstack-installer-kuryr.html)\n - Certified 3rd party CNI plugins\n   - [Red Hat Ecosystem Catalog](https://catalog.redhat.com/software/containers/search?q=CNI)\n   - [Knowledge Base Article](https://access.redhat.com/articles/5436171)\n",
          "more_info": "",
          "reason": "{{? ! pydata.network_type}}\nThe cluster doesn't set the CNI plugin which is not supported by Red Hat.\n\n* **Platform:** {{=pydata.platform}}\n{{??}}\nThis cluster is using a 3rd-party CNI plugin that has NOT been [certified](https://access.redhat.com/articles/5436171) by Red Hat.\n\n* **OCP version:** {{=pydata.current_version}}\n* **Platform:** {{=pydata.platform}}\n* **CNI plugin:**  {{=pydata.network_type}}\n\nNOTE: This combination may still be supported by a Red Hat partner and/or the CNI plugin vendor.\n{{?}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends using Red Hat CNI plugins or Red Hat certified 3rd party CNI plugins:\n\n - [*OpenShiftSDN*](https://docs.openshift.com/container-platform/{{=pydata.current_version}}/networking/cluster-network-operator.html)\n - [*OVNKubernetes*](https://docs.openshift.com/container-platform/{{=pydata.ovn_docs_version}}/networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.html)\n - [*Kuryr*](https://docs.openshift.com/container-platform/{{=pydata.current_version}}/installing/installing_openstack/installing-openstack-installer-kuryr.html)\n - Certified 3rd party CNI plugins\n   - [Red Hat Ecosystem Catalog](https://catalog.redhat.com/software/containers/search?q=CNI)\n   - [Knowledge Base Article](https://access.redhat.com/articles/5436171)\n",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4620671",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.failed_to_set_up_mount_unit_for_garbage_collect_mode"
      },
      "error_keys": {
        "ERROR_SYSTEMD_FAILED_TO_SET_UP_MOUNT_UNIT": {
          "metadata": {
            "description": "Systemd failed to set up the mount unit with the error Argument list too long",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Systemd failed to set up the mount unit with the error \"Argument list too long\".\n",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1924502](https://bugzilla.redhat.com/show_bug.cgi?id=1924502) for more information.",
          "reason": "The garbage collection option cannot set property. That leads Systemd to fail to set up<br>\nmount unit and the pods will fail to create containers and stuck in container creating.<br>\n\nThe following error messaage is found in the pod's events:<br>\n  \"Output: Failed to start transient scope unit: Argument list too long\"",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1924502](https://bugzilla.redhat.com/show_bug.cgi?id=1924502) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4993841",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.auth_operator_idp_connection_refused"
      },
      "error_keys": {
        "AUTHENTICATION_OP_IDP_CONNECTION_REFUSED_ERROR": {
          "metadata": {
            "description": "This rule is checking the Authentication degradedoperator status and reason",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule is checking the Authentication degradedoperator status and reason.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Authentication operator is degraded with error message<br>\n\nOperator name  : {{=pydata.operator[\"name\"]}}<br>\nDegraded status: {{=pydata.operator[\"degraded\"][\"status\"]}}<br>\nMessage        : {{=pydata.operator[\"degraded\"][\"message\"]}}<br>\nReason         : {{=pydata.operator[\"degraded\"][\"reason\"]}}<br>\nLast Transition: {{=pydata.operator[\"degraded\"][\"last_trans_time\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5429331",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.service_mesh_op_forever_updating"
      },
      "error_keys": {
        "SERVICE_MESH_OPERATORS_UPDATING_FOREVER": {
          "metadata": {
            "description": "OpenShift Service Mesh operators' status forever updating",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "OpenShift Service Mesh operators' status forever updating.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Summary     : OSSM Operators being installed from the OperatorHub suddenly go into 'unknown' or 'UpgradePending' states and will not deploy.<br>\n\n{{for (var pod_detail in pydata.result) { }}\nPod Name: {{=pydata.result[pod_detail][\"pod_name\"]}}\nMessage: {{=pydata.result[pod_detail][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.openshift_sdn_node_offline"
      },
      "error_keys": {
        "ERROR_OPENSHIFT_SDN_NODE_OFFLINE": {
          "metadata": {
            "description": "One or more egress nodes have been marked offline",
            "impact": 2,
            "likelihood": 4,
            "publish_date": "2021-03-30 12:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift"
            ]
          },
          "total_risk": 3,
          "generic": "One or more egress nodes have been marked offline\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "An egress node has failed enough probes to have been marked offline for egress IPs.\nIf it has egress CIDRs assigned, its egress IPs have been moved to other nodes.\nThat indicates issues at either the node or the network between the master and the node.\n\nThe following egress{{?pydata.res.length>1}}nodes are{{??}}node is{{?}} experiencing this issue:\n{{ for (var index in pydata.res) { }}\n * {{=pydata.res[index]}}\n{{?}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "An egress node has failed enough probes to have been marked offline for egress IPs.\nIf it has egress CIDRs assigned, its egress IPs have been moved to other nodes.\nThat indicates issues at either the node or the network between the master and the node.\n\nThe following egress{{?pydata.res.length>1}}nodes are{{??}}node is{{?}} experiencing this issue:\n{{ for (var index in pydata.res) { }}\n * {{=pydata.res[index]}}\n{{?}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5315421",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.mcp_unexpected_on_disk_state_content_mismatch_chrony_conf"
      },
      "error_keys": {
        "MCP_UNEXPECTED_ON_DISK_STATE_CONTENT_MISMATCH_CHRONY_CONF": {
          "metadata": {
            "description": "on-disk validation fails on file content mismatch during MCO upgrade..../....../",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "on-disk validation fails on file content mismatch during MCO upgrade..../....../.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Found 'content mismatch for file /etc/chrony.conf' log for following pods:<br>\n~{{for (var pod in pydata.pods) { }}{{=pydata.pods[pod]}}{{}}}~<br>\n\nThe list may be incomplete. Run the below command to get a complete list:<br>\n~<br>\n    $ find path/to/archive -name current.log |\\<br>\n            xargs grep -l \"content mismatch for file /etc/chrony.conf:\"<br>\n~<br>\n\nLog messages sample from pod \"{{=pydata.pods[0]}}\": <br>\n~<br>\n{{=pydata.content_mismatch_message}}\n{{=pydata.marking_degraded_message}}\n~",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5234861",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.bug_1852545"
      },
      "error_keys": {
        "BUGZILLA_BUG_1852545": {
          "metadata": {
            "description": "The cluster shows symptoms of BZ 1852545: OCP 4.5 IPI installation on VSphere is failing",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The cluster shows symptoms of BZ 1852545: OCP 4.5 IPI installation on VSphere is failing. Worker machines refuse to come up.\n",
          "summary": "",
          "resolution": "",
          "more_info": "\n* See [KCS 5234861](https://access.redhat.com/solutions/5234861) for information about possible solutions.\n* See [BZ 1852545](https://bugzilla.redhat.com/show_bug.cgi?id=1852545) for more information about the bug.\n",
          "reason": "The cluster shows symptoms of BZ 1852545: OCP 4.5 IPI installation on VSphere is failing.\n\nWorker machines refuse to come up, as indicated by the following Machine API pod logs:\n\n{{for (var pod_detail in pydata.results) { }}\n* Pod: **{{=pydata.results[pod_detail][\"pod_name\"]}}**<br/>\n  `{{=pydata.results[pod_detail][\"message\"]}}{{}}}`\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "\n* See [KCS 5234861](https://access.redhat.com/solutions/5234861) for information about possible solutions.\n* See [BZ 1852545](https://bugzilla.redhat.com/show_bug.cgi?id=1852545) for more information about the bug.\n",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.operators_check"
      },
      "error_keys": {
        "OPERATOR_HEALTHY": {
          "metadata": {
            "description": "All operators are in desired state",
            "impact": 1,
            "likelihood": 1,
            "publish_date": "2019-11-03 08:25:00",
            "status": "active",
            "tags": null
          },
          "total_risk": 1,
          "generic": "All operators are healthy.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The following operators are experiencing issues:\n{{~ pydata.operators :op }}\n    {{=op[\"operator\"][\"name\"]}}\n    Issues:\n\t{{~ op[\"issues\"] :issues }}\n\t{{=issues[0]}}\n    - Reason: {{=issues[1][\"reason\"]}}\n    - Message: {{=issues[1][\"message\"]}}\n    - LastTransition: {{=issues[1][\"last_trans_time\"]}}\n\t{{~}}\n{{~}}",
          "HasReason": true
        },
        "OPERATOR_ISSUE": {
          "metadata": {
            "description": "If an operator is in Not Available/Degraded/Progressing state, then its having an issue that needs further investigation",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2019-11-03 08:25:00",
            "status": "active",
            "tags": [
              "openshift",
              "incident"
            ]
          },
          "total_risk": 2,
          "generic": "One or more operators are in one of the following unhealthy states:\n- Not Available\n- Degraded\n- Progressing state\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The following operators are experiencing issues:\n{{~ pydata.operators :op }}\n    {{=op[\"operator\"][\"name\"]}}\n    Issues:\n\t{{~ op[\"issues\"] :issues }}\n\t{{=issues[0]}}\n    - Reason: {{=issues[1][\"reason\"]}}\n    - Message: {{=issues[1][\"message\"]}}\n    - LastTransition: {{=issues[1][\"last_trans_time\"]}}\n\t{{~}}\n{{~}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "The following operators are experiencing issues:\n{{~ pydata.operators :op }}\n    {{=op[\"operator\"][\"name\"]}}\n    Issues:\n\t{{~ op[\"issues\"] :issues }}\n\t{{=issues[0]}}\n    - Reason: {{=issues[1][\"reason\"]}}\n    - Message: {{=issues[1][\"message\"]}}\n    - LastTransition: {{=issues[1][\"last_trans_time\"]}}\n\t{{~}}\n{{~}}",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5070671",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.bug_rules.bug_1832986"
      },
      "error_keys": {
        "BUGZILLA_BUG_1832986": {
          "metadata": {
            "description": "The `openshift-etcd-operator` project will get events of `EtcdMemberDegraded` and `UnhealthyEtcdMember` messages",
            "impact": 1,
            "likelihood": 2,
            "publish_date": "2020-05-07 15:08:00",
            "status": "active",
            "tags": [
              "fault_tolerance",
              "openshift",
              "etcd"
            ]
          },
          "total_risk": 1,
          "generic": "Due to a known bug in OpenShift 4.4.3, 4.4.4, 4.4.5 and 4.4.6, the `EtcdMemberDegraded` and `UnhealthyEtcdMember` events are incorrectly triggered in the `openshift-etcd` project. As a result, etcd members are incorrectly marked as unhealthy.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/5070671)\n",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to a version 4.4.7 or above.",
          "more_info": "For more in-depth information check the following:\n1. [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1832986).\n1. [KCS](https://access.redhat.com/solutions/5070671)",
          "reason": "On this OCP 4 cluster, a known bug causes false events in the `openshift-etcd-operator` operator. These events state that the etcd members are unhealthy while they are healthy actually.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to a version 4.4.7 or above.",
      "more_info": "For more in-depth information check the following:\n1. [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1832986).\n1. [KCS](https://access.redhat.com/solutions/5070671)",
      "reason": "On this OCP 4 cluster, a known bug causes false events in the `openshift-etcd-operator` operator. These events state that the etcd members are unhealthy while they are healthy actually.",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5536051",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.openshift_cluster_csi_drivers_pods_pending_during_upgrade"
      },
      "error_keys": {
        "BUGZILLA_BUG_1894025": {
          "metadata": {
            "description": "The cluster shows symptoms of BZ1894025",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule checks if pods in namespace \"openshift-cluster-csi-drivers\" stay \"Pending\" during upgrade to OCP 4.6 and storage operator is in \"Progressing\" state.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Possible known bug found.<br>\nSummary     : Storage operator found in Progressing state.<br>\n\nOperator Name               : {{=pydata.operator[\"name\"]}}<br>\nOperator Progressing state  : {{=pydata.operator[\"progressing\"][\"status\"]}}<br>\nMessage                     : {{=pydata.operator[\"progressing\"][\"message\"]}}<br>\nReason                      : {{=pydata.operator[\"progressing\"][\"reason\"]}}<br>\n\n{{?Object.keys(pydata.pending_pods).length>1}}\nPods from namespace \"openshift-cluster-csi-drivers\" found in Pending state:<br>\n{{for (var pod in pydata.pending_pods) { }}\n~~~\n{{=pydata.pending_pods[pod]}}<br>\n~~~\n{{}}}\n{{?}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5099331",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.mcp_unexpected_on_disk_state_content_mismatch_kubelet_ca_cert"
      },
      "error_keys": {
        "MCP_UNEXPECTED_ON_DISK_STATE_CONTENT_MISMATCH_KUBELET_CA_CERT": {
          "metadata": {
            "description": "Upgrade fails when a node is degraded due to \"content mismatch for file /etc/kubernetes/kubelet-ca.crt\"",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Upgrade fails when a node is degraded due to \"content mismatch for file /etc/kubernetes/kubelet-ca.crt\".",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Found 'content mismatch for file /etc/kubernetes/kubelet-ca.crt' log for following pods:<br>\n~~~{{for (var pod in pydata.pods) { }}{{=pydata.pods[pod]}}{{}}}~~~\n\nThe list may be incomplete. Run this to get a complete list:<br>\n~~~\n    $ find path/to/archive -name current.log |\\<br>\n            xargs grep -l \"content mismatch for file /etc/kubernetes/kubelet-ca.crt\"<br>\n~~~\n\nLog messages sample from pod \"{{=pydata.pods[0]}}\": <br>\n~~~\n{{=pydata.content_mismatch_message}}\n{{=pydata.marking_degraded_message}}\n~~~",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.certificates_expired"
      },
      "error_keys": {
        "CERTIFICATES_EXPIRED": {
          "metadata": {
            "description": "Control plane certificates have expired",
            "impact": 2,
            "likelihood": 4,
            "publish_date": "2020-01-09 09:21:00",
            "status": "active",
            "tags": null
          },
          "total_risk": 3,
          "generic": "Some control plane certificates have expired.\n",
          "summary": "",
          "resolution": "Red Hat recommends you to follow steps in [Recovering from expired control plane certificates](https://docs.openshift.com/container-platform/{{=pydata.version}}/backup_and_restore/disaster_recovery/scenario-3-expired-certs.html).\n\nIf you know a better solution for this issue, please let us know at ccx@redhat.com!\n",
          "more_info": "",
          "reason": "The following control plane certificates have expired:\n\n{{~ pydata.expired :cert }}\n* {{=cert.name}} (expired on {{=cert.not_valid_after}})\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends you to follow steps in [Recovering from expired control plane certificates](https://docs.openshift.com/container-platform/{{=pydata.version}}/backup_and_restore/disaster_recovery/scenario-3-expired-certs.html).\n\nIf you know a better solution for this issue, please let us know at ccx@redhat.com!\n",
      "more_info": "",
      "reason": "The following control plane certificates have expired:\n\n{{~ pydata.expired :cert }}\n* {{=cert.name}} (expired on {{=cert.not_valid_after}})\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4539631",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.image_registry_failed_to_push_image"
      },
      "error_keys": {
        "IMAGE_REGISTRY_FAILED_TO_PUSH": {
          "metadata": {
            "description": "Failed to push images to the internal registry when the NFSshare was created with an incompatible set of permissions",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Failed to push images to the internal registry when the NFSshare was created with an incompatible set of permissions.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Failed to push images to the internal registry when the NFS<br>\nshare was created with an incompatible set of permissions<br>\n\n{{for (var log in pydata.logs) { }}\nPod Name    : {{=pydata.logs[log][\"pod_name\"]}}\nMessage     :<br>\n{{=pydata.logs[log][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5636901",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.auth_op_degraded_due_to_wellknownreadycontroller_syncerror"
      },
      "error_keys": {
        "AUTH_OPERATOR_DEGRADED_DUE_TO_WELLKNOWNREADYCONTROLLER_SYNCERROR": {
          "metadata": {
            "description": "This rule checks if authentication operator is degraded due to wellknownreadycontroller sync error",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule checks if authentication operator is degraded due to wellknownreadycontroller sync error.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Operator Name   : {{=pydata.op[\"name\"]}}<br>\nDegraded Status : {{=pydata.op[\"degraded\"][\"status\"]}}<br>\n<br>\nMessage : {{=pydata.op[\"degraded\"][\"message\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4834881",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.provision_volume_storageclass_thin_permission"
      },
      "error_keys": {
        "PROVISION_VOLUME_STORAGECLASS_THIN_PERMISSION": {
          "metadata": {
            "description": "Permission was denied when provisioning volume with StorageClass thin",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Permission was denied when provisioning volume with StorageClass thin.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Creating pvc with storageClass type thin prompting errors \"Permission to perform this operation was denied\"<br>\n\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage : {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.nodes_requirements_check"
      },
      "error_keys": {
        "NODES_MINIMUM_REQUIREMENTS_NOT_MET": {
          "metadata": {
            "description": "An OCP node behaves unexpectedly when it doesn't meet the minimum resource requirements",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2019-10-29 15:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "configuration",
              "performance"
            ]
          },
          "total_risk": 2,
          "generic": "Some cluster nodes do not meet minimum resource requirements for Openshift Container Platform 4. The lack of resources can cause the nodes to behave unexpectedly.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you configure your nodes to meet the minimum resource requirements.\n\nMake sure that:\n\n{{~ pydata.nodes :node }}\n1. Node {{=node[\"name\"]}} ({{=node[\"roles\"]}}){{?node[\"memory\"]}}\n   * Has enough memory, minimum requirement is {{=node[\"memory_req\"]}}. Currently its only configured with {{=node[\"memory\"]}}GB.{{?}}{{?node.cpu}}\n   * Has enough allocatable cpu, minimum requirement is {{=node[\"cpu_req\"]}}. Currently its only configured with {{=node[\"cpu\"]}}.{{?}}{{~}}\n",
          "more_info": "For more information about the minimum resource requirements, refer to the [Minimum resource requirements](https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html#minimum-resource-requirements_installing-bare-metal) section in the OCP4 documentation.\n",
          "reason": "Node{{?pydata.nodes.length>1}}s{{?}} not meeting the minimum requirements:\n{{~ pydata.nodes :node }}\n1. {{=node[\"name\"]}}\n  * Roles: {{=node[\"roles\"]}}{{?node.memory}}\n  * Minimum memory requirement is {{=node[\"memory_req\"]}}, but the node is configured with {{=node[\"memory\"]}}.{{?}}{{?node.cpu}}\n  * Minimum cpu requirement is {{=node[\"cpu_req\"]}}, but the node is configured with {{=node[\"cpu\"]}}.{{?}}{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you configure your nodes to meet the minimum resource requirements.\n\nMake sure that:\n\n{{~ pydata.nodes :node }}\n1. Node {{=node[\"name\"]}} ({{=node[\"roles\"]}}){{?node[\"memory\"]}}\n   * Has enough memory, minimum requirement is {{=node[\"memory_req\"]}}. Currently its only configured with {{=node[\"memory\"]}}GB.{{?}}{{?node.cpu}}\n   * Has enough allocatable cpu, minimum requirement is {{=node[\"cpu_req\"]}}. Currently its only configured with {{=node[\"cpu\"]}}.{{?}}{{~}}\n",
      "more_info": "For more information about the minimum resource requirements, refer to the [Minimum resource requirements](https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html#minimum-resource-requirements_installing-bare-metal) section in the OCP4 documentation.\n",
      "reason": "Node{{?pydata.nodes.length>1}}s{{?}} not meeting the minimum requirements:\n{{~ pydata.nodes :node }}\n1. {{=node[\"name\"]}}\n  * Roles: {{=node[\"roles\"]}}{{?node.memory}}\n  * Minimum memory requirement is {{=node[\"memory_req\"]}}, but the node is configured with {{=node[\"memory\"]}}.{{?}}{{?node.cpu}}\n  * Minimum cpu requirement is {{=node[\"cpu_req\"]}}, but the node is configured with {{=node[\"cpu\"]}}.{{?}}{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5252831",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.tls_handshake_fails_in_azure"
      },
      "error_keys": {
        "TLS_HANDSHAKE_FAILS_IN_AZURE": {
          "metadata": {
            "description": "OpenShift clusters on Azure are at risk of becoming very slow with many operators becoming unhealthy",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-04-12 16:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift",
              "operator"
            ]
          },
          "total_risk": 2,
          "generic": "OpenShift clusters on Azure are at risk of becoming very slow with many operators becoming unhealthy due to an Azure bug.\n",
          "summary": "",
          "resolution": "{{?pydata.version.startsWith(\"4.6\")}}\nRed Hat recommends that you upgrade to 4.6.38 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.6/updating/updating-cluster-between-minor.html) for the steps.\n{{??}}\nRed Hat recommends that you upgrade to 4.7.18 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.7/updating/updating-cluster-between-minor.html) for the steps.\n{{?}}",
          "more_info": "",
          "reason": "This OpenShift cluster is running on Azure with the version {{=pydata.version}}. Due to an Azure bug, the cluster is at risk of entering a state in which it would start discarding large packets in inter-node communication. That would eventually lead to the cluster becoming very slow and many operators becoming unhealthy.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "{{?pydata.version.startsWith(\"4.6\")}}\nRed Hat recommends that you upgrade to 4.6.38 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.6/updating/updating-cluster-between-minor.html) for the steps.\n{{??}}\nRed Hat recommends that you upgrade to 4.7.18 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.7/updating/updating-cluster-between-minor.html) for the steps.\n{{?}}",
      "more_info": "",
      "reason": "This OpenShift cluster is running on Azure with the version {{=pydata.version}}. Due to an Azure bug, the cluster is at risk of entering a state in which it would start discarding large packets in inter-node communication. That would eventually lead to the cluster becoming very slow and many operators becoming unhealthy.\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.openshift_router_duplicate_targetports"
      },
      "error_keys": {
        "ROUTE_REFERENCES_DUPLICATE_TARGETPORTS": {
          "metadata": {
            "description": "Router fails to reload with error no child processes in Openshift 4",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Router fails to reload with error no child processes in Openshift 4.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1858879](https://bugzilla.redhat.com/show_bug.cgi?id=1858879) for more information.",
          "reason": "OpenShift router fails with a route referencing a service with duplicate targetPort values.<br>\n\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage : {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1858879](https://bugzilla.redhat.com/show_bug.cgi?id=1858879) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.node_degraded_due_to_deleted_rendered_config"
      },
      "error_keys": {
        "NODE_DEGRADED_DUE_TO_DELETED_MACHINECONFIG": {
          "metadata": {
            "description": "Deletion of rendered config should be avoided as it leads to node to be degraded",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Deletion of rendered config should be avoided as it leads to node to be degraded.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Kcs](https://access.redhat.com/solutions/4970731) for more information.",
          "reason": "Found node degraded log due to deleted rendered config:<br>\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage: {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Kcs](https://access.redhat.com/solutions/4970731) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5588161",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.azure_service_principals_keys_expired"
      },
      "error_keys": {
        "AZURE_SERVICE_PRINCIPALS_KEY_EXPIRED": {
          "metadata": {
            "description": "Cloud credential operator is degraded due to expired Azure service principals keys",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Cloud credential operator is degraded due to expired Azure service principals keys.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Operator Name      : {{=pydata.op[\"name\"]}}<br>\nDegraded Status    : {{=pydata.op[\"degraded\"][\"status\"]}}<br>\nMessage            : {{=pydata.op[\"degraded\"][\"message\"]}}<br>\n<br>\nIt seems that Azure service principals key have expired.<br>\nUpdate the service principal key by getting proper credentials from Microsoft Azure Console.<br>\n<br>\nFollowing Error has been found:<br>\n{{for (var pod_data in pydata.result) { }}\nPod Name: {{=pydata.result[pod_data][\"pod_name\"]}}\nMessage: {{=pydata.result[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4516391",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.image_registry_multiple_storage_types"
      },
      "error_keys": {
        "IMAGE_REGISTRY_MULTIPLE_STORAGE_TYPES": {
          "metadata": {
            "description": "The Image Registry Operator fails to apply Image Registry configuration when multiple storage types are specified",
            "impact": 1,
            "likelihood": 4,
            "publish_date": "2020-01-27 12:00:00",
            "status": "active",
            "tags": [
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "The Image Registry Operator requires exactly one storage type in the `storage` section of its configuration in the `configs.imageregistry.operator.openshift.io` resource. When multiple storage types are specified, the operator fails to apply the configuration and reports an error.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you keep exactly one storage type in the `storage` section in the `configs.imageregistry.operator.openshift.io` resource.\n",
          "more_info": "* For more information about troubleshooting and resolving the issue, see the [knowledgebase article](https://access.redhat.com/solutions/4516391).\n* For more information about the Image Registry and the Image Registry Operator, see the [corresponding section](https://docs.openshift.com/container-platform/latest/registry/configuring-registry-operator.html) in the product documentation.\n",
          "reason": "The Image Registry Operator reports that the `storage` section of its configuration in the `configs.imageregistry.operator.openshift.io` resource contains multiple storage types:\n\n> {{=pydata[\"info\"][\"condition\"][\"message\"]}}\n\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you keep exactly one storage type in the `storage` section in the `configs.imageregistry.operator.openshift.io` resource.\n",
      "more_info": "* For more information about troubleshooting and resolving the issue, see the [knowledgebase article](https://access.redhat.com/solutions/4516391).\n* For more information about the Image Registry and the Image Registry Operator, see the [corresponding section](https://docs.openshift.com/container-platform/latest/registry/configuring-registry-operator.html) in the product documentation.\n",
      "reason": "The Image Registry Operator reports that the `storage` section of its configuration in the `configs.imageregistry.operator.openshift.io` resource contains multiple storage types:\n\n> {{=pydata[\"info\"][\"condition\"][\"message\"]}}\n\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6177542",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.upgrade_failed_for_network_operator"
      },
      "error_keys": {
        "CLUSTER_OPERATOR_NETWORK_NOT_AVAILABLE": {
          "metadata": {
            "description": "Cluster upgrade failed for the drop-icmp container of sdn pods used an invalid kubeconfig",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Cluster upgrade failed for the drop-icmp container of sdn pods used an invalid kubeconfig.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The cluster failed to upgrade to {{=pydata.version}} for the kubeconfig used by drop-icmp containers<br>\nin the following pods is invalid. And there should be a `/root/.kube/config` file exists<br>\nin the nodes where the pods are scheduled.<br>\n\n{{for (var p in pydata.pods) { }}\n - **Pod**: {{=pydata.pods[p].pod}} on **Node**: {{=pydata.pods[p].node}}{{}}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5442201",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.kube_scheduler_certs_not_renewed"
      },
      "error_keys": {
        "BUGZILLA_BUG_1881322": {
          "metadata": {
            "description": "kube-scheduler certificates not renewed automatically",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "kube-scheduler certificates not renewed automatically.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1881322](https://bugzilla.redhat.com/show_bug.cgi?id=1881322) for more information.",
          "reason": "Possible known Bug Found.<br>\nSummary     : Due to 'kube-scheduler-client-cert-key' secret mismatch, kube-scheduler pods are encountering 'Unauthorized' messages.<br>\n\n{{for (var pod_detail in pydata.res) { }}\nPod Name    : {{=pydata.res[pod_detail][\"pod_name\"]}}\nMessage     :<br>\n{{=pydata.res[pod_detail][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1881322](https://bugzilla.redhat.com/show_bug.cgi?id=1881322) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6272781",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.kube_rbac_proxy_crashloop_during_authentication"
      },
      "error_keys": {
        "ERROR_KUBE_RBAC_PROXY_CRASHLOOP": {
          "metadata": {
            "description": "kube-rbac-proxy pods crashloop instead of loop waiting for the certificate to be available",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "kube-rbac-proxy pods crashloop instead of loop waiting for the certificate to be available.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The kube-rbac-proxy pods crashloop instead of loop waiting for the certificate to be available.<br>\nThe following errors are found in kubelet service:<br>\n{{for (var msg in pydata.results) { }}Message: {{=pydata.results[msg]}}<br>{{}}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4950151",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.create_smcp_under_operator_ns"
      },
      "error_keys": {
        "CREATE_SMCP_UNDER_OPERATOR_NS": {
          "metadata": {
            "description": "ServiceMeshControlPlane resource was created under the namespace same as Service Mesh Operator",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "ServiceMeshControlPlane resource was created under the namespace same as Service Mesh Operator\n",
          "summary": "",
          "resolution": "Red Hat recommends that you to perform the following steps:\n1. Delete the ServiceMeshControlPlane resource from `{{=pydata.namespace}}` namespace, as that namespace should not have any SMCPs:\n   ~~~\n   $oc delete $(oc get smcp -o name -n {{=pydata.namespace}}) -n {{=pydata.namespace}}\n   ~~~\n1. See https://docs.openshift.com/container-platform/4.8/service_mesh/v1x/installing-ossm.html to recreate it.\n",
          "more_info": "",
          "reason": "ServiceMeshControlPlane resource was created under `{{=pydata.namespace}}` namespace. Creating SMCP resources in the same namespace as the Service Mesh Operator installed is not permitted, and no ServiceMeshControlPlane resource will be created.\n\n**Message:** {{=pydata.logs}}\n\nSee https://docs.openshift.com/container-platform/4.8/service_mesh/v1x/installing-ossm.html to get more information about how to create it.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you to perform the following steps:\n1. Delete the ServiceMeshControlPlane resource from `{{=pydata.namespace}}` namespace, as that namespace should not have any SMCPs:\n   ~~~\n   $oc delete $(oc get smcp -o name -n {{=pydata.namespace}}) -n {{=pydata.namespace}}\n   ~~~\n1. See https://docs.openshift.com/container-platform/4.8/service_mesh/v1x/installing-ossm.html to recreate it.\n",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.master_defined_as_machinesets"
      },
      "error_keys": {
        "MASTER_DEFINED_AS_MACHINESETS": {
          "metadata": {
            "description": "Additional risks would occur possibly when having the masters defined as machinesets",
            "impact": 2,
            "likelihood": 2,
            "publish_date": "2020-11-13 16:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "configuration",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "Additional risks can occur when the master nodes are defined as MachineSets.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you delete the affected machineset, replace master with a new healthy one.\n\nPlease refer to this [documentation](https://docs.openshift.com/container-platform/4.4/backup_and_restore/replacing-unhealthy-etcd-member.html#restore-replace-stopped-etcd-member_replacing-unhealthy-etcd-member) for detail information.\n",
          "more_info": "",
          "reason": "The following MachineSet{{?pydata.result.length > 1}}s{{?}} include{{?pydata.result.length == 1}}s{{?}} master node. While this configuration does not provide obvious benefits, it does pose additional risks.\n\n{{~pydata.result:item}}\n**MachineSet Name**: {{=item}}\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you delete the affected machineset, replace master with a new healthy one.\n\nPlease refer to this [documentation](https://docs.openshift.com/container-platform/4.4/backup_and_restore/replacing-unhealthy-etcd-member.html#restore-replace-stopped-etcd-member_replacing-unhealthy-etcd-member) for detail information.\n",
      "more_info": "",
      "reason": "The following MachineSet{{?pydata.result.length > 1}}s{{?}} include{{?pydata.result.length == 1}}s{{?}} master node. While this configuration does not provide obvious benefits, it does pose additional risks.\n\n{{~pydata.result:item}}\n**MachineSet Name**: {{=item}}\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5448851",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.openshift_apiserver_pnf_featuregates"
      },
      "error_keys": {
        "BUGZILLA_BUG_1888309": {
          "metadata": {
            "description": "API Priority and Fairness Alpha featureGate throttles requests in OpenShift",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "API Priority and Fairness Alpha featureGate throttles requests in OpenShift.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1888309](https://bugzilla.redhat.com/show_bug.cgi?id=1888309) for more information.",
          "reason": "Certain workloads can trigger Priority and Fairness to throttle requets from OAS<br>\nwhich can cause an OpenShift 4.5/4.6/4.7 cluster to become unresponsive.<br>\n\n{{for (var pod_detail in pydata.res) { }}\nPod Name    : {{=pydata.res[pod_detail][\"pod_name\"]}}\nMessage     :<br>\n{{=pydata.res[pod_detail][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1888309](https://bugzilla.redhat.com/show_bug.cgi?id=1888309) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5663021",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.cmo_container_run_as_non_root"
      },
      "error_keys": {
        "CMO_CONTAINER_RUN_AS_NON_ROOT": {
          "metadata": {
            "description": "Cluster monitoring operator pods are stuck in pending status due to an OpenShift bug",
            "impact": 2,
            "likelihood": 4,
            "publish_date": "2021-02-02 16:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift",
              "operator"
            ]
          },
          "total_risk": 3,
          "generic": "Cluster monitoring operator pods are stuck in pending status due to an OpenShift bug.",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade to 4.6.12 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.6/updating/updating-cluster-between-minor.html) for the steps.",
          "more_info": "For more in-depth information please check the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1906836).",
          "reason": "This OpenShift cluster is running in the version {{=pydata.version}}. Due to a bug in this version, the following cluster monitor operator pods are stuck in pending status with an error message `container has runAsNonRoot and image has non-numeric user (nobody)`.\n\n**POD Name:**\n{{~pydata.issue_pods:item}}\n- {{=item}}\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you upgrade to 4.6.12 or above to avoid this issue. Please refer to the [OpenShift Documentation](https://docs.openshift.com/container-platform/4.6/updating/updating-cluster-between-minor.html) for the steps.",
      "more_info": "For more in-depth information please check the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1906836).",
      "reason": "This OpenShift cluster is running in the version {{=pydata.version}}. Due to a bug in this version, the following cluster monitor operator pods are stuck in pending status with an error message `container has runAsNonRoot and image has non-numeric user (nobody)`.\n\n**POD Name:**\n{{~pydata.issue_pods:item}}\n- {{=item}}\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4978291",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.build_slowness_due_to_dynatrace_oneagent"
      },
      "error_keys": {
        "BUILD_SLOWNESS_DUE_TO_DYNATRACE_ONEAGENT": {
          "metadata": {
            "description": "OpenShift Builds time performance is significantly degraded when the dynatrace-oneagent-operator was installed",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "OpenShift Builds time performance is significantly degraded when the dynatrace-oneagent-operator was installed.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1908704](https://bugzilla.redhat.com/show_bug.cgi?id=1908704) for more information.",
          "reason": "The operator {{=pydata.operator}} was installed in the cluster.<br>\nIt will cause OpenShift Builds take much longer time to complete.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1908704](https://bugzilla.redhat.com/show_bug.cgi?id=1908704) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.high_severity_alerts"
      },
      "error_keys": {
        "HIGH_SEVERITY_ALERTS": {
          "metadata": {
            "description": "High severity alerts are firing on the cluster",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "High severity alerts are firing on the cluster.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Following alerts are firing on the cluster.\n\n\n<table border=\"1\" align=\"left\">\n  <tr>\n    <th style=\"text-align:center;\">Alert Name</th>\n    <th style=\"text-align:center;\">Severity</th>\n    <th style=\"text-align:center;\">State</th>\n    <th style=\"text-align:center;\">Namespace</th>\n    <th style=\"text-align:center;\">Message</th>\n  </tr>\n{{~pydata.alerts:alert}}\n<tr>\n    <td style=\"background-color:#FAFAFA;text-align:center;font-weight:normal;\">{{=alert[\"alertname\"]}} </td>\n    <td style=\"background-color:#FAFAFA;text-align:left;font-weight:normal;\">{{=alert[\"severity\"]}} </td>\n    <td style=\"background-color:#FAFAFA;text-align:left;font-weight:normal;\">{{=alert[\"state\"]}} </td>\n    <td style=\"background-color:#FAFAFA;text-align:left;font-weight:normal;\">{{=alert[\"namespace\"]}} </td>\n    <td style=\"background-color:#FAFAFA;text-align:left;font-weight:normal;\">{{=alert[\"message\"]}} </td>\n</tr>\n{{~}}\n</table>",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.version_forced"
      },
      "error_keys": {
        "FORCED_VERSION_UPDATES": {
          "metadata": {
            "description": "Found forced version updates that can void support",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2020-02-20 08:25:00",
            "status": "active",
            "tags": [
              "openshift"
            ]
          },
          "total_risk": 2,
          "generic": "A version in the cluster version history is not verified which means the upgrade must have been forced. Forcing a version upgrade can void support.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Forced versions:\n\n{{~ pydata.forced_versions :fv }}\n    Version '{{=fv[\"version\"]}}'\n    - State          : {{=fv[\"state\"]}}\n    - Started time   : {{=fv[\"started_time\"]}}\n    - Completion time: {{=fv[\"completion_time\"]}}\n    - Verified       : {{=fv[\"verified\"]}}\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "Forced versions:\n\n{{~ pydata.forced_versions :fv }}\n    Version '{{=fv[\"version\"]}}'\n    - State          : {{=fv[\"state\"]}}\n    - Started time   : {{=fv[\"started_time\"]}}\n    - Completion time: {{=fv[\"completion_time\"]}}\n    - Verified       : {{=fv[\"verified\"]}}\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5494581",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.check_pods_stuck_in_NodeAffinity_state"
      },
      "error_keys": {
        "BUGZILLA_BUG_1868645": {
          "metadata": {
            "description": "This rule checks if any pods are stuck in NodeAffinity state",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule checks if any pods are stuck in NodeAffinity state.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1868645](https://bugzilla.redhat.com/show_bug.cgi?id=1868645) for more information.",
          "reason": "Possible known bug found.<br>\nSummary : Found below pods in \"Failed\" state with reason \"NodeAffinity\":<br>\n~~~\n{{for (var pod in pydata.pods) { }}{{=pydata.pods[pod]}}\n{{}}}~~~",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1868645](https://bugzilla.redhat.com/show_bug.cgi?id=1868645) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6051631",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.security.CVE_2021_30465_runc"
      },
      "error_keys": {
        "CVE_2021_30465_RUNC_VULN": {
          "metadata": {
            "description": "CVE-2021-30465: runc vulnerable to privilege escalation",
            "impact": 3,
            "likelihood": 2,
            "publish_date": "2021-05-19 10:00:00",
            "status": "active",
            "tags": [
              "security",
              "openshift",
              "cve"
            ]
          },
          "total_risk": 2,
          "generic": "OpenShift versions prior to 4.5.40, 4.6.30, or 4.7.12 are vulnerable to CVE-2021-30465, a vulnerability in runc that could lead to information leakage or privilege escalation. This vulnerability has a security impact of Important.\n\nAn attacker is able to request a container configuration that results in the host file being bind-mounted into the container filesystem, allowing for a symlink exchange attack which could result in information leakage from other containers or cluster-level privilege escalation.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you update the cluster to the most recent version for which fixes are available.\n\n{{? ! pydata.fix_available }}Fixes are not available for this minor version. See the *Updating a cluster between minor versions* section of the OpenShift documentation:\n* [OpenShift {{=pydata.minor_version}} Documentation](https://docs.openshift.com/container-platform/{{=pydata.minor_version}}/updating/updating-cluster-between-minor.html){{??}}\nFor more information, see the *Updating a cluster within a minor version from the web console* section of the OpenShift documentation:\n\n* [OpenShift {{=pydata.minor_version}} Documentation](https://docs.openshift.com/container-platform/{{=pydata.minor_version}}/updating/updating-cluster.html){{?}}\n",
          "more_info": "* For more information about the flaw, see its [security bulletin](https://access.redhat.com/security/vulnerabilities/RHSB-2021-004).\n* For more information about this CVE, see its [CVE page](https://access.redhat.com/security/cve/CVE-2021-30465)\n* For information on upgrading OpenShift, see [the OpenShift documentation](https://docs.openshift.com/container-platform/4.7/updating/understanding-the-update-service.html).\n* The Customer Portal page for the [Red Hat Security Team](https://access.redhat.com/security/) contains more information about policies, procedures, and alerts for Red Hat Products.\n* The Security Team also maintains a frequently updated blog at [securityblog.redhat.com](https://securityblog.redhat.com).\n",
          "reason": "* The cluster is running a vulnerable version of OpenShift, {{=pydata.current_version}}.{{? ! pydata.fix_available}}\n* This OpenShift version is beyond end-of-life. Fixes are not available for this version.{{?}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you update the cluster to the most recent version for which fixes are available.\n\n{{? ! pydata.fix_available }}Fixes are not available for this minor version. See the *Updating a cluster between minor versions* section of the OpenShift documentation:\n* [OpenShift {{=pydata.minor_version}} Documentation](https://docs.openshift.com/container-platform/{{=pydata.minor_version}}/updating/updating-cluster-between-minor.html){{??}}\nFor more information, see the *Updating a cluster within a minor version from the web console* section of the OpenShift documentation:\n\n* [OpenShift {{=pydata.minor_version}} Documentation](https://docs.openshift.com/container-platform/{{=pydata.minor_version}}/updating/updating-cluster.html){{?}}\n",
      "more_info": "* For more information about the flaw, see its [security bulletin](https://access.redhat.com/security/vulnerabilities/RHSB-2021-004).\n* For more information about this CVE, see its [CVE page](https://access.redhat.com/security/cve/CVE-2021-30465)\n* For information on upgrading OpenShift, see [the OpenShift documentation](https://docs.openshift.com/container-platform/4.7/updating/understanding-the-update-service.html).\n* The Customer Portal page for the [Red Hat Security Team](https://access.redhat.com/security/) contains more information about policies, procedures, and alerts for Red Hat Products.\n* The Security Team also maintains a frequently updated blog at [securityblog.redhat.com](https://securityblog.redhat.com).\n",
      "reason": "* The cluster is running a vulnerable version of OpenShift, {{=pydata.current_version}}.{{? ! pydata.fix_available}}\n* This OpenShift version is beyond end-of-life. Fixes are not available for this version.{{?}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5396191",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.failed_to_create_scope_for_machine"
      },
      "error_keys": {
        "FAILED_TO_CREATE_SCOPE_FOR_MACHINE": {
          "metadata": {
            "description": "This rule is checking if bootstrap/master nodes are unablecommunicate with the vCenter SDK endpoint",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule is checking if bootstrap/master nodes are unablecommunicate with the vCenter SDK endpoint.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The installation is not progressing due to some cluster-operators still degraded<br>\n(ingress and all the operators depending on ingress such as authentication,<br>\nand workers not being deployed)<br>\n\nFound the following error logs:{{for (var pod_data in pydata.results) { }}Pod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage: {{=pydata.results[pod_data][\"message\"]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4874631",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.ingress_op_degraded_unmatched_hosted_zone"
      },
      "error_keys": {
        "INGRESS_OPERATOR_DEGRADED_DNS_RECORD_PROVIDER_ERROR": {
          "metadata": {
            "description": "Cluster ingress operator degraded due to DNS Record Provider error",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Cluster ingress operator degraded due to DNS Record Provider error.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Cluster ingress operator degraded due to DNS Record Provider error.<br>\n\nOperator Name           : {{=pydata.state[\"name\"]}}<br>\nOperator Degraded State : {{=pydata.state[\"degraded\"][\"status\"]}}<br>\nMessage                 : {{=pydata.state[\"degraded\"][\"message\"]}}<br>\nReason                  : {{=pydata.state[\"degraded\"][\"reason\"]}}<br>\nDNSRecord Message       : {{=pydata.dnsrecord_msg}}<br>\n<br>\nPlease refer to the KCS for detailed troubleshooting info.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5317441",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.mcp_set_to_pause"
      },
      "error_keys": {
        "MCP_SET_TO_PAUSE": {
          "metadata": {
            "description": "MachineConfig operator will be degraded on upgrade when MachineConfig pools are paused",
            "impact": 2,
            "likelihood": 2,
            "publish_date": "2021-07-28 12:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "configuration",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "MachineConfig operator will be degraded on upgrade when one ore more MachineConfig pools are paused\n",
          "summary": "",
          "resolution": "Red Hat recommends you change `spec.paused` to **false** for each identified MachineConfigpool having `spec.paused: true` with the following command:\n\n~~~\n{{ for (var index in pydata.pause) { }}\n$ oc patch mcp {{=pydata.pause[index]['name']}} --type=merge -p '{\"spec\": {\"paused\": false}}'\n{{?}}\n~~~\n",
          "more_info": "",
          "reason": "The MachineConfig operator upgrade will be blocked because the following MachineConfigPools are paused:\n\n{{ for (var index in pydata.pause) { }}\n- {{=pydata.pause[index]['name']}}\n{{?}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends you change `spec.paused` to **false** for each identified MachineConfigpool having `spec.paused: true` with the following command:\n\n~~~\n{{ for (var index in pydata.pause) { }}\n$ oc patch mcp {{=pydata.pause[index]['name']}} --type=merge -p '{\"spec\": {\"paused\": false}}'\n{{?}}\n~~~\n",
      "more_info": "",
      "reason": "The MachineConfig operator upgrade will be blocked because the following MachineConfigPools are paused:\n\n{{ for (var index in pydata.pause) { }}\n- {{=pydata.pause[index]['name']}}\n{{?}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.bug_rules.bug_1893386"
      },
      "error_keys": {
        "BUGZILLA_BUG_1893386": {
          "metadata": {
            "description": "The `authentication` operator falsely reports `ReadyIngressNodes_NoReadyIngressNodes`",
            "impact": 2,
            "likelihood": 2,
            "publish_date": "2020-10-30 23:40:00",
            "status": "active",
            "tags": [
              "openshift",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "Due to a known bug in OpenShift \"4.6.x\", the `authentication` operator reports `Available` `False` with the `ReadyIngressNodes_NoReadyIngressNodes` reason, even in situations where ingress nodes are ready.\n\n[Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1893386).\n",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to version 4.6.4 or above.  Alternatively, you can provision at least one node with the `node-role.kubernetes.io/worker` label.\n",
          "more_info": "For more in-depth information check the following:\n1. [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1893386)\n",
          "reason": "On this OCP 4.6 cluster, a known bug causes the `authentication` operator to interpret the lack of `worker` nodes as a ingress blocker, regardless of whether the `default` `IngressController`'s `nodePlacement` allows router pods to be scheduled on nodes with different roles.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to version 4.6.4 or above.  Alternatively, you can provision at least one node with the `node-role.kubernetes.io/worker` label.\n",
      "more_info": "For more in-depth information check the following:\n1. [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1893386)\n",
      "reason": "On this OCP 4.6 cluster, a known bug causes the `authentication` operator to interpret the lack of `worker` nodes as a ingress blocker, regardless of whether the `default` `IngressController`'s `nodePlacement` allows router pods to be scheduled on nodes with different roles.\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4382761",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.kube_controller_manager_operator_degraded_error"
      },
      "error_keys": {
        "KUBE_CONTROLLER_MANAGER_OPERATOR_DEGRADED_WITH_ERROR": {
          "metadata": {
            "description": "This rule is checking the kube-controller-manager operator statuswhich is in degraded state",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule is checking the kube-controller-manager operator statuswhich is in degraded state.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "kube-controller-manager operator is degraded with error message<br>\n\nOperator name  : {{=pydata.operator[\"name\"]}}<br>\nDegraded status: {{=pydata.operator[\"degraded\"][\"status\"]}}<br>\nMessage        : {{=pydata.operator[\"degraded\"][\"message\"]}}<br>\nReason         : {{=pydata.operator[\"degraded\"][\"reason\"]}}<br>\nLast Transition: {{=pydata.operator[\"degraded\"][\"last_trans_time\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5598401",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.mcp_degraded_due_to_osimage_newer_version"
      },
      "error_keys": {
        "MCP_DEGRADED_DUE_TO_WRONG_OSIMAGEURL_ON_OCP46_AND_HIGHER": {
          "metadata": {
            "description": "The rule checks Machine Config Pool degraded dye to wrong OSimage URL in OCP 4.6and Higher",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The rule checks Machine Config Pool degraded dye to wrong OSimage URL in OCP 4.6and Higher.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The Following MCPS are degraded:<br>\n\n{{for (var temp in pydata.mcp) { }}\n- {{=pydata.mcp[temp]}}\n{{}}}\n\nFollowing ERROR logs have been found:<br>\n\n{{for (var pod in pydata.logs) { }}\nPod Name: {{=pydata.logs[pod][\"pod_name\"]}}\n\nMessage: {{=pydata.logs[pod][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5622051",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.psp_installed"
      },
      "error_keys": {
        "PSP_INSTALLED": {
          "metadata": {
            "description": "Workloads are using the deprecated PodSecurityPolicy API",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-09-08 12:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "performance",
              "osd_customer"
            ]
          },
          "total_risk": 2,
          "generic": "The PodSecurityPolicy API is deprecated since OCP 4.4 (Kubernetes v1.21) and will be removed in a future OCP version (Kubernetes v1.25). The OpenShift cluster load also increases disproportionately when PodSecurityPolicies are used.",
          "summary": "",
          "resolution": "Red Hat recommends that you use [SecurityContextConstraints(SCC)](https://docs.openshift.com/container-platform/4.8/authentication/managing-security-context-constraints.html\n) to replace PodSecurityPolicy. SecurityContextConstraints are more feature-rich and more stable. \n",
          "more_info": "",
          "reason": "In the OpenShift cluster, the following {{?pydata.psps.length>1}}PodSecurityPolicies are{{??}}PodSecurityPolicy is{{?}} installed:\n{{~pydata.psps:psp}}\n{{=psp}}\n{{~}}\n\nThe PodSecurityPolicy API is deprecated since OCP 4.4 (Kubernetes v1.21) and will be removed in a future OCP version (Kubernetes v1.25). The OpenShift cluster load also increases disproportionately when PodSecurityPolicies are used.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you use [SecurityContextConstraints(SCC)](https://docs.openshift.com/container-platform/4.8/authentication/managing-security-context-constraints.html\n) to replace PodSecurityPolicy. SecurityContextConstraints are more feature-rich and more stable. \n",
      "more_info": "",
      "reason": "In the OpenShift cluster, the following {{?pydata.psps.length>1}}PodSecurityPolicies are{{??}}PodSecurityPolicy is{{?}} installed:\n{{~pydata.psps:psp}}\n{{=psp}}\n{{~}}\n\nThe PodSecurityPolicy API is deprecated since OCP 4.4 (Kubernetes v1.21) and will be removed in a future OCP version (Kubernetes v1.25). The OpenShift cluster load also increases disproportionately when PodSecurityPolicies are used.",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5861731",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.portworx_unlinkat_private_json"
      },
      "error_keys": {
        "PORTWORX_UNLINKAT_PRIVATE_JSON": {
          "metadata": {
            "description": "Machine config pool is stuck when node is unable to apply new os image dueto Portworx files being marked as unmutable",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Machine config pool is stuck when node is unable to apply new os image dueto Portworx files being marked as unmutable.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The cluster seems to have the Portworx operator installed and the MCP is stuck due to the immutable bit being set on files in /etc/pwx/.<br>\nPortworx has a fix in the 2.6 release for this issue<br>\n\n{{for (var pod in pydata.logs) { }}\nPod Name: {{=pydata.logs[pod][\"pod_name\"]}}\n\nMessage: {{=pydata.logs[pod][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4901071",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.auth_invalid_name_contain_slash"
      },
      "error_keys": {
        "AUTH_INVALID_NAME_CONTAINS_SLASH": {
          "metadata": {
            "description": "Failure to login when resource name contains \"/\"",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Failure to login when resource name contains \"/\".",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1812206](https://bugzilla.redhat.com/show_bug.cgi?id=1812206) for more information.",
          "reason": "Found login failure due to resource name containing \"/\":<br>\n\n{{for (var pod_detail in pydata.results) { }}\nPod Name: {{=pydata.results[pod_detail][\"pod_name\"]}}\nMessage: {{=pydata.results[pod_detail][\"message\"]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1812206](https://bugzilla.redhat.com/show_bug.cgi?id=1812206) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.machine_pool_check"
      },
      "error_keys": {
        "MACHINE_POOL_NOT_OK": {
          "metadata": {
            "description": "Some Machine Config Pools are 'Degraded' or not 'Updated'",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2020-01-23 08:25:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift"
            ]
          },
          "total_risk": 2,
          "generic": "Some Machine Config Pools are \"Degraded\" or not \"Updated\".\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The following Machine Config Pools are experiencing issues:\n{{~ pydata.machines :mac }}\n* {{=mac[\"name\"]}}\n  {{? mac[\"degraded\"][\"status\"] === true }}* Degraded: {{=mac[\"degraded\"][\"status\"]}}\n    * Message: {{=mac[\"degraded\"][\"message\"]}}\n    * Reason: {{=mac[\"degraded\"][\"reason\"]}}\n  {{?}}{{? mac[\"updated\"][\"status\"] === true }}* Updated : {{=mac[\"updated\"][\"status\"]}}\n    * Message: {{=mac[\"updated\"][\"message\"]}}\n    * Reason: {{=mac[\"updated\"][\"reason\"]}}\n  {{?}}{{? mac[\"render_degraded\"][\"status\"] === true }}* RenderDegraded: {{=mac[\"render_degraded\"][\"status\"]}}\n    * Message: {{=mac[\"render_degraded\"][\"message\"]}}\n    * Reason: {{=mac[\"render_degraded\"][\"reason\"]}}\n  {{?}}{{? mac[\"node_degraded\"][\"status\"] === true }}* NodeDegraded: {{=mac[\"node_degraded\"][\"status\"]}}\n    * Message: {{=mac[\"node_degraded\"][\"message\"]}}\n    * Reason: {{=mac[\"node_degraded\"][\"reason\"]}}{{?}}{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "The following Machine Config Pools are experiencing issues:\n{{~ pydata.machines :mac }}\n* {{=mac[\"name\"]}}\n  {{? mac[\"degraded\"][\"status\"] === true }}* Degraded: {{=mac[\"degraded\"][\"status\"]}}\n    * Message: {{=mac[\"degraded\"][\"message\"]}}\n    * Reason: {{=mac[\"degraded\"][\"reason\"]}}\n  {{?}}{{? mac[\"updated\"][\"status\"] === true }}* Updated : {{=mac[\"updated\"][\"status\"]}}\n    * Message: {{=mac[\"updated\"][\"message\"]}}\n    * Reason: {{=mac[\"updated\"][\"reason\"]}}\n  {{?}}{{? mac[\"render_degraded\"][\"status\"] === true }}* RenderDegraded: {{=mac[\"render_degraded\"][\"status\"]}}\n    * Message: {{=mac[\"render_degraded\"][\"message\"]}}\n    * Reason: {{=mac[\"render_degraded\"][\"reason\"]}}\n  {{?}}{{? mac[\"node_degraded\"][\"status\"] === true }}* NodeDegraded: {{=mac[\"node_degraded\"][\"status\"]}}\n    * Message: {{=mac[\"node_degraded\"][\"message\"]}}\n    * Reason: {{=mac[\"node_degraded\"][\"reason\"]}}{{?}}{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5670841",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.prometheus_mount_nfs_volumes_permission_denied"
      },
      "error_keys": {
        "BUGZILLA_BUG_1911016": {
          "metadata": {
            "description": "Prometheus fails to mount NFS volumes due a bug in RunC",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Prometheus fails to mount NFS volumes due a bug in RunC.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1911016](https://bugzilla.redhat.com/show_bug.cgi?id=1911016) for more information.",
          "reason": "There is a bug in RunC that the user who runs RunC doesn't have access to the directory it attempts<br>\nto chdir, it fails with \"chdir to cwd (%q) set in config.json failed...\". That lead to a mount<br>\nfailure in Prometheus pods.<br>\n\nHere is the error message:<br>\nPod Name    : {{=pydata.result[\"pod_name\"]}}<br>\nMessage     :<br>\n{{=pydata.result[\"message\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1911016](https://bugzilla.redhat.com/show_bug.cgi?id=1911016) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4685861",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.authentication_operator_degraded_route_issue"
      },
      "error_keys": {
        "AUTH_OPERATOR_DEGRADED_ROUTE_ISSUE": {
          "metadata": {
            "description": "Authentication Operator in Unknown state due to route issue",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Authentication Operator in Unknown state due to route issue.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Found degraded authentication operator due to route issue:<br>\n\nOperator Name           : {{=pydata.state[\"name\"]}}<br>\nOperator Degraded State : {{=pydata.state[\"degraded\"][\"status\"]}}<br>\nMessage                 : {{=pydata.state[\"degraded\"][\"message\"]}}<br>\nReason                  : {{=pydata.state[\"degraded\"][\"reason\"]}}<br>\n<br>\nPlease refer to the KCS for detail troubleshooting info.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.security.CVE_2020_8555_kubernetes"
      },
      "error_keys": {
        "CVE_2020_8555_KUBERNETES": {
          "metadata": {
            "description": "CVE-2020-8555: compromised node could escalate to cluster level privileges",
            "impact": 3,
            "likelihood": 2,
            "publish_date": "2021-01-08 12:00:00",
            "status": "active",
            "tags": [
              "security",
              "openshift",
              "cve"
            ]
          },
          "total_risk": 2,
          "generic": "OpenShift versions prior to 4.2.36, 4.3.25, 4.4.8, 4.5.1, or 4.6.1 are vulnerable to CVE-2020-8555: Compromised node could escalate to cluster level privileges.\n\nThis flaw allows an attacker who can intercept requests on a compromised node to redirect those requests, along with their credentials, to perform actions on other endpoints that trust those credentials (including other clusters), allowing for escalation of privileges.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to a more recent version. \n\nFor more information, see the *Updating a cluster within a minor version from the web console* section of the documentation of the OpenShift version:\n\n* [OpenShift {{=pydata.minor_version}}](https://docs.openshift.com/container-platform/{{=pydata.minor_version}}/updating/updating-cluster.html)\n",
          "more_info": "* For more information about the flaw, see [CVE-2020-8555](https://access.redhat.com/security/cve/CVE-2020-8555).\n* The Customer Portal page for the [Red Hat Security Team](https://access.redhat.com/security/) contains more information about policies, procedures, and alerts for Red Hat Products.\n* The Security Team also maintains a frequently updated blog at [securityblog.redhat.com](https://securityblog.redhat.com).\n",
          "reason": "* The cluster is running OpenShift {{=pydata.current_version}}.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to a more recent version. \n\nFor more information, see the *Updating a cluster within a minor version from the web console* section of the documentation of the OpenShift version:\n\n* [OpenShift {{=pydata.minor_version}}](https://docs.openshift.com/container-platform/{{=pydata.minor_version}}/updating/updating-cluster.html)\n",
      "more_info": "* For more information about the flaw, see [CVE-2020-8555](https://access.redhat.com/security/cve/CVE-2020-8555).\n* The Customer Portal page for the [Red Hat Security Team](https://access.redhat.com/security/) contains more information about policies, procedures, and alerts for Red Hat Products.\n* The Security Team also maintains a frequently updated blog at [securityblog.redhat.com](https://securityblog.redhat.com).\n",
      "reason": "* The cluster is running OpenShift {{=pydata.current_version}}.\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4773161",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.mcp_unexpected_on_disk_state_mode_mismatch"
      },
      "error_keys": {
        "MCP_UNEXPECTED_ON_DISK_STATE_MODE_MISMATCH": {
          "metadata": {
            "description": "Upgrade fails when a node is degraded due to \"mode mismatch for file\"",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Upgrade fails when a node is degraded due to \"mode mismatch for file\".",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Found degraded nodes due to \"mode mismatch for file\":<br>\n{{for (var pod in pydata.pods) { }}\nNode: {{=pydata.pods[pod][\"node\"]}}\nPod:  {{=pydata.pods[pod][\"pod\"]}}\n{{=pydata.pods[pod][\"mode_mismatch_message\"]}}\n{{=pydata.pods[pod][\"marking_degraded_message\"]}}\n{{}}}\nThe list above includes only sample messages and may not list all affected nodes/pods. Run<br>\n\n    $ find path/to/archive -name current.log | xargs grep \"mode mismatch for file\"<br>\n\nto find all \"mode mismatch for file\" messages from all pods.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5704671",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.mcp_unexpected_on_disk_state_content_mismatch_resolv_conf"
      },
      "error_keys": {
        "MCP_DEGRADED_DUE_TO_CONTENT_MISMATCH_RESOLV_CONF": {
          "metadata": {
            "description": "Machine Config Pools are degraded due to content mismatch in /etc/resolv.conf",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Machine Config Pools are degraded due to content mismatch in /etc/resolv.conf.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Content of /etc/resolv.conf file does not match MachineConfigPool (MCP) configuration.<br>\nThis file is managed by the NetworkManager on the nodes. A modification using an MCP<br>\ncan result in the degradation of the MCP.<br>\n\nThe following nodes are degraded for this reason:\n{{for (var pod in pydata.pods) { }}\nNode:     {{=pydata.pods[pod][\"node\"]}}<br>\nPod Name: {{=pydata.pods[pod][\"pod\"]}}<br>\n{{=pydata.pods[pod][\"files\"][\"/etc/resolv.conf\"]}}<br>\n{{=pydata.pods[pod][\"message\"]}}<br>\n{{}}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5597461",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.machine_config_controller_streams_line_too_long"
      },
      "error_keys": {
        "MACHINE_CONFIG_CONTROLLER_UNABLE_TO_PARSE_PARAMETER_LINE_TOO_LONG": {
          "metadata": {
            "description": "The rule checks if machine-config-controller streams \"line too long (max 2048 bytes)\" errordue to machine-config-operator unable to render a parameter longer-than-2048-bytesin an ignition template",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The rule checks if machine-config-controller streams \"line too long (max 2048 bytes)\" errordue to machine-config-operator unable to render a parameter longer-than-2048-bytesin an ignition template.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1898169) for more information.",
          "reason": "The machine-config operator is in degraded state<br>\n\nFollowing error message have been found in openshift-machine-config-operator namespace:<br>\n\nPod : {{=pydata.log[0][\"pods\"]}}<br>\n<br>\nMessage: {{=pydata.log[0][\"message\"]}}<br>\n<br>\nIt is possible that there was a failure in rendering a parameter longer-than-2048-bytes<br>\nin ignition template. Currently The MachineConfigOperator has a hard SYSTEMD_LINE_MAX = 2048.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1898169) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5517491",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.mco_degraded_due_to_invalid_character_J"
      },
      "error_keys": {
        "MACHINE_CONFIG_OPERATOR_DEGRADED_DUE_TO_INVALID_CHARACTER_J": {
          "metadata": {
            "description": "Machine Config Operator degraded due to invalid character 'J'",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Machine Config Operator degraded due to invalid character 'J'.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Issue: Truncated journal log on a node cause the Machine Config Operator to go into a Degraded state.<br>\nThe machine-config-daemon pod logs of the node shows \"invalid character 'J' looking for beginning of value\" messages.<br>\n\nMachine Config Operator Degraded Status : {{=pydata.state[\"degraded\"][\"status\"]}}<br>\n<br>\nFound following logs:<br>\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage: {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5428871",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.hpa_fails_getting_cpu_metrics"
      },
      "error_keys": {
        "HPA_FAILS_GETTING_CPU_METRICS": {
          "metadata": {
            "description": "Horizontal Pod Autoscaler (HPA) fails getting CPU consumption",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Horizontal Pod Autoscaler (HPA) fails getting CPU consumption.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Horizontal Pod Autoscaler (HPA) fails getting CPU consumption: \"the HPA was unable to compute the replica count\"",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5428951",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.mcd_logs_no_such_host_for_localhost"
      },
      "error_keys": {
        "MCD_LOGS_NO_SUCH_HOST_FOR_LOCALHOST": {
          "metadata": {
            "description": "Machine-config-deamon pod logs messages like 'no such host' for localhost",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Machine-config-deamon pod logs messages like 'no such host' for localhost.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Found 'no such host' log for localhost:<br>\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage: {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.openshift_sdn_egress_ip_in_no_hostsubnet"
      },
      "error_keys": {
        "OPENSHIFT_SDN_EGRESS_IP_IN_NO_HOSTSUBNET": {
          "metadata": {
            "description": "The cluster drops outgoing traffic from a project when its egress IP address is not assigned to any node",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-02-09 14:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "configuration",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "The cluster drops outgoing traffic from a project when its egress IP address is not assigned to any node.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you fix the configuration for {{?pydata.no_hostsubnet.length>1}}these egress IP addresses{{??}}this egress IP address{{?}}:\n\n{{~ pydata.no_hostsubnet :entry }}\n- NetNamespace '{{=entry['netnamespace']}}'\n  {{~ entry['egress_ips'] : egress_ip }}- {{=egress_ip}}\n  {{~}}\n{{~}}\n\nThe configuration for each egress IP address can be fixed in one of these two ways:\n- If the NetNamespace requests the intended egress IP address, make sure that HostSubnets are configured to host that egress IP address.\n- If the NetNamespace requests an incorrect egress IP address, change it to an egress IP address that a HostSubnet is configured for.\n\nSee [Configuring egress IPs for a project](https://docs.openshift.com/container-platform/{{=pydata.ocp_version}}/networking/openshift_sdn/assigning-egress-ips.html) for more information about configuring egress IPs.\n",
          "more_info": "",
          "reason": "{{?pydata.no_hostsubnet.length>1\n}}These NetNamespaces request egress IP addresses that are not specified in any HostSubnet{{?? pydata.no_hostsubnet[0]['egress_ips'].length>1\n}}This NetNamespace requests egress IP addresses that are not specified in any HostSubnet{{??\n}}This NetNamespace requests an egress IP address that is not specified in any HostSubnet{{?}}:\n\n{{~ pydata.no_hostsubnet :entry }}\n- NetNamespace '{{=entry['netnamespace']}}'\n  {{~ entry['egress_ips'] : egress_ip }}- {{=egress_ip}}\n  {{~}}\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you fix the configuration for {{?pydata.no_hostsubnet.length>1}}these egress IP addresses{{??}}this egress IP address{{?}}:\n\n{{~ pydata.no_hostsubnet :entry }}\n- NetNamespace '{{=entry['netnamespace']}}'\n  {{~ entry['egress_ips'] : egress_ip }}- {{=egress_ip}}\n  {{~}}\n{{~}}\n\nThe configuration for each egress IP address can be fixed in one of these two ways:\n- If the NetNamespace requests the intended egress IP address, make sure that HostSubnets are configured to host that egress IP address.\n- If the NetNamespace requests an incorrect egress IP address, change it to an egress IP address that a HostSubnet is configured for.\n\nSee [Configuring egress IPs for a project](https://docs.openshift.com/container-platform/{{=pydata.ocp_version}}/networking/openshift_sdn/assigning-egress-ips.html) for more information about configuring egress IPs.\n",
      "more_info": "",
      "reason": "{{?pydata.no_hostsubnet.length>1\n}}These NetNamespaces request egress IP addresses that are not specified in any HostSubnet{{?? pydata.no_hostsubnet[0]['egress_ips'].length>1\n}}This NetNamespace requests egress IP addresses that are not specified in any HostSubnet{{??\n}}This NetNamespace requests an egress IP address that is not specified in any HostSubnet{{?}}:\n\n{{~ pydata.no_hostsubnet :entry }}\n- NetNamespace '{{=entry['netnamespace']}}'\n  {{~ entry['egress_ips'] : egress_ip }}- {{=egress_ip}}\n  {{~}}\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.kube_api_too_large_resource_version"
      },
      "error_keys": {
        "KUBE_API_TOO_LARGE_RESOURCE_VERSION": {
          "metadata": {
            "description": "KubeAPIErrorsHigh alert firing due to \"too large resource version\"",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "KubeAPIErrorsHigh alert firing due to \"too large resource version\".",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1877346](https://bugzilla.redhat.com/show_bug.cgi?id=1877346) for more information.",
          "reason": "KubeAPIErrorsHigh alert firing due to \"too large resource version\".<br>\n\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage : {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1877346](https://bugzilla.redhat.com/show_bug.cgi?id=1877346) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5362041",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.node_exporter_fails_parsing_mountstat_with_invalid_nfs_operation_stats"
      },
      "error_keys": {
        "BUGZILLA_BUG_1870261": {
          "metadata": {
            "description": "This rule will check if Node exporter floods with logs like\"ERROR:mountstats collector failed after 0.000539s:failedto parse mountstats:invalid NFS per-operations stats:[NULL:1 1 0 44 24 2 0 3 0]\"  source=\"collector.go:132\"",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule will check if Node exporter floods with logs like\"ERROR: mountstats collector failed after 0.000539s: failedto parse mountstats: invalid NFS per-operations stats:[NULL: 1 1 0 44 24 2 0 3 0]\"  source=\"collector.go:132\".",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1870261](https://bugzilla.redhat.com/show_bug.cgi?id=1870261) for more information.",
          "reason": "Possible known bug found.<br>\nSummary     : node-exporter pods show \"failed to parse mountstats: invalid NFS per-operations stats\" message.<br>\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage : {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1870261](https://bugzilla.redhat.com/show_bug.cgi?id=1870261) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5846071",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.nfs_version_mismatch"
      },
      "error_keys": {
        "NFS_VERSION_MISSMATCH": {
          "metadata": {
            "description": "Pods stuck in pending status when the nfs PVs mount failed for nfs version mismatch",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Pods stuck in pending status with error 'mount.nfs: Protocol family not supported'.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you to create nfs PVs with version mount options:\n  \n1. Example PV yaml file\n~~~\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-pv\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: slow\n  mountOptions:\n    - nfsvers=3        # Replace the supported NFS version which is offered by the NFS server. \n  nfs:\n    path: /tmp\n    server: 10.X.X.\n~~~\n**Note:** In OCP 4, only NFS v4 has been tested\n",
          "more_info": "",
          "reason": "Following pods stuck in the Pending status due to the nfs volumes mount failed with mismatch version:\n\n{{for (var key in pydata.results) { }}\n**Pod:** {{=pydata.results[key]['pod']}}<br>\n**Message:** {{=pydata.results[key]['message']}}\n{{}}}\n**Note:**  By default, Pods in OCP4 try to mount nfs volumes with NFSv4\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you to create nfs PVs with version mount options:\n  \n1. Example PV yaml file\n~~~\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: nfs-pv\nspec:\n  capacity:\n    storage: 5Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: slow\n  mountOptions:\n    - nfsvers=3        # Replace the supported NFS version which is offered by the NFS server. \n  nfs:\n    path: /tmp\n    server: 10.X.X.\n~~~\n**Note:** In OCP 4, only NFS v4 has been tested\n",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.mcp_unexpected_on_disk_state_content_mismatch"
      },
      "error_keys": {
        "MCP_UNEXPECTED_ON_DISK_STATE_CONTENT_MISMATCH": {
          "metadata": {
            "description": "Upgrade failed when there are mismatch files existing",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2020-07-10 12:00:00",
            "status": "active",
            "tags": null
          },
          "total_risk": 2,
          "generic": "When the machine-config-operator detects that a file on a node file system was modified externally (e.g manually running commands inside the node), it marks the node as Degraded. When this happens during an upgrade, the upgrade fails.\n",
          "summary": "",
          "resolution": "1. Check the mismatch files, copy the original file from a node that is working and make sure they have the same checksum.\n\n   ~~~\n   {{~pydata.files:item}} {{=item}} {{~}}\n   ~~~\n\n1. Wait for the node to be Available again.\n\n1. Check if the machine-config-operator is working again.\n",
          "more_info": "",
          "reason": "Found machines upgrade failed due to mismatch file:\n\n{{~pydata.machines:item}}\n**Name:** {{=item[\"name\"]}}\n- *Reason:* {{=item[\"reason\"]}}\n- *Message:* {{=item[\"message\"]}}\n- *Last_trans_time:* {{=item[\"last_trans_time\"]}}\n{{~}}\n\n**Mismatch file**:\n{{~pydata.files:item}}\n\n{{=item}}\n{{~}}\n\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "1. Check the mismatch files, copy the original file from a node that is working and make sure they have the same checksum.\n\n   ~~~\n   {{~pydata.files:item}} {{=item}} {{~}}\n   ~~~\n\n1. Wait for the node to be Available again.\n\n1. Check if the machine-config-operator is working again.\n",
      "more_info": "",
      "reason": "Found machines upgrade failed due to mismatch file:\n\n{{~pydata.machines:item}}\n**Name:** {{=item[\"name\"]}}\n- *Reason:* {{=item[\"reason\"]}}\n- *Message:* {{=item[\"message\"]}}\n- *Last_trans_time:* {{=item[\"last_trans_time\"]}}\n{{~}}\n\n**Mismatch file**:\n{{~pydata.files:item}}\n\n{{=item}}\n{{~}}\n\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5344791",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.authentication_operator_degraded_due_to_missingendpoints"
      },
      "error_keys": {
        "AUTH_OPERATOR_DEGRADED_DUE_TO_MISSINGENDPOINTS": {
          "metadata": {
            "description": "Authentication Operator in degraded state due to missing end points",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Authentication Operator in degraded state due to missing end points.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Authentication operator degraded due to missing end points.<br>\n\nOperator Name           : {{=pydata.state[\"name\"]}}<br>\nOperator Degraded State : {{=pydata.state[\"degraded\"][\"status\"]}}<br>\nOperator message        : {{=pydata.state[\"degraded\"][\"message\"]}}<br>\nReason                  : {{=pydata.state[\"degraded\"][\"reason\"]}}<br>\nLast Transition         : {{=pydata.state[\"degraded\"][\"last_trans_time\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6205892",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.azure_ssh_not_work"
      },
      "error_keys": {
        "AZURE_SSH_NOT_WORK": {
          "metadata": {
            "description": "Cannot SSH into RHCOS nodes on OpenShift clusters on Azure due to an OpenShift bug",
            "impact": 2,
            "likelihood": 4,
            "publish_date": "2021-08-06 16:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "openshift",
              "azure",
              "ssh"
            ]
          },
          "total_risk": 3,
          "generic": "Cannot SSH into RHCOS nodes on OpenShift clusters on Azure due to an OpenShift bug.",
          "summary": "",
          "resolution": "Red Hat recommends that you use the following workaround to ssh into RHCOS nodes.\n\n~~~\n$ ssh core@<node ip> bash -i\n~~~\n\n",
          "more_info": "For more in-depth information please check the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1984449).",
          "reason": "This OpenShift cluster is running in the version **{{=pydata.version}}** on Microsoft Azure. Due to a bug in this version, when trying to ssh to any of the OpenShift RHCOS nodes on Microsoft Azure, the ssh session hangs.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you use the following workaround to ssh into RHCOS nodes.\n\n~~~\n$ ssh core@<node ip> bash -i\n~~~\n\n",
      "more_info": "For more in-depth information please check the [Bugzilla](https://bugzilla.redhat.com/show_bug.cgi?id=1984449).",
      "reason": "This OpenShift cluster is running in the version **{{=pydata.version}}** on Microsoft Azure. Due to a bug in this version, when trying to ssh to any of the OpenShift RHCOS nodes on Microsoft Azure, the ssh session hangs.",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4542531",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.auth_operator_degraded_on_replacing_ingress_certificate"
      },
      "error_keys": {
        "AUTHENTICATION_OPERATOR_DEGRADED_WITH_ERROR_ROUTERCERTSDEGRADED": {
          "metadata": {
            "description": "Authentication operator found to be in a degraded state \"/\"",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Authentication operator found to be in a degraded state \"/\".",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Authentication operator degraded on replacing ingress certificate.<br>\n\nOperator name  : {{=pydata.operator[\"name\"]}}<br>\nDegraded status: {{=pydata.operator[\"degraded\"][\"status\"]}}<br>\nMessage        : {{=pydata.operator[\"degraded\"][\"message\"]}}<br>\nReason         : {{=pydata.operator[\"degraded\"][\"reason\"]}}<br>\nLast Transition: {{=pydata.operator[\"degraded\"][\"last_trans_time\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6137972",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.CSI_resultServer_pods_schedule_issue"
      },
      "error_keys": {
        "ERROR_CSI_RESULTSERVER_PODS_ISSUE": {
          "metadata": {
            "description": "Check the CSI resultServer pod issue",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Check the CSI resultServer pod issue.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The CSI resultServer pods are are in \"Not Ready\" and \"Initializing\" state.<br>\nThe following errors are found in kubelet service:\n{{for (var msg in pydata.results) { }}\nMessage: {{=pydata.results[msg]}}\n{{}}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5682881",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.cloud_credential_operator_s3_not_found"
      },
      "error_keys": {
        "BUGZILLA_BUG_1911812": {
          "metadata": {
            "description": "cloud-credentials operator is degraded due to missing operator S3 creds",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "cloud-credentials operator is degraded due to missing operator S3 creds.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1911812](https://bugzilla.redhat.com/show_bug.cgi?id=1911812) for more information.",
          "reason": "Possible known bug found.<br>\nSummary     : cloud-credential operator degraded due to missing cloud-credential-operator-s3 credentials.<br>\n\nOperator Details:<br>\nOperator Name            : {{=pydata.operator[\"name\"]}}<br>\nOperator Degaraded state : {{=pydata.operator[\"degraded\"][\"status\"]}}<br>\nMessage                  : {{=pydata.operator[\"degraded\"][\"message\"]}}<br>\nReason                   : {{=pydata.operator[\"degraded\"][\"reason\"]}}<br>\n{{for (var pod in pydata.results) { }}\nPod Name : {{=pydata.results[pod][\"pod_name\"]}}\nMessage  : {{=pydata.results[pod][\"message\"]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1911812](https://bugzilla.redhat.com/show_bug.cgi?id=1911812) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4787391",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.invalid_url_scheme_for_ignition_config"
      },
      "error_keys": {
        "IGNITION_SPECIFICATION_HAVE_A_MALFORMED_URL_WHICH_CANNOT_BE_PARSED": {
          "metadata": {
            "description": "This rule checks if Node degraded due to invalid url scheme used for Ignition config",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule checks if Node degraded due to invalid url scheme used for Ignition config.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The machine config operator in degraded state<br>\n\nThe following error message has been found<br>\n{{for (var item in pydata.mcp_list) { }}\nMachineConfigPool Name : {{=pydata.mcp_list[item][\"name\"]}}\n\nError message Found > {{=pydata.mcp_list[item][\"error\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4455731",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.auth_operator_unknown_state_due_to_route"
      },
      "error_keys": {
        "AUTH_OPERATOR_DEGRADED_DUE_TO_ROUTE_HEALTH_DEGRADED": {
          "metadata": {
            "description": "This rule checks Authentication clusteroperator is in'Unknown State' causing Console clusteroperatorto fail because of the route for openshift-authentication not being accessible",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule checks Authentication clusteroperator is in'Unknown State' causing Console clusteroperatorto fail because of the route for openshift-authentication not being accessible.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Operator Name      : {{=pydata.op[\"name\"]}}<br>\nDegraded Status    : {{=pydata.op[\"degraded\"][\"status\"]}}<br>\nProgressing Status : Unknown<br>\nAvailable Status   : Unknown<br>\n{{for (var pod_detail in pydata.results) { }}\n\nPod Name: {{=pydata.results[pod_detail][\"pod_name\"]}}\n\nMessage: {{=pydata.results[pod_detail][\"message\"]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5203491",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.cmo_degraded_due_to_invalid_conf"
      },
      "error_keys": {
        "CLUSTER_MONITORING_OPERATOR_DEGRADED_DUE_TO_INVALID_CONFIGURATION_OF_CONFIGMAP": {
          "metadata": {
            "description": "This rule will check if the ClusterMonitoring Operator is degradeddue to invalid configuration in cluster-monitoring config map",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule will check if the ClusterMonitoring Operator is degradeddue to invalid configuration in cluster-monitoring config map.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Docs](https://docs.openshift.com/container-platform/4.3/monitoring/cluster_monitoring/configuring-the-monitoring-stack.html) for more information.",
          "reason": "Cluster Monitoring Operator Degraded   : {{=pydata.status[\"degraded\"][\"status\"]}}<br>\n<br>\nReason                                 : {{=pydata.status[\"degraded\"][\"reason\"]}}<br>\n<br>\nFollowing error message has been found :<br>\n<br>\n{{=pydata.status[\"degraded\"][\"message\"]}}<br>\n<br>\nThis issue is dues an invalid configuration in the Cluster Monitoring Configmap.<br>\nPlease Check the Cluster Monitoring Configmap and validate that all fields are valid reading the<br>\ndocumentation mentioned",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Docs](https://docs.openshift.com/container-platform/4.3/monitoring/cluster_monitoring/configuring-the-monitoring-stack.html) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5675821",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.fluentd_buffer_queue_length_burst"
      },
      "error_keys": {
        "ERROR_FLUENTD_BUFFER_QUEUE_LENGTH_BURST": {
          "metadata": {
            "description": "Log collector is experiencing buffer queue length burst on RHOCP 4.6",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Log collector is experiencing buffer queue length burst on RHOCP 4.6.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bz](https://bugzilla.redhat.com/show_bug.cgi?id=1891623) for more information.",
          "reason": "FluentdQueueLengthBurst alerts detected.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bz](https://bugzilla.redhat.com/show_bug.cgi?id=1891623) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5906281",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.insights_operator_degraded_due_to_source_config_could_not_be_retrieved"
      },
      "error_keys": {
        "INSIGHTS_CLUSTER_OPERATOR_DEGRADED_DUE_TO_SOURCE_CONFIG_COULD_NOT_BE_RETRIEVED": {
          "metadata": {
            "description": "Insights Operator is degraded because it cannot gather pod logs",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The Insights Operator tries to grab logs from containers in the cluster. The operator is\nmarked degraded when the Insights Operator pod is not able to gather those logs during its\ninitialization. If this happens during a cluster upgrade, the upgrade will be stuck.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The Insights Operator is Degraded.\n{{~ pydata.results:data }}\n* **Degraded Status:** {{=data[\"op_status\"]}}\n* **Degraded Message:**<br/>\n  `{{=data[\"op_message\"]}}``\n{{~}}\n\n\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5175271",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.coredns_panic_without_probe_failures"
      },
      "error_keys": {
        "BUGZILLA_BUG_1869310": {
          "metadata": {
            "description": "CoreDNS pods observing Panic with no Probe failure messages",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "CoreDNS pods observing Panic with no Probe failure messages.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1869310](https://bugzilla.redhat.com/show_bug.cgi?id=1869310) for more information.",
          "reason": "Possible known Bug Found.<br>\nSummary     : The following CoreDNS pods are observing panics.<br>\n{{for (var pod_detail in pydata.results) { }}\n\nPod Name: {{=pydata.results[pod_detail][\"pod_name\"]}}\nMessage: {{=pydata.results[pod_detail][\"message\"]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1869310](https://bugzilla.redhat.com/show_bug.cgi?id=1869310) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5875621",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.service_ca_operator_stuck_for_scc"
      },
      "error_keys": {
        "ERROR_SERVICE_CA_OPERATOR_STUCK_FOR_SCC": {
          "metadata": {
            "description": "The service-ca operator is stuck in progressing for the restricted SCC",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The service-ca operator is stuck in progressing for the restricted SCC.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "The service-ca operator is stuck in progressing for the restricted SCC, any other SCC<br>\nbeing applied to it will cause the CreateContainerConfigError issue. The monitoring<br>\ncluster operator is degraded due to the issue with trusting of internal service-serving<br>\ncertificates caused by the unavailable service-ca pod.<br>\n\nFollowing error logs are detected:<br>\n{{for (var log in pydata.logs) { }}\n{{=pydata.logs[log][\"name\"]}}\nMessage     :<br>\n    {{=pydata.logs[log][\"msg\"]}}\nReason      :<br>\n    {{=pydata.logs[log][\"reason\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5116561",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.pod_creation_failures_due_to_sdn_segfaults"
      },
      "error_keys": {
        "BUGZILLA_BUG_1913411": {
          "metadata": {
            "description": "When openshift-sdn segfaults, pod creation fails with error 'reserving pod name ... name is reserved'",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "When openshift-sdn segfaults, pod creation fails with 'error reserving pod name ...: name is reserved'.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Possible known Bug Found.<br>\nSummary     : Due to openshift-sdn segfaults, pod creation fails with 'error reserving pod name ...: name is reserved'<br>\n{{for (var pod_detail in pydata.result) { }}\n\nPod Creation failures:<br>\n{{=pydata.result[pod_detail][\"message\"][0]}}<br>\n\nNetPlugin failures:<br>\n{{=pydata.result[pod_detail][\"message\"][1]}}<br>\n{{}}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4967301",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.machine_config_controller_degraded_during_upgrade"
      },
      "error_keys": {
        "MACHINE_CONFIG_CONTROLLER_OPERATOR_DEGRADED_DURING_UPGRADE": {
          "metadata": {
            "description": "Rule to check if the Machine-config controller operator degraded whileupgrading the cluster",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Rule to check if the Machine-config controller operator degraded whileupgrading the cluster.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [KCS 4857671](https://access.redhat.com/solutions/4857671) for more information.",
          "reason": "Machine-config controller operator is degraded with error message<br>\n\nOperator name  : {{=pydata.operator[\"name\"]}}<br>\nDegraded status: {{=pydata.operator[\"degraded\"][\"status\"]}}<br>\nMessage        : {{=pydata.operator[\"degraded\"][\"message\"]}}<br>\nReason         : {{=pydata.operator[\"degraded\"][\"reason\"]}}<br>\nLast Transition: {{=pydata.operator[\"degraded\"][\"last_trans_time\"]}}<br>\n<br>\n{{for (var pod in pydata.results) { }}\nPod Name: {{=pydata.results[pod][\"pod_name\"]}}\nMessage: {{=pydata.results[pod][\"message\"]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [KCS 4857671](https://access.redhat.com/solutions/4857671) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.master_nodes_are_schedulable"
      },
      "error_keys": {
        "MASTER_NODES_ARE_SCHEDULABLE_FAIL": {
          "metadata": {
            "description": "Some master nodes are schedulable",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Some master nodes are schedulable. The master nodes have the `node-role.kubernetes.io/worker` label.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "\n{{? pydata.schedulable_count == 0 }}\nNo master node is schedulable.\n\nThe master nodes do not have the `node-role.kubernetes.io/worker` label.\n\n{{?? pydata.schedulable_count == 1 }}\n1 master node is schedulable.\n\nThe master node has the `node-role.kubernetes.io/worker` label.\n\n{{??}}\n{{=pydata.schedulable_count}} master nodes are schedulable.\n\nThe master nodes have the `node-role.kubernetes.io/worker` label.\n{{?}}\n\n<table>\n  <tr>\n    <th>Master Node&nbsp;&nbsp;</th>\n    <th>Schedulable&nbsp;&nbsp;</th>\n    {{? pydata.other_roles }}<th>Other Roles</th>{{?}}\n  </tr>\n{{~ pydata.nodes :node}}\n<tr>\n    <td>{{=node[\"name\"]}}&nbsp;&nbsp;</td>\n    <td>{{? node[\"schedulable\"] }}YES{{??}}NO{{?}}&nbsp;&nbsp;</td>{{? pydata.other_roles }}\n    <td>{{for (index in node.other_roles) { }}{{? index == 0 }}{{=node.other_roles[index]}}{{??}},{{=node.other_roles[index]}}{{?}}{{ } }}</td>{{?}}\n</tr>\n{{~}}\n</table>\n",
          "HasReason": true
        },
        "MASTER_NODES_ARE_SCHEDULABLE_PASS": {
          "metadata": {
            "description": "No master node is schedulable",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "No master node is schedulable. The master nodes do not have the `node-role.kubernetes.io/worker` label.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "\n{{? pydata.schedulable_count == 0 }}\nNo master node is schedulable.\n\nThe master nodes do not have the `node-role.kubernetes.io/worker` label.\n\n{{?? pydata.schedulable_count == 1 }}\n1 master node is schedulable.\n\nThe master node has the `node-role.kubernetes.io/worker` label.\n\n{{??}}\n{{=pydata.schedulable_count}} master nodes are schedulable.\n\nThe master nodes have the `node-role.kubernetes.io/worker` label.\n{{?}}\n\n<table>\n  <tr>\n    <th>Master Node&nbsp;&nbsp;</th>\n    <th>Schedulable&nbsp;&nbsp;</th>\n    {{? pydata.other_roles }}<th>Other Roles</th>{{?}}\n  </tr>\n{{~ pydata.nodes :node}}\n<tr>\n    <td>{{=node[\"name\"]}}&nbsp;&nbsp;</td>\n    <td>{{? node[\"schedulable\"] }}YES{{??}}NO{{?}}&nbsp;&nbsp;</td>{{? pydata.other_roles }}\n    <td>{{for (index in node.other_roles) { }}{{? index == 0 }}{{=node.other_roles[index]}}{{??}},{{=node.other_roles[index]}}{{?}}{{ } }}</td>{{?}}\n</tr>\n{{~}}\n</table>\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "\n{{? pydata.schedulable_count == 0 }}\nNo master node is schedulable.\n\nThe master nodes do not have the `node-role.kubernetes.io/worker` label.\n\n{{?? pydata.schedulable_count == 1 }}\n1 master node is schedulable.\n\nThe master node has the `node-role.kubernetes.io/worker` label.\n\n{{??}}\n{{=pydata.schedulable_count}} master nodes are schedulable.\n\nThe master nodes have the `node-role.kubernetes.io/worker` label.\n{{?}}\n\n<table>\n  <tr>\n    <th>Master Node&nbsp;&nbsp;</th>\n    <th>Schedulable&nbsp;&nbsp;</th>\n    {{? pydata.other_roles }}<th>Other Roles</th>{{?}}\n  </tr>\n{{~ pydata.nodes :node}}\n<tr>\n    <td>{{=node[\"name\"]}}&nbsp;&nbsp;</td>\n    <td>{{? node[\"schedulable\"] }}YES{{??}}NO{{?}}&nbsp;&nbsp;</td>{{? pydata.other_roles }}\n    <td>{{for (index in node.other_roles) { }}{{? index == 0 }}{{=node.other_roles[index]}}{{??}},{{=node.other_roles[index]}}{{?}}{{ } }}</td>{{?}}\n</tr>\n{{~}}\n</table>\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.node_ip_does_not_match_machine_network"
      },
      "error_keys": {
        "NODE_IP_ADDRESSES_NOT_IN_MACHINE_NETWORK_FAIL": {
          "metadata": {
            "description": "Internal IP addresses of some nodes do NOT match the Machine Network",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Internal IP addresses of some nodes do NOT match the Machine Network.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "\n{{? pydata.mismatch_ip_count }}\nThe total of {{=pydata.mismatch_ip_count}} internal IP {{?\n    pydata.mismatch_ip_count == 1 }}address{{??\n    }}addresses{{?\n}} on {{=pydata.mismatch_node_count}} {{?\n   pydata.mismatch_node_count == 1 }}node{{??\n   }}nodes{{?\n}} {{?\n    pydata.mismatch_ip_count == 1 }}DOES NOT{{??\n    }}DO NOT{{?\n}} match the Machine Network.\n{{??}}\nAll internal IP addresses of all nodes match the Machine Network.\n{{?}}\n\n**Machine Network:** <br/>\n{{=pydata.cidr}}\n\n<table>\n  <tr>\n    <th style=\"text-align:center;\">Node</th>\n    <th style=\"text-align:center;\">IP Address</th>\n    <th style=\"text-align:center;\">Matches CIDR</th>\n  </tr>\n{{~ pydata.nodes :node}}\n{{ for ( var index in node.ip ) { }}\n{{? index == 0 }}\n<tr>\n    <td valign=\"top\" rowspan={{=node.ip.length}}>{{=node[\"name\"]}}&nbsp;&nbsp; </td>\n    <td>{{=node.ip[index][0]}}&nbsp;&nbsp; </td>\n    <td>{{? node.ip[index][1]}}YES{{??}}NO{{?}} </td>\n</tr>\n{{??}}\n<tr>\n    <td>{{=node.ip[index][0]}}&nbsp;&nbsp; </td>\n    <td>{{? node.ip[index][1]}}YES{{??}}NO{{?}}&nbsp;&nbsp; </td>\n</tr>\n{{?}}\n{{ } }}\n{{~}}\n</table>\n",
          "HasReason": true
        },
        "NODE_IP_ADDRESSES_NOT_IN_MACHINE_NETWORK_PASS": {
          "metadata": {
            "description": "All internal IP addresses of all nodes match the Machine Network",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "All internal IP addresses of all nodes match the Machine Network.\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "\n{{? pydata.mismatch_ip_count }}\nThe total of {{=pydata.mismatch_ip_count}} internal IP {{?\n    pydata.mismatch_ip_count == 1 }}address{{??\n    }}addresses{{?\n}} on {{=pydata.mismatch_node_count}} {{?\n   pydata.mismatch_node_count == 1 }}node{{??\n   }}nodes{{?\n}} {{?\n    pydata.mismatch_ip_count == 1 }}DOES NOT{{??\n    }}DO NOT{{?\n}} match the Machine Network.\n{{??}}\nAll internal IP addresses of all nodes match the Machine Network.\n{{?}}\n\n**Machine Network:** <br/>\n{{=pydata.cidr}}\n\n<table>\n  <tr>\n    <th style=\"text-align:center;\">Node</th>\n    <th style=\"text-align:center;\">IP Address</th>\n    <th style=\"text-align:center;\">Matches CIDR</th>\n  </tr>\n{{~ pydata.nodes :node}}\n{{ for ( var index in node.ip ) { }}\n{{? index == 0 }}\n<tr>\n    <td valign=\"top\" rowspan={{=node.ip.length}}>{{=node[\"name\"]}}&nbsp;&nbsp; </td>\n    <td>{{=node.ip[index][0]}}&nbsp;&nbsp; </td>\n    <td>{{? node.ip[index][1]}}YES{{??}}NO{{?}} </td>\n</tr>\n{{??}}\n<tr>\n    <td>{{=node.ip[index][0]}}&nbsp;&nbsp; </td>\n    <td>{{? node.ip[index][1]}}YES{{??}}NO{{?}}&nbsp;&nbsp; </td>\n</tr>\n{{?}}\n{{ } }}\n{{~}}\n</table>\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "\n{{? pydata.mismatch_ip_count }}\nThe total of {{=pydata.mismatch_ip_count}} internal IP {{?\n    pydata.mismatch_ip_count == 1 }}address{{??\n    }}addresses{{?\n}} on {{=pydata.mismatch_node_count}} {{?\n   pydata.mismatch_node_count == 1 }}node{{??\n   }}nodes{{?\n}} {{?\n    pydata.mismatch_ip_count == 1 }}DOES NOT{{??\n    }}DO NOT{{?\n}} match the Machine Network.\n{{??}}\nAll internal IP addresses of all nodes match the Machine Network.\n{{?}}\n\n**Machine Network:** <br/>\n{{=pydata.cidr}}\n\n<table>\n  <tr>\n    <th style=\"text-align:center;\">Node</th>\n    <th style=\"text-align:center;\">IP Address</th>\n    <th style=\"text-align:center;\">Matches CIDR</th>\n  </tr>\n{{~ pydata.nodes :node}}\n{{ for ( var index in node.ip ) { }}\n{{? index == 0 }}\n<tr>\n    <td valign=\"top\" rowspan={{=node.ip.length}}>{{=node[\"name\"]}}&nbsp;&nbsp; </td>\n    <td>{{=node.ip[index][0]}}&nbsp;&nbsp; </td>\n    <td>{{? node.ip[index][1]}}YES{{??}}NO{{?}} </td>\n</tr>\n{{??}}\n<tr>\n    <td>{{=node.ip[index][0]}}&nbsp;&nbsp; </td>\n    <td>{{? node.ip[index][1]}}YES{{??}}NO{{?}}&nbsp;&nbsp; </td>\n</tr>\n{{?}}\n{{ } }}\n{{~}}\n</table>\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5483591",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.nil_pointer_dereference_panic_in_mcc_or_mco"
      },
      "error_keys": {
        "PANIC_DUE_TO_NIL_POINTER_DEREFERENCE": {
          "metadata": {
            "description": "This rule checks if machine-config-operator or machine-config-controllerpods has panic due to \"invalid memory address or nil pointer dereference\"",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "This rule checks if machine-config-operator or machine-config-controllerpods has panic due to \"invalid memory address or nil pointer dereference\".",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the following links for more information:\n* [Bugzilla 1886636](https://bugzilla.redhat.com/show_bug.cgi?id=1886636)\n* [Bugzilla 1858026](https://bugzilla.redhat.com/show_bug.cgi?id=1858026)\n",
          "reason": "Possible bug found.<br>\nSummary     : Observed a panic: \"invalid memory address or nil pointer dereference\" (runtime error: invalid memory address or nil pointer dereference)<br>\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage: {{=pydata.results[pod_data][\"message\"]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the following links for more information:\n* [Bugzilla 1886636](https://bugzilla.redhat.com/show_bug.cgi?id=1886636)\n* [Bugzilla 1858026](https://bugzilla.redhat.com/show_bug.cgi?id=1858026)\n",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.version_retarget"
      },
      "error_keys": {
        "ABORTED_UPDATES_IN_RECENT_HISTORY": {
          "metadata": {
            "description": "Support may get problematic if the cluster state is a result of an aborted update",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2020-02-20 15:25:00",
            "status": "active",
            "tags": null
          },
          "total_risk": 2,
          "generic": "There have been *aborted updates* since the last successfully completed update.\n\nAn update attempt is considered aborted if its entry in the cluster version history\n    - has 'Partial' state\n    - has defined 'completionTime'\n",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "There have been aborted updates since the last successfully completed update:\n\n{{~ pydata.aborted_updates :entry }}\n    Version '{{=entry[\"version\"]}}'\n    - State          : {{=entry[\"state\"]}}\n    - Started time   : {{=entry[\"started_time\"]}}\n    - Completion time: {{=entry[\"completion_time\"]}}\n    - Verified       : {{=entry[\"verified\"]}}\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "There have been aborted updates since the last successfully completed update:\n\n{{~ pydata.aborted_updates :entry }}\n    Version '{{=entry[\"version\"]}}'\n    - State          : {{=entry[\"state\"]}}\n    - Started time   : {{=entry[\"started_time\"]}}\n    - Completion time: {{=entry[\"completion_time\"]}}\n    - Verified       : {{=entry[\"verified\"]}}\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.vsphere_upi_machine_is_in_phase"
      },
      "error_keys": {
        "VSPHERE_UPI_MACHINE_WITH_NO_RUNNING_PHASE": {
          "metadata": {
            "description": "OpenShift cluster reports 'MachineWithNoRunningPhase' alarms when invalid control plane Machines and compute MachineSets are created during installation on vSphere with user provided infrastructure (UPI)",
            "impact": 1,
            "likelihood": 3,
            "publish_date": "2020-12-03 12:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "fault_tolerance"
            ]
          },
          "total_risk": 2,
          "generic": "The OpenShift installer creates invalid objects when the control plane Machine and compute MachineSet manifests are not deleted during installation on vSphere using user provided infrastructure (UPI). The invalid objects trigger the 'MachineWithNoRunningPhase' alarms.\n",
          "summary": "",
          "resolution": "**Option 1:** If feasible, Red Hat recommends that you redeploy the cluster with the folowing modification of the installation procedure:\n\n1. Generate the Kubernetes manifests as instructed by the documentation:\n~~~\n$ openshift-install create manifests --dir=<installation_directory>\n~~~\n\n2. *NEW:* Delete the Machine and MachineSet manifests.\n~~~\n$ cd <installation_directory>\n$ find . -name '*machineset*' -o -name '*master-machine*'\n$ rm -f openshift/99_openshift-cluster-api_master-machines-*.yaml openshift/99_openshift-cluster-api_worker-machineset-*.yaml\n~~~\n\n3. Continue with creating the ignition files as instructed by the documentation:\n~~~\n$ openshift-install create ignition-configs --dir=<installation_directory>\n~~~\n\nSee also the [Remove Machines and MachineSets](https://github.com/openshift/installer/blob/master/docs/user/vsphere/install_upi.md#remove-machines-and-machinesets) step in the OpenShift installer documentation for installation on vSphere using user provided infrastructure (UPI).\n\n**Option 2:** If redeploying the cluster is not feasible, RedHat recommends that you remove the invalid control plane Machine and compute MachineSet objects as a workaround.\n\n*This workaround works only for OpenShift 4.4. Do not apply this workaround on OpenShift 4.5 and higher.*\n\n~~~\n$ oc delete machine unnecessary_machine_name_xxx\n$ oc delete machinesets unnecessary_machinesets_name_xxx\n~~~\n",
          "more_info": "",
          "reason": "Starting with OpenShift 4.4, the OpenShift installer generates control plane Machines and compute MachineSets manifests for IPI installation on vSphere. If administrators do not delete these manifests when doing UPI installation on vSphere, the OpenShift installer creates invalid Machine and MachineSet objects which trigger the alarms.\n\nPlease review [Alarms for 'Machine is in phase' when deploying OpenShift Container Platform 4.4 on VMware](https://access.redhat.com/solutions/5086271) for more information.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "**Option 1:** If feasible, Red Hat recommends that you redeploy the cluster with the folowing modification of the installation procedure:\n\n1. Generate the Kubernetes manifests as instructed by the documentation:\n~~~\n$ openshift-install create manifests --dir=<installation_directory>\n~~~\n\n2. *NEW:* Delete the Machine and MachineSet manifests.\n~~~\n$ cd <installation_directory>\n$ find . -name '*machineset*' -o -name '*master-machine*'\n$ rm -f openshift/99_openshift-cluster-api_master-machines-*.yaml openshift/99_openshift-cluster-api_worker-machineset-*.yaml\n~~~\n\n3. Continue with creating the ignition files as instructed by the documentation:\n~~~\n$ openshift-install create ignition-configs --dir=<installation_directory>\n~~~\n\nSee also the [Remove Machines and MachineSets](https://github.com/openshift/installer/blob/master/docs/user/vsphere/install_upi.md#remove-machines-and-machinesets) step in the OpenShift installer documentation for installation on vSphere using user provided infrastructure (UPI).\n\n**Option 2:** If redeploying the cluster is not feasible, RedHat recommends that you remove the invalid control plane Machine and compute MachineSet objects as a workaround.\n\n*This workaround works only for OpenShift 4.4. Do not apply this workaround on OpenShift 4.5 and higher.*\n\n~~~\n$ oc delete machine unnecessary_machine_name_xxx\n$ oc delete machinesets unnecessary_machinesets_name_xxx\n~~~\n",
      "more_info": "",
      "reason": "Starting with OpenShift 4.4, the OpenShift installer generates control plane Machines and compute MachineSets manifests for IPI installation on vSphere. If administrators do not delete these manifests when doing UPI installation on vSphere, the OpenShift installer creates invalid Machine and MachineSet objects which trigger the alarms.\n\nPlease review [Alarms for 'Machine is in phase' when deploying OpenShift Container Platform 4.4 on VMware](https://access.redhat.com/solutions/5086271) for more information.\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4893541",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.volume_attach_failed"
      },
      "error_keys": {
        "VOLUME_ATTACH_FAILED": {
          "metadata": {
            "description": "Pods get stuck in pending status when volume attach failed",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Pods get stuck in pending status when volume attach failed.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Following pods stuck in the Pending status due to volume attach failure.<br>\n\n{{for (var key in pydata.results) { }}\nPod Name/Namespace: {{=key}}\nMessage: {{=pydata.results[key]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5917331",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.high_core_dns_errors_high_alerts"
      },
      "error_keys": {
        "ERROR_HIGH_COREDNSERRORSHIGH_ALERTS": {
          "metadata": {
            "description": "High 'CoreDNSErrorsHigh' alerts",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "High 'CoreDNSErrorsHigh' alerts.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "CoreDNSErrorsHigh alerts are being triggered when it receives `SERVFAIL` from<br>\nthe core-dns pods. `SERVFAIL` is usually gotten when the core-dns does not have<br>\nthe A or AAAA record and it is not able to get it from the upstream nameserver.<br>\n\nPossible known Bug Found.<br>\nSummary     : The following CoreDNS pods are observing panics.<br>\n{{for (var pod_detail in pydata.results) { }}\n\nPod Name: {{=pydata.results[pod_detail][\"pod_name\"]}}\nMessage: {{=pydata.results[pod_detail][\"message\"]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5317441",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.master_mcp_degraded_due_to_etcd_manifest_validation"
      },
      "error_keys": {
        "MACHINE_CONFIG_POOL_OF_MASTER_IS_DEGRADED_DUE_TO_ETCD_MANIFEST_VALIDATION": {
          "metadata": {
            "description": "Cluster upgrade Master node Machine Config Daemon `could not stat file:\"/etc/kubernetes/manifests/etcd-member.yaml\"`",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Cluster upgrade Master node Machine Config Daemon `could not stat file: \"/etc/kubernetes/manifests/etcd-member.yaml\"`.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Current Cluster version: {{=pydata.current}}\n\nDesired Cluster version: {{=pydata.desired}}\n\nMachine Config Operator Degraded Status : {{=pydata.state[\"degraded\"][\"status\"]}}<br>\n<br>\nMatser Machine Config Pool is Degraded<br>\n<br>\nFound following logs:<br>\n<br>\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage: {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.ocp_version_end_of_life_eus"
      },
      "error_keys": {
        "OCP4XEUS_BEYOND_EOL": {
          "metadata": {
            "description": "The running OpenShift Extended Update Support version has reached its End of Life",
            "impact": 1,
            "likelihood": 4,
            "publish_date": "2021-02-17 15:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "The running OpenShift Extended Update Support version has reached its End of Life. It is no longer eligible to receive maintenance updates. Technical support is limited only to aid upgrades to in-support versions of OpenShift 4. \n",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to the next Extended Update Support release.\n\nFor more information, about the Extended Update Support channel, see the product documentation:\n\n* [Understanding Upgrade Channels](https://docs.openshift.com/container-platform/{{=pydata.version_major}}.{{=pydata.version_minor}}/updating/updating-cluster-between-minor.html#understanding-upgrade-channels_updating-cluster-between-minor)\n* [Updating a cluster between minor versions](https://docs.openshift.com/container-platform/{{=pydata.version_major}}.{{=pydata.version_minor}}/updating/updating-cluster-between-minor.html)\n",
          "more_info": "See also [Red Hat OpenShift Container Platform Life Cycle Policy](https://access.redhat.com/support/policy/updates/openshift#dates) for more information.\n\n",
          "reason": "The cluster is running OpenShift {{=pydata.version_full}} with Extended Update Support (EUS).\n\nOpenShift {{=pydata.version_major}}.{{=pydata.version_minor}} EUS has reached its End of Life on {{=pydata.eol_date}}.\n",
          "HasReason": true
        },
        "OCP4XEUS_EOL_APPROACHING": {
          "metadata": {
            "description": "The running OpenShift Extended Update Support version will reach its End of Life in less than 3 months",
            "impact": 1,
            "likelihood": 1,
            "publish_date": "2021-02-17 15:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "service_availability"
            ]
          },
          "total_risk": 1,
          "generic": "The running OpenShift Extended Update Support version will reach its End of Life in less than 3 months. It will no longer be eligible to receive maintenance updates. Technical support will be limited only to aid upgrades to in-support versions of OpenShift 4. \n",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to the next Extended Update Support release.\n\nFor more information, about the Extended Update Support channel, see the product documentation:\n\n* [Understanding Upgrade Channels](https://docs.openshift.com/container-platform/{{=pydata.version_major}}.{{=pydata.version_minor}}/updating/updating-cluster-between-minor.html#understanding-upgrade-channels_updating-cluster-between-minor)\n* [Updating a cluster between minor versions](https://docs.openshift.com/container-platform/{{=pydata.version_major}}.{{=pydata.version_minor}}/updating/updating-cluster-between-minor.html)\n",
          "more_info": "See also [Red Hat OpenShift Container Platform Life Cycle Policy](https://access.redhat.com/support/policy/updates/openshift#dates) for more information.\n\n",
          "reason": "The cluster is running OpenShift {{=pydata.version_full}} with Extended Update Support (EUS).\n\nOpenShift {{=pydata.version_major}}.{{=pydata.version_minor}} EUS will reach its End of Life on {{=pydata.eol_date}}.\n",
          "HasReason": true
        },
        "OCP4XEUS_EOL_IMMINENT": {
          "metadata": {
            "description": "The running OpenShift Extended Update Support version will reach its End of Life in less than 1 month",
            "impact": 1,
            "likelihood": 2,
            "publish_date": "2021-02-17 15:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "service_availability"
            ]
          },
          "total_risk": 1,
          "generic": "The running OpenShift Extended Update Support version will reach its End of Life in less than 1 month. It will no longer be eligible to receive maintenance updates. Technical support will be limited only to aid upgrades to in-support versions of OpenShift 4. \n",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to the next Extended Update Support release.\n\nFor more information, about the Extended Update Support channel, see the product documentation:\n\n* [Understanding Upgrade Channels](https://docs.openshift.com/container-platform/{{=pydata.version_major}}.{{=pydata.version_minor}}/updating/updating-cluster-between-minor.html#understanding-upgrade-channels_updating-cluster-between-minor)\n* [Updating a cluster between minor versions](https://docs.openshift.com/container-platform/{{=pydata.version_major}}.{{=pydata.version_minor}}/updating/updating-cluster-between-minor.html)\n",
          "more_info": "See also [Red Hat OpenShift Container Platform Life Cycle Policy](https://access.redhat.com/support/policy/updates/openshift#dates) for more information.\n\n",
          "reason": "The cluster is running OpenShift {{=pydata.version_full}} with Extended Update Support (EUS).\n\nOpenShift {{=pydata.version_major}}.{{=pydata.version_minor}} EUS will reach its End of Life on {{=pydata.eol_date}}.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to the next Extended Update Support release.\n\nFor more information, about the Extended Update Support channel, see the product documentation:\n\n* [Understanding Upgrade Channels](https://docs.openshift.com/container-platform/{{=pydata.version_major}}.{{=pydata.version_minor}}/updating/updating-cluster-between-minor.html#understanding-upgrade-channels_updating-cluster-between-minor)\n* [Updating a cluster between minor versions](https://docs.openshift.com/container-platform/{{=pydata.version_major}}.{{=pydata.version_minor}}/updating/updating-cluster-between-minor.html)\n",
      "more_info": "See also [Red Hat OpenShift Container Platform Life Cycle Policy](https://access.redhat.com/support/policy/updates/openshift#dates) for more information.\n\n",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4466631",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.mc_degraded_wrong_osimageurl"
      },
      "error_keys": {
        "MC_DEGRADED_WRONG_OSIMAGEURL": {
          "metadata": {
            "description": "Machine config pools degraded due to unexpected target osImageURL",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Machine config pools degraded due to unexpected target osImageURL.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Found machines upgrade failed due to wrong target osImageURL.<br>\nThe following machine config deamon pods report expected<br>\ntarget osImageURL while it is not used actually:<br>\n{{for (var pod in pydata.results) { }}\nPod Name: {{=pydata.results[pod][\"pod_name\"]}}\nMessage: {{=pydata.results[pod][\"message\"]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4565251",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.ingress_operator_degraded_with_reason_ingresscontrollers"
      },
      "error_keys": {
        "INGRESS_OPERATOR_DEGRADED_WITH_REASON_INGRESSCONTROLLERSDEGRADED": {
          "metadata": {
            "description": "The Rule is checking the status of ingress operator whichis found to be in degraded state",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The Rule is checking the status of ingress operator whichis found to be in degraded state.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Ingress operator is degraded with reason IngressControllersDegraded.<br>\n\nOperator name  : {{=pydata.operator[\"name\"]}}<br>\nDegraded status: {{=pydata.operator[\"degraded\"][\"status\"]}}<br>\nMessage        : {{=pydata.operator[\"degraded\"][\"message\"]}}<br>\nReason         : {{=pydata.operator[\"degraded\"][\"reason\"]}}<br>\nLast Transition: {{=pydata.operator[\"degraded\"][\"last_trans_time\"]}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5413911",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.upgrade_fails_due_to_packageserver_cert"
      },
      "error_keys": {
        "BUGZILLA_BUG_1873411": {
          "metadata": {
            "description": "While upgrading packageserver shows tls: bad certificate",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "While upgrading packageserver shows tls: bad certificate.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Possible known Bug Found.<br>\nSummary     : The following packageserver pods are observing the TLS issue.<br>\n\nPods:{{for (var pod_detail in pydata.result) { }}\n    - {{=pydata.result[pod_detail][\"pod_name\"]}}{{}}}.<br>\n\nMessage: {{=pydata.result[0][\"message\"]}}<br>\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.subnets_migration_failure_massive_egressip"
      },
      "error_keys": {
        "SUBNETS_MIGRATION_FAILURE_MASSIVE_EGRESSIP": {
          "metadata": {
            "description": "Egress IP migration fails when there are massive egress IPs",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2020-10-30 16:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "configuration",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "After migrations of massive egress IPs, some of the changes in the host subnets are not reflected on the new node, the egress IPs are still in original node.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade to 4.5 to avoid this issue.\n\n[Updating a cluster between minor versions](https://docs.openshift.com/container-platform/{{=pydata.version}}/updating/updating-cluster-between-minor.html) (official OCP4 documentation)\n",
          "more_info": "[How to Upgrade OpenShift 4 between different minor versions via \"oc\" cli](https://access.redhat.com/solutions/4606811) (a KCS that extends the official documentation)\n",
          "reason": "This cluster is running with version **{{=pydata.version}}**. Due to a known bug in this version, Egress IPs migration is in high risk of failing when the number of egress IPs is high. Following is hostsubnet{{?pydata.result.length>1}}s{{?}} with massive egress IPs.\n{{~pydata.result:item }}\n  * Host: {{=item[\"host\"]}}\n  * Number: {{=item[\"length\"]}}\n\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you upgrade to 4.5 to avoid this issue.\n\n[Updating a cluster between minor versions](https://docs.openshift.com/container-platform/{{=pydata.version}}/updating/updating-cluster-between-minor.html) (official OCP4 documentation)\n",
      "more_info": "[How to Upgrade OpenShift 4 between different minor versions via \"oc\" cli](https://access.redhat.com/solutions/4606811) (a KCS that extends the official documentation)\n",
      "reason": "This cluster is running with version **{{=pydata.version}}**. Due to a known bug in this version, Egress IPs migration is in high risk of failing when the number of egress IPs is high. Following is hostsubnet{{?pydata.result.length>1}}s{{?}} with massive egress IPs.\n{{~pydata.result:item }}\n  * Host: {{=item[\"host\"]}}\n  * Number: {{=item[\"length\"]}}\n\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6018601",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.authentication_operator_memory_leak"
      },
      "error_keys": {
        "ERROR_AUTHENTICATION_OPERATOR_MEMORY_LEAK": {
          "metadata": {
            "description": "Authentication operator is allocating a lot of memory",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Authentication operator is allocating a lot of memory.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1941840](https://bugzilla.redhat.com/show_bug.cgi?id=1941840) for more information.",
          "reason": "The Authentication operator is consuming an excessive amount of memory, growing by<br>\nthe hour without any apparent reason. That means it might have a memory leak.",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1941840](https://bugzilla.redhat.com/show_bug.cgi?id=1941840) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6024251",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.mco_controller_version_mismatch"
      },
      "error_keys": {
        "BUGZILLA_BUG_1955517": {
          "metadata": {
            "description": "Upgrade stuck due to Controller version mismatch reported by MCO",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Upgrade stuck due to Controller version mismatch reported by MCO.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1955517](https://bugzilla.redhat.com/show_bug.cgi?id=1955517) for more information.",
          "reason": "Possible known Bug Found.<br>\nSummary     : Upgrade stuck due to Controller version mismatch reported by MCO<br>\n\nOperator Name               : {{=pydata.state[\"name\"]}}<br>\nOperator Available State    : {{=pydata.state[\"available\"][\"status\"]}}<br>\nOperator Progressing State  : {{=pydata.state[\"progressing\"][\"status\"]}}<br>\nOperator Degraded State     : {{=pydata.state[\"degraded\"][\"status\"]}}<br>\nMessage                     : {{=pydata.state[\"degraded\"][\"message\"]}}<br>\nReason                      : {{=pydata.state[\"degraded\"][\"reason\"]}}<br>\n<br>\n{{for (var pod_detail in pydata.result) { }}\nPod Name: {{=pydata.result[pod_detail][\"pod_name\"]}}\nMessage: {{=pydata.result[pod_detail][\"message\"]}}{{}}}\n\nFollowing multiple machineconfigs could be observed:<br>\n{{for (var mc in pydata.mc_details) { }}\n** {{=pydata.mc_details[mc]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1955517](https://bugzilla.redhat.com/show_bug.cgi?id=1955517) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.ocp_version_end_of_life"
      },
      "error_keys": {
        "OCP4X_BEYOND_EOL": {
          "metadata": {
            "description": "The running OpenShift version has reached its End of Life",
            "impact": 1,
            "likelihood": 4,
            "publish_date": "2020-06-17 15:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "The running OpenShift version has reached its End of Life. It is no longer eligible to receive maintenance updates. Technical support is limited only to aid upgrades to in-support versions of OpenShift 4. \n",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to a more recent version. \n\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n\n* [OpenShift {{=pydata.minor_plus[1]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[1]['version']}}/updating/updating-cluster-between-minor.html)\n* [OpenShift {{=pydata.minor_plus[2]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[2]['version']}}/updating/updating-cluster-between-minor.html){{?pydata.beyond_eol}}\n* [OpenShift {{=pydata.minor_plus[3]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[3]['version']}}/updating/updating-cluster-between-minor.html){{?}}\n",
          "more_info": "",
          "reason": "  * The cluster is running OpenShift {{=pydata.current_version}}.\n  * OpenShift {{=pydata.minor_plus[0]['version']}} has reached its End of Life when OpenShift {{=pydata.minor_plus[3]['version']}} was released on {{=pydata.minor_plus[3]['ga']}}.\n\nPlease review [Red Hat OpenShift Container Platform Life Cycle Policy](https://access.redhat.com/support/policy/updates/openshift#dates) for more information.\n",
          "HasReason": true
        },
        "OCP4X_EOL_APPROACHING": {
          "metadata": {
            "description": "The running OpenShift version is estimated to reach its End of Life in less than 3 months",
            "impact": 1,
            "likelihood": 1,
            "publish_date": "2020-06-17 15:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "service_availability"
            ]
          },
          "total_risk": 1,
          "generic": "The running OpenShift version is estimated to reach its End of Life in less than 3 months. It will no longer be eligible to receive maintenance updates. Technical support will be limited only to aid upgrades to in-support versions of OpenShift 4. \n",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to a more recent version. \n\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n\n* [OpenShift {{=pydata.minor_plus[1]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[1]['version']}}/updating/updating-cluster-between-minor.html)\n* [OpenShift {{=pydata.minor_plus[2]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[2]['version']}}/updating/updating-cluster-between-minor.html){{?pydata.beyond_eol}}\n* [OpenShift {{=pydata.minor_plus[3]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[3]['version']}}/updating/updating-cluster-between-minor.html){{?}}\n",
          "more_info": "",
          "reason": "  * The cluster is running OpenShift {{=pydata.current_version}}.\n  * OpenShift {{=pydata.minor_plus[0]['version']}} will reach its End of Life at the release of OpenShift {{=pydata.minor_plus[3]['version']}}.\n  * Red Hat releases new minor versions of OpenShift 4 approximately every three months.\n  * OpenShift {{=pydata.minor_plus[2]['version']}} was released on {{=pydata.minor_plus[2]['ga']}}.\n  * OpenShift {{=pydata.minor_plus[3]['version']}} is estimated to be released in less than\n    {{?pydata.months_to_eol == 1}} {{=pydata.months_to_eol}} month\n    {{?}}{{?pydata.months_to_eol != 1}} {{=pydata.months_to_eol}} months{{?}}.\n\nPlease review [Red Hat OpenShift Container Platform Life Cycle Policy](https://access.redhat.com/support/policy/updates/openshift#dates) for more information.\n",
          "HasReason": true
        },
        "OCP4X_EOL_IMMINENT": {
          "metadata": {
            "description": "The running OpenShift version is estimated to reach its End of Life in less than 1 month",
            "impact": 1,
            "likelihood": 2,
            "publish_date": "2020-06-17 15:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "service_availability"
            ]
          },
          "total_risk": 1,
          "generic": "The running OpenShift version is estimated to reach its End of Life in less than 1 month. It will no longer be eligible to receive maintenance updates. Technical support will be limited only to aid upgrades to in-support versions of OpenShift 4. \n",
          "summary": "",
          "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to a more recent version. \n\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n\n* [OpenShift {{=pydata.minor_plus[1]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[1]['version']}}/updating/updating-cluster-between-minor.html)\n* [OpenShift {{=pydata.minor_plus[2]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[2]['version']}}/updating/updating-cluster-between-minor.html){{?pydata.beyond_eol}}\n* [OpenShift {{=pydata.minor_plus[3]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[3]['version']}}/updating/updating-cluster-between-minor.html){{?}}\n",
          "more_info": "",
          "reason": "  * The cluster is running OpenShift {{=pydata.current_version}}.\n  * OpenShift {{=pydata.minor_plus[0]['version']}} will reach its End of Life at the release of OpenShift {{=pydata.minor_plus[3]['version']}}.\n  * Red Hat releases new minor versions of OpenShift 4 approximately every three months.\n  * OpenShift {{=pydata.minor_plus[2]['version']}} was released on {{=pydata.minor_plus[2]['ga']}}.\n  * OpenShift {{=pydata.minor_plus[3]['version']}} is estimated to be released in less than\n    {{?pydata.months_to_eol == 1}} {{=pydata.months_to_eol}} month\n    {{?}}{{?pydata.months_to_eol != 1}} {{=pydata.months_to_eol}} months{{?}}.\n\nPlease review [Red Hat OpenShift Container Platform Life Cycle Policy](https://access.redhat.com/support/policy/updates/openshift#dates) for more information.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you upgrade the OpenShift cluster to a more recent version. \n\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n\n* [OpenShift {{=pydata.minor_plus[1]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[1]['version']}}/updating/updating-cluster-between-minor.html)\n* [OpenShift {{=pydata.minor_plus[2]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[2]['version']}}/updating/updating-cluster-between-minor.html){{?pydata.beyond_eol}}\n* [OpenShift {{=pydata.minor_plus[3]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[3]['version']}}/updating/updating-cluster-between-minor.html){{?}}\n",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4828091",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.bug_1802248"
      },
      "error_keys": {
        "BUGZILLA_BUG_1802248": {
          "metadata": {
            "description": "The cluster shows symptoms of BZ 1802248: Upgrading from 4.2.16 -> 4.3.0 causes alerts with the openshift-ingress-operator",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The cluster shows symptoms of BZ 1802248: Upgrading from 4.2.16 -> 4.3.0 causes alerts with the openshift-ingress-operator.\n",
          "summary": "",
          "resolution": "See [KCS 4828091](https://access.redhat.com/solutions/4828091).\n",
          "more_info": "* See [KCS 4828091](https://access.redhat.com/solutions/4828091) for more information about possible solutions.\n* See [BZ 1802248](https://bugzilla.redhat.com/show_bug.cgi?id=1802248) for more information about the bug.\n",
          "reason": "The cluster shows symptoms of BZ 1802248.\n\nThe version is 4.2.x or 4.3.x and openshift-ingress-operator's deployment is using port 60000 for metrics.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "See [KCS 4828091](https://access.redhat.com/solutions/4828091).\n",
      "more_info": "* See [KCS 4828091](https://access.redhat.com/solutions/4828091) for more information about possible solutions.\n* See [BZ 1802248](https://bugzilla.redhat.com/show_bug.cgi?id=1802248) for more information about the bug.\n",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4563171",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.samples_op_failed_image_import_check"
      },
      "error_keys": {
        "SAMPLES_FAILED_IMAGE_IMPORT_ERR": {
          "metadata": {
            "description": "Pods could fail to start if openshift-samples is degraded due to FailedImageImport which is caused by a hiccup while talking to the Red Hat registry",
            "impact": 2,
            "likelihood": 2,
            "publish_date": "2020-02-07 14:19:00",
            "status": "active",
            "tags": [
              "openshift",
              "incident",
              "networking",
              "registry",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "The `openshift-samples` cluster-operator is degraded due to `FailedImageImport` because of a hiccup while talking to the Red Hat registry.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4563171)\n",
          "summary": "",
          "resolution": "Red Hat recommends that you to follow these steps:\n\n1. Fix 1, Try running:\n~~~\n# oc import-image <for the ImageStream(s) in question>\n~~~\n\n1. Fix 2, Try running:\n~~~\n# oc delete configs.samples cluster\n~~~",
          "more_info": "",
          "reason": "Due to a temporary hiccup talking to the Red Hat registry the openshift-samples failed to import some of the imagestreams.\n\n\nSource of the issue:\n\n**Cluster-operator:**  **{{=pydata.info[\"name\"]}}**\n- *Condition:* {{=pydata.info[\"condition\"]}}\n- *Reason:* {{=pydata.info[\"reason\"]}}\n- *Message:* {{=pydata.info[\"message\"]}}\n- *Last* Transition: {{=pydata.info[\"lastTransitionTime\"]}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you to follow these steps:\n\n1. Fix 1, Try running:\n~~~\n# oc import-image <for the ImageStream(s) in question>\n~~~\n\n1. Fix 2, Try running:\n~~~\n# oc delete configs.samples cluster\n~~~",
      "more_info": "",
      "reason": "Due to a temporary hiccup talking to the Red Hat registry the openshift-samples failed to import some of the imagestreams.\n\n\nSource of the issue:\n\n**Cluster-operator:**  **{{=pydata.info[\"name\"]}}**\n- *Condition:* {{=pydata.info[\"condition\"]}}\n- *Reason:* {{=pydata.info[\"reason\"]}}\n- *Message:* {{=pydata.info[\"message\"]}}\n- *Last* Transition: {{=pydata.info[\"lastTransitionTime\"]}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4569191",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.cluster_wide_proxy_auth_check"
      },
      "error_keys": {
        "AUTH_OPERATOR_PROXY_ERROR": {
          "metadata": {
            "description": "The authentication operator is degraded when cluster is configured to use a cluster-wide proxy",
            "impact": 2,
            "likelihood": 2,
            "publish_date": "2020-02-03 08:25:00",
            "status": "active",
            "tags": [
              "security",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "When the cluster is configured to use a cluster-wide proxy, the `authentication` operator is `Degraded`.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4569191)\n",
          "summary": "",
          "resolution": "Red Hat recommends that you to follow steps in the KCS article.\n * [Authentication operator Degraded with Reason `WellKnownEndpointDegradedError`](https://access.redhat.com/solutions/4569191)\n",
          "more_info": "For more information about the configuring the proxy, refer to [enabling the cluster-wide proxy](https://docs.openshift.com/container-platform/4.3/networking/enable-cluster-wide-proxy.html#nw-proxy-configure-object_config-cluster-wide-proxy)\n",
          "reason": "Requests to routes and/or the public API endpoint are not being proxied to the cluster.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you to follow steps in the KCS article.\n * [Authentication operator Degraded with Reason `WellKnownEndpointDegradedError`](https://access.redhat.com/solutions/4569191)\n",
      "more_info": "For more information about the configuring the proxy, refer to [enabling the cluster-wide proxy](https://docs.openshift.com/container-platform/4.3/networking/enable-cluster-wide-proxy.html#nw-proxy-configure-object_config-cluster-wide-proxy)\n",
      "reason": "Requests to routes and/or the public API endpoint are not being proxied to the cluster.\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "4608081",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.auth_operator_unhonoring_oauth_configuration"
      },
      "error_keys": {
        "AUTHENTICATION_OPERATOR_NOT_HONORING_OAUTH": {
          "metadata": {
            "description": "The authentication operator can't honor OAuth configuration",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "The authentication operator can't honor OAuth configuration.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "Found authentication operator failing to honor OAuth OpenID due to unknown certificate authority.<br>\n\n{{for (var pod_detail in pydata.results) { }}\nPod Name: {{=pydata.results[pod_detail][\"pod_name\"]}}\nMessage: {{=pydata.results[pod_detail][\"message\"]}}{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5350721",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.check_crio_ephemeral_storage"
      },
      "error_keys": {
        "EVENT_CRIO_EPHEMERAL_STORAGE": {
          "metadata": {
            "description": "A node can't run any pods when there is an issue in crio ephemeral-storage",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "A node can't run any pods when there is an issue in crio ephemeral-storage.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "When there is an issue in crio ephemeral storage, A node can't run any pods with the error:<br>\n  {{=pydata.message}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6012101",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.mcp_unavailable_machine_count"
      },
      "error_keys": {
        "ERROR_MCP_UNAVAILABLEMACHINECOUNT": {
          "metadata": {
            "description": "MachineConfigPool will never finish updating when the `unavailableMachineCount` is greater than `maxUnavailable` in the MachineConfigPool",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2021-07-30 12:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "MachineConfigPool will never finish updating when the `unavailableMachineCount` is greater than `maxUnavailable` in the MachineConfigPool.\n",
          "summary": "",
          "resolution": "**Option 1:** Red Hat recommends that you increase the `.spec.maxUnavailable` count.\n\n~~~\n{{ for (var index in pydata.mcps) { }}\n$ oc patch mcp {{=pydata.mcps[index]['name']}} --type=merge -p \"{\\\"spec\\\":{\\\"maxUnavailable\\\": {{=pydata.mcps[index]['unavailable_machine_count']+10}} }}\"\n{{?}}\n~~~\n\n**Option 2:** Red Hat recommends that you make nodes back available by fixing the errors reported in the respective node logs.\n",
          "more_info": "",
          "reason": "A MachineConfigPool will never finish updating when the number of unavailable nodes (`unavailableMachineCount`) is greater than the allowed number of unavailable nodes (`maxUnavailable`).\n\nThe following MachineConfigPool{{?Object.keys(pydata.mcps).length>=2}}s are{{??}} is{{?}} in that state:\n{{ for (var mcp in pydata.mcps) { }}\n  - MachineConfigPool: {{=pydata.mcps[mcp][\"mcp_name\"]}}, `unavailableMachineCount`: {{=pydata.mcps[mcp][\"unavailableMachineCount\"]}}, `maxUnavailable`: {{=pydata.mcps[mcp][\"maxUnavailable\"]}}\n{{ } }}\n\nThe `unavailableMachineCount` is increased for each node that matches any of the following conditions:\n- Node is in NotReady state\n- Node is in SchedulingDisabled state\n- Node is reporting NodeDiskPressure\n- Node is reporting NodeNetworkUnavailable\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "**Option 1:** Red Hat recommends that you increase the `.spec.maxUnavailable` count.\n\n~~~\n{{ for (var index in pydata.mcps) { }}\n$ oc patch mcp {{=pydata.mcps[index]['name']}} --type=merge -p \"{\\\"spec\\\":{\\\"maxUnavailable\\\": {{=pydata.mcps[index]['unavailable_machine_count']+10}} }}\"\n{{?}}\n~~~\n\n**Option 2:** Red Hat recommends that you make nodes back available by fixing the errors reported in the respective node logs.\n",
      "more_info": "",
      "reason": "A MachineConfigPool will never finish updating when the number of unavailable nodes (`unavailableMachineCount`) is greater than the allowed number of unavailable nodes (`maxUnavailable`).\n\nThe following MachineConfigPool{{?Object.keys(pydata.mcps).length>=2}}s are{{??}} is{{?}} in that state:\n{{ for (var mcp in pydata.mcps) { }}\n  - MachineConfigPool: {{=pydata.mcps[mcp][\"mcp_name\"]}}, `unavailableMachineCount`: {{=pydata.mcps[mcp][\"unavailableMachineCount\"]}}, `maxUnavailable`: {{=pydata.mcps[mcp][\"maxUnavailable\"]}}\n{{ } }}\n\nThe `unavailableMachineCount` is increased for each node that matches any of the following conditions:\n- Node is in NotReady state\n- Node is in SchedulingDisabled state\n- Node is reporting NodeDiskPressure\n- Node is reporting NodeNetworkUnavailable\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5514091",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.prometheus_too_old_resource_version"
      },
      "error_keys": {
        "PROMETHEUS_TOO_OLD_RESOURCE_VERSION": {
          "metadata": {
            "description": "Alert manager is not connected and not ingesting samples due to'too old resource version'",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Alert manager is not connected and not ingesting samples due to'too old resource version'.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1845561](https://bugzilla.redhat.com/show_bug.cgi?id=1845561) for more information.",
          "reason": "Alert manager is not connected and not ingesting samples due to<br>\n'too old resource version'. The affected pods are listed below.<br>\n\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage : {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1845561](https://bugzilla.redhat.com/show_bug.cgi?id=1845561) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5880571",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.bug_rules.apiserver_degraded_not_available"
      },
      "error_keys": {
        "BUGZILLA_BUG_1912566": {
          "metadata": {
            "description": "Openshift-apiserver degraded and kube-apiserver not available in OCP 4",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Openshift-apiserver degraded and kube-apiserver not available in OCP 4.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the [Bugzilla 1912566](https://bugzilla.redhat.com/show_bug.cgi?id=1912566) for more information.",
          "reason": "Panics from the kube-apiserver are causing API Priority and Fairness (P&F) to reject requests<br>\nand openshift-apiserver is degraded when trying to access the APIs.<br>\n\n{{for (var pod_detail in pydata.res) { }}\nPod Name    : {{=pydata.res[pod_detail][\"pod_name\"]}}\nMessage     :<br>\n{{=pydata.res[pod_detail][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the [Bugzilla 1912566](https://bugzilla.redhat.com/show_bug.cgi?id=1912566) for more information.",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5240831",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.etcd_low_backend_performance"
      },
      "error_keys": {
        "ETCD_LOW_BACKEND_PERFORMANCE": {
          "metadata": {
            "description": "Etcd performance degradation due to low throughput",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "Etcd performance degradation due to low throughput.",
          "summary": "",
          "resolution": "",
          "more_info": "Refer to the following links for more information:\n* [KCS 4770281](https://access.redhat.com/solutions/4770281)\n* [KCS 5340011](https://access.redhat.com/solutions/5340011)\n",
          "reason": "Log messages indicate low etcd backend performance.\n{{for (var pod_detail in pydata.results) { }}\nPod Name: {{=pydata.results[pod_detail][\"pod_name\"]}}<br>\nMessages:\n{{for (var item in pydata.results[pod_detail][\"message\"]) { }}\n{{=pydata.results[pod_detail][\"message\"][item]}}<br>\n{{}}}\n{{? pydata.results[pod_detail][\"statistics_result\"] }}\nStats about etcd \"too long\" messages:<br>\n  first  {{=pydata.results[pod_detail][\"start_time\"]}}<br>\n  last   {{=pydata.results[pod_detail][\"end_time\"]}}<br>\n  count  {{=pydata.results[pod_detail][\"statistics_result\"][\"count\"]}}<br>\n  median {{=pydata.results[pod_detail][\"statistics_result\"][\"median\"]}} ms<br>\n  mean   {{=pydata.results[pod_detail][\"statistics_result\"][\"mean\"]}} ms<br>\n  min    {{=pydata.results[pod_detail][\"statistics_result\"][\"min\"]}} ms<br>\n  max    {{=pydata.results[pod_detail][\"statistics_result\"][\"max\"]}} ms<br>\n{{?}}\n{{}}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "Refer to the following links for more information:\n* [KCS 4770281](https://access.redhat.com/solutions/4770281)\n* [KCS 5340011](https://access.redhat.com/solutions/5340011)\n",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "6264501",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.vsphere_failed_to_create_volumes_for_missing_privilege"
      },
      "error_keys": {
        "ERROR_VSPHERE_FOLDER_PRIVILEGE": {
          "metadata": {
            "description": "OpenShift is unable to create volumes for new Persistent Volume Claims when its vCenter account is not allowed to list volumes in the default datastore",
            "impact": 1,
            "likelihood": 4,
            "publish_date": "2021-09-06 12:00:00",
            "status": "active",
            "tags": [
              "service_availability",
              "vsphere"
            ]
          },
          "total_risk": 2,
          "generic": "OpenShift is unable to create volumes for new Persistent Volume Claims when its vCenter account is not allowed to list volumes in the default datastore.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you grant the [required privileges](https://docs.openshift.com/container-platform/{{=pydata.ocp_version}}/installing/installing_vsphere/installing-vsphere-installer-provisioned.html#installation-vsphere-installer-infra-requirements_installing-vsphere-installer-provisioned) to the vCenter account used by the OpenShift Container Platform.\n",
          "more_info": "",
          "reason": "The `VSphereOpenshiftClusterHealthFail` alert is firing for the `CheckFolderPermissions` check. The Cluster Storage Operator fires this alert when the vCenter account used by the OpenShift cluster does not have permissions to list directories in the default datastore. The operator specifically checks this by trying to list the `/` directory and, if it exists, also the `/kubevols` directory.\n\nThe OpenShift cluster needs the permissions to be able to dynamically provision volumes for new Persistent Volume Claims.\n\nThe following alert{{?pydata.alert.length>1}}s are {{??}} is{{?}} detected:\n{{~pydata.alert:alert}}\n* **Alert Name: {{=alert[\"alertname\"]}}**\n* **Check:** {{=alert[\"check\"]}}\n* **Severity:** {{=alert[\"severity\"]}}\n* **State:** {{=alert[\"status\"]}}\n* **Namespace:** {{=alert[\"namespace\"]}}\n{{~}}\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you grant the [required privileges](https://docs.openshift.com/container-platform/{{=pydata.ocp_version}}/installing/installing_vsphere/installing-vsphere-installer-provisioned.html#installation-vsphere-installer-infra-requirements_installing-vsphere-installer-provisioned) to the vCenter account used by the OpenShift Container Platform.\n",
      "more_info": "",
      "reason": "The `VSphereOpenshiftClusterHealthFail` alert is firing for the `CheckFolderPermissions` check. The Cluster Storage Operator fires this alert when the vCenter account used by the OpenShift cluster does not have permissions to list directories in the default datastore. The operator specifically checks this by trying to list the `/` directory and, if it exists, also the `/kubevols` directory.\n\nThe OpenShift cluster needs the permissions to be able to dynamically provision volumes for new Persistent Volume Claims.\n\nThe following alert{{?pydata.alert.length>1}}s are {{??}} is{{?}} detected:\n{{~pydata.alert:alert}}\n* **Alert Name: {{=alert[\"alertname\"]}}**\n* **Check:** {{=alert[\"check\"]}}\n* **Severity:** {{=alert[\"severity\"]}}\n* **State:** {{=alert[\"status\"]}}\n* **Namespace:** {{=alert[\"namespace\"]}}\n{{~}}\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.empty_prometheus_db_volume"
      },
      "error_keys": {
        "PROMETHEUS_DB_VOLUME_IS_EMPTY": {
          "metadata": {
            "description": "Prometheus metrics data will be lost when the Prometheus pod is restarted or recreated",
            "impact": 1,
            "likelihood": 4,
            "publish_date": "2020-11-17 11:47:00",
            "status": "active",
            "tags": [
              "openshift",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "Prometheus metrics data will be lost when the Prometheus pod is restarted or recreated. The PVC for Prometheus is set to EmptyDir which is a temporary directory.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you configure permanent storage for the Prometheus Cluster Monitoring Stack. For more information, see the *Configuring persistent storage* section of the documentation of the desired OpenShift version:\n  * [OpenShift {{=pydata.ocp_branch}}](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/monitoring/configuring-the-monitoring-stack.html#configuring-persistent-storage)\n",
          "more_info": "",
          "reason": "  * There is no persistent storage for Prometheus to store the metrics data\n\nPlease review [Configuring the monitoring stack](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/monitoring/configuring-the-monitoring-stack.html) for more information.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you configure permanent storage for the Prometheus Cluster Monitoring Stack. For more information, see the *Configuring persistent storage* section of the documentation of the desired OpenShift version:\n  * [OpenShift {{=pydata.ocp_branch}}](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/monitoring/configuring-the-monitoring-stack.html#configuring-persistent-storage)\n",
      "more_info": "",
      "reason": "  * There is no persistent storage for Prometheus to store the metrics data\n\nPlease review [Configuring the monitoring stack](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/monitoring/configuring-the-monitoring-stack.html) for more information.\n",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "",
        "product_code": "",
        "python_module": "ccx_rules_ocp.external.rules.machineconfig_stuck_by_node_taints"
      },
      "error_keys": {
        "NODE_HAS_TAINTS_APPLIED": {
          "metadata": {
            "description": "The Machine Config Daemon stops working when a Node has taints applied",
            "impact": 2,
            "likelihood": 3,
            "publish_date": "2020-11-18 12:00:00",
            "status": "active",
            "tags": [
              "openshift",
              "service_availability"
            ]
          },
          "total_risk": 2,
          "generic": "When the Machine Config Daemon Daemon Set does not set a universal Toleration, the Machine Config Daemon Pod gets removed from tainted nodes due to a bug in OpenShift Container Platform 4. This results in decreased cluster availability.\n",
          "summary": "",
          "resolution": "Red Hat recommends that you perform the following steps:\n1. If MachineConfigDaemon pod gets removed from a node, patch the machine-config Daemonset to allow taints in the nodes and properly schedule the necessary pods:\n~~~\n# oc patch ds machine-config-daemon -n openshift-machine-config-operator  --type=merge -p '{\"spec\": {\"template\": { \"spec\": {\"tolerations\":[{\"operator\":\"Exists\"}]}}}}'\n~~~\n`Note`: this patch will be removed by the operator after some hours but will be sufficient to allow the upgrade on the stuck nodes.\n\n2. Upgrade the OpenShift cluster to a more recent version.\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n  * [OpenShift {{=pydata.ocp_branch}}](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/updating/updating-cluster-between-minor.html)\n",
          "more_info": "",
          "reason": "  * The cluster is running OpenShift {{=pydata.current_version}}.\n  * Some nodes have taints applied.\n\nPlease review [MachineConfig stuck in updating mode if OpenShift Container Platform - Node has taints applied](https://access.redhat.com/solutions/5137781) for more information.\n",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "Red Hat recommends that you perform the following steps:\n1. If MachineConfigDaemon pod gets removed from a node, patch the machine-config Daemonset to allow taints in the nodes and properly schedule the necessary pods:\n~~~\n# oc patch ds machine-config-daemon -n openshift-machine-config-operator  --type=merge -p '{\"spec\": {\"template\": { \"spec\": {\"tolerations\":[{\"operator\":\"Exists\"}]}}}}'\n~~~\n`Note`: this patch will be removed by the operator after some hours but will be sufficient to allow the upgrade on the stuck nodes.\n\n2. Upgrade the OpenShift cluster to a more recent version.\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n  * [OpenShift {{=pydata.ocp_branch}}](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/updating/updating-cluster-between-minor.html)\n",
      "more_info": "",
      "reason": "",
      "HasReason": true
    },
    {
      "plugin": {
        "name": "",
        "node_id": "5425561",
        "product_code": "",
        "python_module": "ccx_rules_ocp.internal.rules.api_server_miss_call_alert"
      },
      "error_keys": {
        "API_SERVER_MISS_CALL_ALERT": {
          "metadata": {
            "description": "KubeAPIErrorsHigh alert occurs due to missed calls to the API server",
            "impact": 1,
            "likelihood": 0,
            "publish_date": "",
            "status": "",
            "tags": null
          },
          "total_risk": 0,
          "generic": "KubeAPIErrorsHigh alert occurs due to missed calls to the API server.",
          "summary": "",
          "resolution": "",
          "more_info": "",
          "reason": "KubeAPIErrorsHigh alert occurs due to missed calls to the API server.<br>\nBuilds are intermittently failing.<br>\n\n{{for (var pod_data in pydata.results) { }}\nPod Name: {{=pydata.results[pod_data][\"pod_name\"]}}\nMessage : {{=pydata.results[pod_data][\"message\"]}}\n{{}}}",
          "HasReason": true
        }
      },
      "generic": "",
      "summary": "",
      "resolution": "",
      "more_info": "",
      "reason": "",
      "HasReason": true
    }
  ],
  "status": "ok"
}