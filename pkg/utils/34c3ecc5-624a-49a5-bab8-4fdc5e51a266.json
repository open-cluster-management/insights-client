{"reports": {
    "34c3ecc5-624a-49a5-bab8-4fdc5e51a266": {"report":{"meta":{"count":15,"last_checked_at":"2021-02-23T16:36:03Z"},"data":[{"rule_id":"ccx_rules_ocm.tutorial_rule.report","created_at":"2020-04-08T00:42:00Z","description":"Introducing Insights for Red Hat OpenShift Container Platform","details":"Red Hat Insights for OpenShift is a proactive management solution. It provides ongoing infrastructure analyses of your Red Hat OpenShift Container Platform 4.2 and later installations. Red Hat Insights helps you identify, prioritize, and resolve risks to security, performance, availability, and stability before they become urgent issues.\nRed Hat Insights for OpenShift uses the Remote Health Monitoring feature of OpenShift 4. The health checks are created by Red Hat subject matter experts and assessed according to severity and impact.\nThis is an example  recommendation that you can safely ignore. To disable it, click  the triple-dot menu button next to the header, and select Disable.\n\n[Knowledgebase Article](https://access.redhat.com/search/#/knowledgebase)\n\n","reason":"","resolution":"","total_risk":1,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"error_key":"TUTORIAL_ERROR","type":"rule"},"tags":["openshift","service_availability"]},{"rule_id":"ccx_rules_ocp.external.rules.container_max_root_partition_size.report","created_at":"2020-11-17T11:10:00Z","description":"Containers leak available disk space of the underlying host when there is no ContainerRuntimeConfig","details":"If there is no ContainerRuntimeConfig, then the root partition of a container shows the available disk space of the underlying host.\n","reason":"* There {{?Object.keys(pydata.mcps).length\u003e1}}are machinepools that don't have{{??}}a machinepool that doesn't have{{?}} the ContainerRuntimeConfig:\n{{ for (var index in pydata.mcps) { }}\n - **{{=pydata.mcps[index]}}**\n{{?}}\n\nThat allows containers to show the available disk space of the underlying host.\n\nPlease review [Setting the default max container root partition size for overlay with cri-o](https://access.redhat.com/solutions/5216861) for more information.\n","resolution":"Red Hat recommends that you perform the following steps:\n1. Create the ContainerRuntimeConfig CRD, like the following example:\n~~~\n# cat \u003c\u003c EOF| oc create -f -\napiVersion: v1\nkind: ContainerRuntimeConfig\nmetadata:\n name: xxx\nspec:\n machineConfigPoolSelector:\n   matchLabels:\n     match_key_xxx: match_value_yyy\n...\nEOF\n~~~\n\n2. To apply the new container runtime configuration to your nodes, add the matchLabels name you set in the ContainerRuntimeConfig CRD to the corresponding machineconfigpool, like the following example:\n~~~\n# oc edit machineconfigpool node_name\napiVersion: machineconfiguration.openshift.io/v1\nkind: MachineConfigPool\nmetadata:\n  labels:\n    match_key_xxx: match_value_yyy\n~~~\n\nFor more information, see the Red Hat document [Setting the default max container root partition size for overlay with cri-o](https://access.redhat.com/solutions/5216861)\n","total_risk":2,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"error_key":"CONTAINER_ROOT_PARTITION_SIZE","mcps":["master"],"type":"rule"},"tags":["openshift","security"]},{"rule_id":"ccx_rules_ocp.external.rules.samples_op_failed_image_import_check.report","created_at":"2020-02-07T14:19:00Z","description":"Pods could fail to start if openshift-samples is degraded due to FailedImageImport which is caused by a hiccup while talking to the Red Hat registry","details":"The `openshift-samples` cluster-operator is degraded due to `FailedImageImport` because of a hiccup while talking to the Red Hat registry.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4563171)\n","reason":"Due to a temporary hiccup talking to the Red Hat registry the openshift-samples failed to import some of the imagestreams.\n\n\nSource of the issue:\n\n**Cluster-operator:**  **{{=pydata.info[\"name\"]}}**\n- *Condition:* {{=pydata.info[\"condition\"]}}\n- *Reason:* {{=pydata.info[\"reason\"]}}\n- *Message:* {{=pydata.info[\"message\"]}}\n- *Last* Transition: {{=pydata.info[\"lastTransitionTime\"]}}\n","resolution":"Red Hat recommends that you to follow these steps:\n\n1. Fix 1, Try running:\n~~~\n# oc import-image \u003cfor the ImageStream(s) in question\u003e\n~~~\n\n1. Fix 2, Try running:\n~~~\n# oc delete configs.samples cluster\n~~~","total_risk":2,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"error_key":"SAMPLES_FAILED_IMAGE_IMPORT_ERR","info":{"condition":"Degraded","lastTransitionTime":"2019-12-06T15:58:09Z","message":"Samples installed at , with image import failures for these imagestreams:","name":"openshift-samples","reason":"FailedImageImports"},"type":"rule"},"tags":["openshift","incident","networking","registry","service_availability","pssa"]},{"rule_id":"ccx_rules_ocp.external.rules.node_installer_degraded.report","created_at":"2020-03-06T12:00:00Z","description":"Clusteroperator is degraded when the installer pods are removed too soon during upgrade","details":"Clusteroperator is degraded with \"NodeInstallerDegraded\" in status.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4849711)\n","reason":"Clusteroperator{{?pydata.degraded_operators.length\u003e1}}s{{?}} degraded with NodeInstallerDegraded in reason:\n\n{{~ pydata.degraded_operators :operator }}\n**Cluster-operator:**  **{{=operator[\"name\"]}}**\n- *Reason:* {{=operator[\"degraded\"][\"reason\"]}}\n- *Message:* {{=operator[\"degraded\"][\"message\"]}}\n- *Last transition*: {{=operator[\"degraded\"][\"last_trans_time\"]}}\n\n{{~}}\n","resolution":"You may be hitting a [known bug](https://bugzilla.redhat.com/show_bug.cgi?id=1723966) and Red Hat recommends that you complete the following steps:\n\n{{~ pydata.degraded_operators :operator }}\n{{? operator[\"name\"] == \"kube-apiserver\"}}\n- For the **kube-apiserver** clusteroperator do:\n~~~\noc patch kubeapiserver/cluster --type merge -p \"{\\\"spec\\\":{\\\"forceRedeploymentReason\\\":\\\"Forcing new revision with random number $RANDOM to make message unique\\\"}}\"\n~~~\n{{?}}\n{{? operator[\"name\"] == \"kube-controller-manager\"}}\n- For the **kube-controller-manager** clusteroperator do:\n~~~\noc patch kubecontrollermanager/cluster --type merge -p \"{\\\"spec\\\":{\\\"forceRedeploymentReason\\\":\\\"Forcing new revision with random number $RANDOM to make message unique\\\"}}\"\n~~~\n{{?}}\n{{? operator[\"name\"] == \"kube-scheduler\"}}\n- For the **kube-scheduler** clusteroperator do:\n~~~\noc patch kubescheduler/cluster --type merge -p \"{\\\"spec\\\":{\\\"forceRedeploymentReason\\\":\\\"Forcing new revision with random number $RANDOM to make message unique\\\"}}\"\n~~~\n{{?}}\nThen wait several minutes and check if the operator is no longer degraded or progressing. If it is still degraded and the same error message is shown, retry (the race condition can be triggered again). If the error message is different or some retries do not make any improvement, open a support case to get further assistance.\n\nIf this solution solves your issue, but you are interested in tracking the definitive resolution of the bug, you can open a support case to do that as well.\n{{~}}","total_risk":3,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"degraded_operators":[{"available":{"last_trans_time":"2020-04-21T12:45:10Z","message":"Available: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 2; 0 nodes have achieved new revision 3","reason":"AsExpected","status":true},"degraded":{"last_trans_time":"2020-04-21T12:46:14Z","message":"NodeControllerDegraded: All master nodes are ready\nStaticPodsDegraded: nodes/ip-10-0-137-172.us-east-2.compute.internal pods/kube-apiserver-ip-10-0-137-172.us-east-2.compute.internal container=\"kube-apiserver-3\" is not ready","reason":"NodeInstallerDegradedInstallerPodFailed","status":true},"name":"kube-apiserver","progressing":{"last_trans_time":"2020-04-21T12:43:00Z","message":"Progressing: 1 nodes are at revision 0; 2 nodes are at revision 2; 0 nodes have achieved new revision 3","reason":"","status":true},"upgradeable":{"last_trans_time":"2020-04-21T12:42:52Z","message":"","reason":"AsExpected","status":true},"version":"4.3.13"}],"error_key":"NODE_INSTALLER_DEGRADED","type":"rule"},"tags":["openshift","service_availability","pssa"]},{"rule_id":"ccx_rules_ocp.external.rules.nodes_requirements_check.report","created_at":"2019-10-29T15:00:00Z","description":"OCP node could behave unexpectedly when it doesn't meet the minimum resource requirements","details":"Minimum resource requirements for Openshift 4 Nodes are not met, which could cause unexpected behavior.\n\n[Knowledgebase Article](https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html#minimum-resource-requirements_installing-bare-metal)\n","reason":"Node{{?pydata.nodes.length\u003e1}}s{{?}} not meeting the minimum requirements:\n{{~ pydata.nodes :node }}\n1. {{=node[\"name\"]}}\n  * Role: {{=node[\"role\"]}}{{?node.memory}}\n  * Minimum memory requirement is {{=node[\"memory_req\"]}}, but the node is configured with {{=node[\"memory\"]}}.{{?}}{{?node.cpu}}\n  * Minimum cpu requirement is {{=node[\"cpu_req\"]}}, but the node is configured with {{=node[\"cpu\"]}}.{{?}}{{~}}","resolution":"Red Hat recommends that you configure your nodes to meet the minimum resource requirements.\n\nMake sure that:\n\n{{~ pydata.nodes :node }}\n1. Node {{=node[\"name\"]}} ({{=node[\"role\"]}}){{?node[\"memory\"]}}\n   * Has enough memory, minimum requirement is {{=node[\"memory_req\"]}}. Currently its only configured with {{=node[\"memory\"]}}GB.{{?}}{{?node.cpu}}\n   * Has enough allocatable cpu, minimum requirement is {{=node[\"cpu_req\"]}}. Currently its only configured with {{=node[\"cpu\"]}}.{{?}}{{~}}\n","total_risk":2,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"error_key":"NODES_MINIMUM_REQUIREMENTS_NOT_MET","nodes":[{"cpu":1,"cpu_req":2,"name":"ip-10-0-144-53.us-east-2.compute.internal","role":"worker"}],"type":"rule"},"tags":["openshift","configuration","performance","pssa"]},{"rule_id":"ccx_rules_ocp.external.bug_rules.bug_1766907.report","created_at":"2020-01-17T11:10:00Z","description":"The OpenShift cluster will experience upgrade failure when the cluster wide proxy is configured due to a bug","details":"Due to a known bug in OpenShift 4 the `clusterversion` operator is not respecting the cluster-wide proxy settings.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4631541)\n","reason":"On this OCP 4 cluster, a cluster wide proxy is set. Due to a bug, the CVO is not using the proxy. This will lead to a upgrade failure.","resolution":"Red Hat recommends that you to use this workaround:\n1. Set the proxy manually\n~~~\n# oc -n openshift-cluster-version set env deploy cluster-version-operator HTTP_PROXY=xxx HTTPS_PROXY=xxx NO_PROXY=xxx\n~~~\n","total_risk":2,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"error_key":"BUGZILLA_BUG_1766907","type":"rule"},"tags":["openshift","networking","service_availability"]},{"rule_id":"ccx_rules_ocp.external.rules.ocp_version_end_of_life.report","created_at":"2020-06-17T15:00:00Z","description":"The running OpenShift version has reached its End of Life","details":"The running OpenShift version has reached its End of Life. It is no longer eligible to receive maintenance updates. Technical support is limited only to aid upgrades to in-support versions of OpenShift 4. \n","reason":"","resolution":"Red Hat recommends that you upgrade the OpenShift cluster to a more recent version. \n\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n\n* [OpenShift {{=pydata.minor_plus[1]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[1]['version']}}/updating/updating-cluster-between-minor.html)\n* [OpenShift {{=pydata.minor_plus[2]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[2]['version']}}/updating/updating-cluster-between-minor.html){{?pydata.beyond_eol}}\n* [OpenShift {{=pydata.minor_plus[3]['version']}}](https://docs.openshift.com/container-platform/{{=pydata.minor_plus[3]['version']}}/updating/updating-cluster-between-minor.html){{?}}\n","total_risk":2,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"beyond_eol":true,"current_version":"4.3.11","error_key":"OCP4X_BEYOND_EOL","minor_plus":[{"ga":"2020-01-23T00:00:00Z","version":"4.3"},{"ga":"2020-05-05T00:00:00Z","version":"4.4"},{"ga":"2020-07-13T00:00:00Z","version":"4.5"},{"ga":"2020-10-27T00:00:00Z","version":"4.6"}],"type":"rule"},"tags":["openshift","service_availability","pssa"]},{"rule_id":"ccx_rules_ocp.external.rules.unsupport_sdn_plugin.report","created_at":"2020-08-06T12:00:00Z","description":"Cluster is running with 3rd party SDN plugin","details":"Cluster is running with a 3rd party SDN plugin that will encounter support limitations.\n","reason":"This cluster is running with 3rd party SDN plugin `{{=pydata.network_type}}`, which will encounter support limitations.\n","resolution":"Red Hat recommends that you migrate to a supported SDN plugin.\n\n**Supported SDN plugin:**\n - *OpenShiftSDN*\n - *OVNKubernetes*\n - *Kuryr*\n\nFor update steps, please refer to this doc:\n{{? ['4.0', '4.1', '4.2', '4.3', '4.4', '4.5'].indexOf(pydata.current_version) \u003e= 0}}\n[Migrating to the OVN-Kubernetes default CNI network provider](https://docs.openshift.com/container-platform/4.5/networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.html)\n{{??}}\n[Migrating to the OVN-Kubernetes default CNI network provider](https://docs.openshift.com/container-platform/{{=pydata.current_version}}/networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.html)\n{{?}}\n\n","total_risk":2,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"current_version":"4.3","error_key":"UNSUPPORT_SDN_PLUGIN","network_type":"Something unsupported","type":"rule"},"tags":["sbr_shift","openshift","sdn","fault_tolerance","pssa"]},{"rule_id":"ccx_rules_ocp.external.rules.empty_prometheus_db_volume.report","created_at":"2020-11-17T11:47:00Z","description":"Prometheus metrics data will be lost when the Prometheus pod is restarted or recreated","details":"Prometheus metrics data will be lost when the Prometheus pod is restarted or recreated. The PVC for Prometheus is set to EmptyDir which is a temporary directory.\n","reason":"  * There is no persistent storage for Prometheus to store the metrics data\n\nPlease review [Configuring the monitoring stack](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/monitoring/configuring-the-monitoring-stack.html) for more information.\n","resolution":"Red Hat recommends that you configure permanent storage for the Prometheus Cluster Monitoring Stack. For more information, see the *Configuring persistent storage* section of the documentation of the desired OpenShift version:\n  * [OpenShift {{=pydata.ocp_branch}}](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/monitoring/cluster_monitoring/configuring-the-monitoring-stack.html#configuring-persistent-storage)\n","total_risk":2,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"error_key":"PROMETHEUS_DB_VOLUME_IS_EMPTY","ocp_branch":"4.3","type":"rule"},"tags":["openshift","service_availability"]},{"rule_id":"ccx_rules_ocp.external.rules.cluster_wide_proxy_auth_check.report","created_at":"2020-02-03T08:25:00Z","description":"The authentication operator is degraded when cluster is configured to use a cluster-wide proxy","details":"When the cluster is configured to use a cluster-wide proxy, the `authentication` operator is `Degraded`.\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4569191)\n","reason":"Requests to routes and/or the public API endpoint are not being proxied to the cluster.\n","resolution":"Red Hat recommends that you to follow steps in the KCS article.\n * [Authentication operator Degraded with Reason `WellKnownEndpointDegradedError`](https://access.redhat.com/solutions/4569191)\n","total_risk":2,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"error_key":"AUTH_OPERATOR_PROXY_ERROR","op":{"available":{"last_trans_time":"2020-04-21T12:46:28Z","message":"","reason":"NoData","status":null},"degraded":{"last_trans_time":"2020-04-21T12:46:29Z","message":"WellKnownEndpointDegraded: failed to GET well-known","reason":"AsExpected","status":true},"name":"authentication","progressing":{"last_trans_time":"2020-04-21T12:46:28Z","message":"","reason":"NoData","status":null},"upgradeable":{"last_trans_time":"2020-04-21T12:46:28Z","message":"","reason":"AsExpected","status":true},"version":null},"type":"rule"},"tags":["security","service_availability","pssa"]},{"rule_id":"ccx_rules_ocp.external.security.CVE_2020_8555_kubernetes.report","created_at":"2021-01-08T12:00:00Z","description":"CVE-2020-8555: compromised node could escalate to cluster level privileges","details":"The running OpenShift version is vulnerable to CVE-2020-8555: Compromised node could escalate to cluster level with Moderate security impact.\n\nThis flaw allows an attacker who can intercept requests on a compromised node to redirect those requests, along with their credentials, to perform actions on other endpoints that trust those credentials (including other clusters), allowing for escalation of privileges.\n","reason":"* The cluster is running OpenShift {{=pydata.current_version}}.\n","resolution":"Red Hat recommends that you upgrade the OpenShift cluster to a more recent version. \n\nFor more information, see the *Updating a cluster within a minor version from the web console* section of the documentation of the OpenShift version:\n\n* [OpenShift {{=pydata.minor_version}}](https://docs.openshift.com/container-platform/{{=pydata.minor_version}}/updating/updating-cluster.html)\n","total_risk":2,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"current_version":"4.3.11","error_key":"CVE_2020_8555_KUBERNETES","minor_version":"4.3","type":"rule"},"tags":["security","openshift","cve"]},{"rule_id":"ccx_rules_ocp.external.bug_rules.bug_1821905.report","created_at":"2020-04-17T16:00:00Z","description":"Cluster upgrade will fail when default SCC gets changed","details":"Due to a known bug in OpenShift 4 operators cannot upgrade due to \"DefaultSecurityContextConstraints_Mutated\".\n\n[Knowledgebase Article](https://access.redhat.com/solutions/4972291)\n","reason":"The OCP-{{=pydata.desired}} update is blocked because default security context constraints (SCC) anyuid, hostaccess, hostmount-anyuid, hostnetwork, nonroot, privileged, or restricted have been modified\n\nUpgrading 4.3.8, 4.3.9, 4.3.10, 4.3.11, or 4.3.12 fails if security context constraints (SCC) are not the default.\n\nOCP 4.3.8 introduced a new check for modified or mutated default SCCs. If any of the SCCs anyuid, hostaccess, hostmount-anyuid, hostnetwork, nonroot, privileged, or restricted have been modified, upgrades to future releases are prevented. For more details see [BZ-1808602](https://bugzilla.redhat.com/show_bug.cgi?id=1808602) and [BZ-1810596](https://bugzilla.redhat.com/show_bug.cgi?id=1810596) from [Bug Fix Advisory RHBA-2020:0858](https://access.redhat.com/errata/RHBA-2020:0858).\n\nThis check is to ensure that environments with modified default SCCs could not be upgraded to 4.4 as changes or removal of the default SCCs could lead to unexpected behavior and system instability.\n\nOCP 4.3.13 ([Bug Fix Advisory RHBA-2020:1481](https://access.redhat.com/errata/RHBA-2020:1481)) relaxes this check and will no longer block the upgrade.\n\n","resolution":"OpenShift Container Platform (OCP) 4.3.13 will no longer block upgrades if the SCC is not the default.\n\nThe original issue raised affected versions 4.3.8, 4.3.9, 4.3.10, 4.3.11, and 4.3.12.\n\n- I have already upgraded to one of the affected versions:\n  - You will need to use the `--force` flag to upgrade.\n- I must upgrade to one of the affected versions before I can upgrade to 4.3.13:\n- This is not recommended. However, if you must upgrade to an affected version, be aware that you will need to use the `--force` flag to perform your next upgrade.\n\n**Using the `--force` flag**:\n\n**IMPORTANT:** Any changes you have made to the default SCCs `anyuid`, `hostaccess`, `hostmount-anyuid`, `hostnetwork`, `nonroot`, `privileged`, or `restricted` may be removed later when you upgrade to 4.4 which could cause system instability. You should address this issue by migrating any changes you made to the mentioned default SCCs to new SCCs.\n\n- Use of the `--force` flag will skip all precondition tests. You must verify that there are no other preconditions which need to be considered.\n- Upgrading using `--force` **will not** remove the changes you have made to the default SCCs. You should create a plan to migrate the changes you made to the default SCCs to new SCCs before you upgrade to 4.4.\n\nThe `--force` flag can be added to your `oc adm upgrade` command. For example:\n~~~\n# oc adm upgrade --force --to 4.3.13\n~~~\n","total_risk":3,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"desired":"4.3.11","error_key":"BUGZILLA_BUG_1821905","type":"rule"},"tags":["openshift","service_availability"]},{"rule_id":"ccx_rules_ocp.external.rules.machineconfig_stuck_by_node_taints.report","created_at":"2020-11-18T12:00:00Z","description":"The Machine Config Daemon stops working when a Node has taints applied","details":"When the Machine Config Daemon Daemon Set does not set a universal Toleration, the Machine Config Daemon Pod gets removed from tainted nodes due to a bug in OpenShift Container Platform 4. This results in decreased cluster availability.\n","reason":"","resolution":"Red Hat recommends that you perform the following steps:\n1. If MachineConfigDaemon pod gets removed from a node, patch the machine-config Daemonset to allow taints in the nodes and properly schedule the necessary pods:\n~~~\n# oc patch ds machine-config-daemon -n openshift-machine-config-operator  --type=merge -p '{\"spec\": {\"template\": { \"spec\": {\"tolerations\":[{\"operator\":\"Exists\"}]}}}}'\n~~~\n`Note`: this patch will be removed by the operator after some hours but will be sufficient to allow the upgrade on the stuck nodes.\n\n2. Upgrade the OpenShift cluster to a more recent version.\nFor more information, see the *Updating a cluster between minor versions* section of the documentation of the desired OpenShift version:\n  * [OpenShift {{=pydata.ocp_branch}}](https://docs.openshift.com/container-platform/{{=pydata.ocp_branch}}/updating/updating-cluster-between-minor.html)\n","total_risk":2,"risk_of_change":0,"disabled":false,"disable_feedback":"","disabled_at":"","internal":false,"user_vote":0,"extra_data":{"current_version":"4.3.11","error_key":"NODE_HAS_TAINTS_APPLIED","ocp_branch":"4.3","type":"rule"},"tags":["openshift","service_availability"]}]},"status":"ok"}}}